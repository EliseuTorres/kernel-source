From: Jiri Slaby <jslaby@suse.cz>
Date: Wed, 13 Jul 2011 21:22:00 +0200
Subject: Revert "mm: prevent concurrent unmap_mapping_range() on the same inode"
Patch-mainline: never

This reverts commit ffdd12eabed263a487ddc05fdf65be6e4fc717b4
(2aa15890f3c191326678f1bd68af61ec6b8753ec upstream).

It breaks kabi. We have a non-kabi breaking fix in:
patches.fixes/mm-prevent-concurrent-unmap_mapping_range-on-.patch

Signed-off-by: Jiri Slaby <jslaby@suse.cz>
---
 fs/inode.c         |   22 +++++++---------------
 fs/nilfs2/btnode.c |   13 +++++++++++++
 fs/nilfs2/btnode.h |    1 +
 fs/nilfs2/super.c  |    2 +-
 include/linux/fs.h |    2 --
 mm/memory.c        |    2 --
 6 files changed, 22 insertions(+), 20 deletions(-)

diff --git a/fs/inode.c b/fs/inode.c
index 8bbe005..4d8e3be 100644
--- a/fs/inode.c
+++ b/fs/inode.c
@@ -256,20 +256,6 @@ void destroy_inode(struct inode *inode)
 		kmem_cache_free(inode_cachep, (inode));
 }
 
-void address_space_init_once(struct address_space *mapping)
-{
-	memset(mapping, 0, sizeof(*mapping));
-	INIT_RADIX_TREE(&mapping->page_tree, GFP_ATOMIC);
-	spin_lock_init(&mapping->tree_lock);
-	spin_lock_init(&mapping->i_mmap_lock);
-	INIT_LIST_HEAD(&mapping->private_list);
-	spin_lock_init(&mapping->private_lock);
-	INIT_RAW_PRIO_TREE_ROOT(&mapping->i_mmap);
-	INIT_LIST_HEAD(&mapping->i_mmap_nonlinear);
-	mutex_init(&mapping->unmap_mutex);
-}
-EXPORT_SYMBOL(address_space_init_once);
-
 /*
  * These are initializations that only need to be done
  * once, because the fields are idempotent across use
@@ -281,7 +267,13 @@ void inode_init_once(struct inode *inode)
 	INIT_HLIST_NODE(&inode->i_hash);
 	INIT_LIST_HEAD(&inode->i_dentry);
 	INIT_LIST_HEAD(&inode->i_devices);
-	address_space_init_once(&inode->i_data);
+	INIT_RADIX_TREE(&inode->i_data.page_tree, GFP_ATOMIC);
+	spin_lock_init(&inode->i_data.tree_lock);
+	spin_lock_init(&inode->i_data.i_mmap_lock);
+	INIT_LIST_HEAD(&inode->i_data.private_list);
+	spin_lock_init(&inode->i_data.private_lock);
+	INIT_RAW_PRIO_TREE_ROOT(&inode->i_data.i_mmap);
+	INIT_LIST_HEAD(&inode->i_data.i_mmap_nonlinear);
 	i_size_ordered_init(inode);
 #ifdef CONFIG_INOTIFY
 	INIT_LIST_HEAD(&inode->inotify_watches);
diff --git a/fs/nilfs2/btnode.c b/fs/nilfs2/btnode.c
index 8dff317..84c2538 100644
--- a/fs/nilfs2/btnode.c
+++ b/fs/nilfs2/btnode.c
@@ -34,6 +34,19 @@
 #include "btnode.h"
 
 
+void nilfs_btnode_cache_init_once(struct address_space *btnc)
+{
+	memset(btnc, 0, sizeof(*btnc));
+	INIT_RADIX_TREE(&btnc->page_tree, GFP_ATOMIC);
+	spin_lock_init(&btnc->tree_lock);
+	INIT_LIST_HEAD(&btnc->private_list);
+	spin_lock_init(&btnc->private_lock);
+
+	spin_lock_init(&btnc->i_mmap_lock);
+	INIT_RAW_PRIO_TREE_ROOT(&btnc->i_mmap);
+	INIT_LIST_HEAD(&btnc->i_mmap_nonlinear);
+}
+
 static const struct address_space_operations def_btnode_aops = {
 	.sync_page		= block_sync_page,
 };
diff --git a/fs/nilfs2/btnode.h b/fs/nilfs2/btnode.h
index 067913e..3e22751 100644
--- a/fs/nilfs2/btnode.h
+++ b/fs/nilfs2/btnode.h
@@ -37,6 +37,7 @@ struct nilfs_btnode_chkey_ctxt {
 	struct buffer_head *newbh;
 };
 
+void nilfs_btnode_cache_init_once(struct address_space *);
 void nilfs_btnode_cache_init(struct address_space *, struct backing_dev_info *);
 void nilfs_btnode_cache_clear(struct address_space *);
 int nilfs_btnode_submit_block(struct address_space *, __u64, sector_t,
diff --git a/fs/nilfs2/super.c b/fs/nilfs2/super.c
index 93c11af3..63e7b10 100644
--- a/fs/nilfs2/super.c
+++ b/fs/nilfs2/super.c
@@ -166,7 +166,7 @@ static void init_once(void *obj)
 #ifdef CONFIG_NILFS_XATTR
 	init_rwsem(&ii->xattr_sem);
 #endif
-	address_space_init_once(&ii->i_btnode_cache);
+	nilfs_btnode_cache_init_once(&ii->i_btnode_cache);
 	ii->i_bmap = (struct nilfs_bmap *)&ii->i_bmap_union;
 	inode_init_once(&ii->vfs_inode);
 }
diff --git a/include/linux/fs.h b/include/linux/fs.h
index 1b9a47a..1ff0962 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -635,7 +635,6 @@ struct address_space {
 	spinlock_t		private_lock;	/* for use by the address_space */
 	struct list_head	private_list;	/* ditto */
 	struct address_space	*assoc_mapping;	/* ditto */
-	struct mutex		unmap_mutex;    /* to protect unmapping */
 } __attribute__((aligned(sizeof(long))));
 	/*
 	 * On most architectures that alignment is already the case; but
@@ -2159,7 +2158,6 @@ extern loff_t vfs_llseek(struct file *file, loff_t offset, int origin);
 
 extern int inode_init_always(struct super_block *, struct inode *);
 extern void inode_init_once(struct inode *);
-extern void address_space_init_once(struct address_space *mapping);
 extern void inode_add_to_lists(struct super_block *, struct inode *);
 extern void iput(struct inode *);
 extern struct inode * igrab(struct inode *);
diff --git a/mm/memory.c b/mm/memory.c
index 6c836d3..53c1da0 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -2454,7 +2454,6 @@ void unmap_mapping_range(struct address_space *mapping,
 		details.last_index = ULONG_MAX;
 	details.i_mmap_lock = &mapping->i_mmap_lock;
 
-	mutex_lock(&mapping->unmap_mutex);
 	spin_lock(&mapping->i_mmap_lock);
 
 	/* Protect against endless unmapping loops */
@@ -2471,7 +2470,6 @@ void unmap_mapping_range(struct address_space *mapping,
 	if (unlikely(!list_empty(&mapping->i_mmap_nonlinear)))
 		unmap_mapping_range_list(&mapping->i_mmap_nonlinear, &details);
 	spin_unlock(&mapping->i_mmap_lock);
-	mutex_unlock(&mapping->unmap_mutex);
 }
 EXPORT_SYMBOL(unmap_mapping_range);
 
-- 
1.7.6

