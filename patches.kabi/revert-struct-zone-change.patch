From: Jiri Slaby <jslaby@suse.cz>
Subject: Fix struct zone kabi breakage
Patch-mainline: never

Avoid changing struct zone contents since it is inlined in struct
pglist_data (aka pg_data_t) and it breaks kabi heavily. Really, there
may be users of those structures and we don't want to change the
layout here. The workaround is to move it into pg_data_t as an array
and to access it via zone->zone_pgdat. It's OK to have it there, at
the end of the structure, since it is defined only statically
(non-NUMA) or allocated by the core at boot time (NUMA).

Introduced by commit aa45484031ddee09b06350ab8528bfe5b2c76d1c
upstream.

Signed-off-by: Jiri Slaby <jslaby@suse.cz>
---
 include/linux/mmzone.h |   17 ++++++++++-------
 mm/mmzone.c            |    2 +-
 mm/vmstat.c            |    4 ++--
 3 files changed, 13 insertions(+), 10 deletions(-)

--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -290,13 +290,6 @@ struct zone {
 	unsigned long watermark[NR_WMARK];
 
 	/*
-	 * When free pages are below this point, additional steps are taken
-	 * when reading the number of free pages to avoid per-cpu counter
-	 * drift allowing watermarks to be breached
-	 */
-	unsigned long percpu_drift_mark;
-
-	/*
 	 * We don't know if the memory that we're going to allocate will be freeable
 	 * or/and it will be released eventually, so to avoid totally wasting several
 	 * GB of ram we must reserve some of the lower zone memory (otherwise we risk
@@ -652,6 +645,16 @@ typedef struct pglist_data {
 	wait_queue_head_t kswapd_wait;
 	struct task_struct *kswapd;
 	int kswapd_max_order;
+#ifndef __GENKSYMS__
+	/*
+	 * When free pages are below this point, additional steps are taken
+	 * when reading the number of free pages to avoid per-cpu counter
+	 * drift allowing watermarks to be breached
+	 *
+	 * It is here solely for kABI purposes. -js
+	 */
+	unsigned long percpu_drift_mark[MAX_NR_ZONES];
+#endif
 } pg_data_t;
 
 #define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)
--- a/mm/mmzone.c
+++ b/mm/mmzone.c
@@ -101,7 +101,7 @@ unsigned long zone_nr_free_pages(struct
 	 * potentially causing a live-lock. While kswapd is awake and
 	 * free pages are low, get a better estimate for free pages
 	 */
-	if (nr_free_pages < zone->percpu_drift_mark &&
+	if (nr_free_pages < zone->zone_pgdat->percpu_drift_mark[zone_idx(zone)] &&
 			!waitqueue_active(&zone->zone_pgdat->kswapd_wait))
 		return zone_page_state_snapshot(zone, NR_FREE_PAGES);
 
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -151,8 +151,8 @@ static void refresh_zone_stat_thresholds
 		tolerate_drift = low_wmark_pages(zone) - min_wmark_pages(zone);
 		max_drift = num_online_cpus() * threshold;
 		if (max_drift > tolerate_drift)
-			zone->percpu_drift_mark = high_wmark_pages(zone) +
-					max_drift;
+			zone->zone_pgdat->percpu_drift_mark[zone_idx(zone)] =
+					high_wmark_pages(zone) + max_drift;
 	}
 }
 
