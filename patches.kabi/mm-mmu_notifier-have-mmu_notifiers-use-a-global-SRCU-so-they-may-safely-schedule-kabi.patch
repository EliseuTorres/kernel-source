From: Mel Gorman <mgorman@suse.de>
Date: Thu, 21 Feb 2013 11:19:28 +0000
Subject: [PATCH] Have mmu_notifiers use SRCU so they may safely schedule kabi
 compatability

References: bnc#578046, bnc#786814, FATE#306952
Patch-mainline: No, never

SP2 merged support for sleeping MMU notifiers that differed from what was
merged upstream. SP2 used a per-mm srcu_struct but upstream noted that
this could have considerable memory overhead. The upstream patches are
now included in SP2 but unfortunately this patch is necessary to convert
back to per-mm SCRU to preserve KABI compatability.

Signed-off-by: Mel Gorman <mgorman@suse.de>
---
 include/linux/mmu_notifier.h |    2 ++
 mm/mmu_notifier.c            |   43 ++++++++++++++++++++++++++-----------------
 2 files changed, 28 insertions(+), 17 deletions(-)

--- a/include/linux/mmu_notifier.h
+++ b/include/linux/mmu_notifier.h
@@ -20,6 +20,8 @@ struct mmu_notifier_ops;
 struct mmu_notifier_mm {
 	/* all mmu notifiers registerd in this mm are queued in this list */
 	struct hlist_head list;
+	/* srcu structure for this mm */
+	struct srcu_struct srcu;
 	/* to serialize the list modifications and hlist_unhashed */
 	spinlock_t lock;
 };
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@ -44,7 +44,7 @@ void __mmu_notifier_release(struct mm_st
 	 * SRCU here will block mmu_notifier_unregister until
 	 * ->release returns.
 	 */
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, node, &mm->mmu_notifier_mm->list, hlist)
 		/*
 		 * If ->release runs before mmu_notifier_unregister it must be
@@ -54,7 +54,7 @@ void __mmu_notifier_release(struct mm_st
 		 */
 		if (mn->ops->release)
 			mn->ops->release(mn, mm);
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 
 	spin_lock(&mm->mmu_notifier_mm->lock);
 	while (unlikely(!hlist_empty(&mm->mmu_notifier_mm->list))) {
@@ -95,12 +95,12 @@ int __mmu_notifier_clear_flush_young(str
 	struct hlist_node *n;
 	int young = 0, id;
 
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, n, &mm->mmu_notifier_mm->list, hlist) {
 		if (mn->ops->clear_flush_young)
 			young |= mn->ops->clear_flush_young(mn, mm, address);
 	}
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 
 	return young;
 }
@@ -110,9 +110,10 @@ int __mmu_notifier_test_young(struct mm_
 {
 	struct mmu_notifier *mn;
 	struct hlist_node *n;
-	int young = 0, id;
+	int young = 0;
+	int id;
 
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, n, &mm->mmu_notifier_mm->list, hlist) {
 		if (mn->ops->test_young) {
 			young = mn->ops->test_young(mn, mm, address);
@@ -120,7 +121,7 @@ int __mmu_notifier_test_young(struct mm_
 				break;
 		}
 	}
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 
 	return young;
 }
@@ -132,7 +133,7 @@ void __mmu_notifier_change_pte(struct mm
 	struct hlist_node *n;
 	int id;
 
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, n, &mm->mmu_notifier_mm->list, hlist) {
 		if (mn->ops->change_pte)
 			mn->ops->change_pte(mn, mm, address, pte);
@@ -143,7 +144,7 @@ void __mmu_notifier_change_pte(struct mm
 		else if (mn->ops->invalidate_page)
 			mn->ops->invalidate_page(mn, mm, address);
 	}
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 }
 
 void __mmu_notifier_invalidate_page(struct mm_struct *mm,
@@ -153,12 +154,12 @@ void __mmu_notifier_invalidate_page(stru
 	struct hlist_node *n;
 	int id;
 
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, n, &mm->mmu_notifier_mm->list, hlist) {
 		if (mn->ops->invalidate_page)
 			mn->ops->invalidate_page(mn, mm, address);
 	}
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 }
 
 void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,
@@ -168,12 +169,12 @@ void __mmu_notifier_invalidate_range_sta
 	struct hlist_node *n;
 	int id;
 
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, n, &mm->mmu_notifier_mm->list, hlist) {
 		if (mn->ops->invalidate_range_start)
 			mn->ops->invalidate_range_start(mn, mm, start, end);
 	}
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 }
 
 void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,
@@ -183,12 +184,12 @@ void __mmu_notifier_invalidate_range_end
 	struct hlist_node *n;
 	int id;
 
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, n, &mm->mmu_notifier_mm->list, hlist) {
 		if (mn->ops->invalidate_range_end)
 			mn->ops->invalidate_range_end(mn, mm, start, end);
 	}
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 }
 
 static int do_mmu_notifier_register(struct mmu_notifier *mn,
@@ -211,6 +212,10 @@ static int do_mmu_notifier_register(stru
 	if (unlikely(!mmu_notifier_mm))
 		goto out;
 
+	ret = init_srcu_struct(&mmu_notifier_mm->srcu);
+	if (unlikely(ret))
+		goto out_kfree;
+
 	if (take_mmap_sem)
 		down_write(&mm->mmap_sem);
 	ret = mm_take_all_locks(mm);
@@ -241,6 +246,9 @@ static int do_mmu_notifier_register(stru
 out_cleanup:
 	if (take_mmap_sem)
 		up_write(&mm->mmap_sem);
+	if (mmu_notifier_mm)
+		cleanup_srcu_struct(&mmu_notifier_mm->srcu);
+out_kfree:
 	/* kfree() does nothing if mmu_notifier_mm is NULL */
 	kfree(mmu_notifier_mm);
 out:
@@ -281,6 +289,7 @@ EXPORT_SYMBOL_GPL(__mmu_notifier_registe
 void __mmu_notifier_mm_destroy(struct mm_struct *mm)
 {
 	BUG_ON(!hlist_empty(&mm->mmu_notifier_mm->list));
+	cleanup_srcu_struct(&mm->mmu_notifier_mm->srcu);
 	kfree(mm->mmu_notifier_mm);
 	mm->mmu_notifier_mm = LIST_POISON1; /* debug */
 }
@@ -306,14 +315,14 @@ void mmu_notifier_unregister(struct mmu_
 		 */
 		int id;
 
-		id = srcu_read_lock(&srcu);
+		id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 		/*
 		 * exit_mmap will block in mmu_notifier_release to guarantee
 		 * that ->release is called before freeing the pages.
 		 */
 		if (mn->ops->release)
 			mn->ops->release(mn, mm);
-		srcu_read_unlock(&srcu, id);
+		srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 
 		spin_lock(&mm->mmu_notifier_mm->lock);
 		/*
