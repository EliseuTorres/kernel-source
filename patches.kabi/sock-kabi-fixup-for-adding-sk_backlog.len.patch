From: Brandon Philips <bphilips@suse.de>
Subject: sock: kabi fixup for adding sk_backlog.len
References: bnc#655973
Patch-mainline: Never

Don't break KABI by adding the sk_backlog.len variable. Instead tack the
necessary bits to the end of the struct sock transparently and use
sock_kabi() to access it.

This is safe because the networking core is in charge of allocating and
deallocating all struct socks.

Signed-off-by: Brandon Philips <bphilips@suse.de>

---
 include/net/sock.h |   31 ++++++++++++++++++++++++++++---
 net/core/sock.c    |    7 +++++--
 2 files changed, 33 insertions(+), 5 deletions(-)

--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -242,7 +242,6 @@ struct sock {
 	struct {
 		struct sk_buff *head;
 		struct sk_buff *tail;
-		int len;
 	} sk_backlog;
 	wait_queue_head_t	*sk_sleep;
 	struct dst_entry	*sk_dst_cache;
@@ -305,6 +304,24 @@ struct sock {
 };
 
 /*
+ * Prevent kabi-breakage by extending the original struct sock
+ */
+struct sock_kabi {
+	struct {
+		int len;
+	} sk_backlog;
+};
+
+#define SOCK_KABI_SIZE ALIGN(sizeof(struct sock_kabi), sizeof(long))
+
+static inline unsigned int sock_kabi_alloc_size(unsigned int prot_sock_size)
+{
+       return ALIGN(prot_sock_size, sizeof(long)) + SOCK_KABI_SIZE;
+}
+
+static inline struct sock_kabi *sock_kabi(const struct sock *sk);
+
+/*
  * Hashed lists helper routines
  */
 static inline struct sock *__sk_head(const struct hlist_head *head)
@@ -579,7 +596,7 @@ static inline void __sk_add_backlog(stru
  */
 static inline bool sk_rcvqueues_full(const struct sock *sk, const struct sk_buff *skb)
 {
-	unsigned int qsize = sk->sk_backlog.len + atomic_read(&sk->sk_rmem_alloc);
+	unsigned int qsize = sock_kabi(sk)->sk_backlog.len + atomic_read(&sk->sk_rmem_alloc);
 
 	return qsize + skb->truesize > sk->sk_rcvbuf;
 }
@@ -591,7 +608,7 @@ static inline __must_check int sk_add_ba
 		return -ENOBUFS;
 
 	__sk_add_backlog(sk, skb);
-	sk->sk_backlog.len += skb->truesize;
+	sock_kabi(sk)->sk_backlog.len += skb->truesize;
 	return 0;
 }
 
@@ -725,6 +742,14 @@ struct proto {
 #endif
 };
 
+static inline struct sock_kabi *sock_kabi(const struct sock *sk)
+{
+	unsigned int obj_size = sk->sk_prot_creator->obj_size;
+	unsigned int kabi_offset = obj_size - SOCK_KABI_SIZE;
+
+	return (struct sock_kabi *)(((u8 *)sk) + kabi_offset);
+}
+
 extern int proto_register(struct proto *prot, int alloc_slab);
 extern void proto_unregister(struct proto *prot);
 
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -1122,7 +1122,7 @@ struct sock *sk_clone(const struct sock
 		sock_lock_init(newsk);
 		bh_lock_sock(newsk);
 		newsk->sk_backlog.head	= newsk->sk_backlog.tail = NULL;
-		newsk->sk_backlog.len = 0;
+		sock_kabi(newsk)->sk_backlog.len = 0;
 
 		atomic_set(&newsk->sk_rmem_alloc, 0);
 		/*
@@ -1531,7 +1531,7 @@ static void __release_sock(struct sock *
 	 * Doing the zeroing here guarantee we can not loop forever
 	 * while a wild producer attempts to flood us.
 	 */
-	sk->sk_backlog.len = 0;
+	sock_kabi(sk)->sk_backlog.len = 0;
 }
 
 /**
@@ -2207,6 +2207,9 @@ static inline void release_proto_idx(str
 
 int proto_register(struct proto *prot, int alloc_slab)
 {
+
+	prot->obj_size = sock_kabi_alloc_size(prot->obj_size);
+
 	if (alloc_slab) {
 		prot->slab = kmem_cache_create(prot->name, prot->obj_size, 0,
 					SLAB_HWCACHE_ALIGN | prot->slab_flags,
