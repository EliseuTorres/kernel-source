From: Heiko Carstens <heiko.carstens@de.ibm.com>
Subject: s390/ftrace: add HAVE_DYNAMIC_FTRACE_WITH_REGS support
Patch-mainline: v3.18-rc1
Git-commit: 10dec7dbd50ab0be96dda085d625d54ce800e426
References: bnc#903279, LTC#118177

Description:  kernel: reduce function tracer overhead
Symptom:      The kernel uses more cpu cycles for each function being
              executed.
Problem:      The kGraft feature requires to instrument the kernel. In
              order to do that the kernel gets compiled with the function
              tracer enabled which causes the compiler to emit code that
              adds an "mcount" call to the prologue of each function.
              This code will be modified by the kernel for function tracing.
              However the implementation was not optimal, since even if
              disabled each function stored a value on the stack and
              afterwards contained an unconditional branch which skipped
              the rest of the mcount prologue code.
              So more instructions than necessary will be executed which
              results in a reduced performance.
Solution:     Patch the mcount prologue code so that only a single
              instruction of the mcount code will be executed. It's either
              a branch that skips the rest of the mcount prologue code or
              a branch to the function tracer.
Reproduction: Compile the kernel with and without function tracer enabled
              and compare cpu time spent in the kernel for identical
              workloads.

Upstream-Description:

              s390/ftrace: add HAVE_DYNAMIC_FTRACE_WITH_REGS support

              This code is based on a patch from Vojtech Pavlik.
              http://marc.info/?l=linux-s390&m=140438885114413&w=2

              The actual implementation now differs significantly:
              Instead of adding a second function "ftrace_regs_caller" which would be nearly
              identical to the existing ftrace_caller function, the current ftrace_caller
              function is now an alias to ftrace_regs_caller and always passes the needed
              pt_regs structure and function_trace_op parameters unconditionally.

              Besides that also use asm offsets to correctly allocate and access the new
              struct pt_regs on the stack.

              While at it we can make use of new instruction to get rid of some indirect
              loads if compiled for new machines.

              The passed struct pt_regs can be changed by the called function and it's new
              contents will replace the current contents.

              Note: to change the return address the embedded psw member of the pt_regs
              structure must be changed. The psw member is right now incomplete, since
              the mask part is missing. For all current use cases this should be sufficent.
              Providing and restoring a sane mask would mean we need to add an epsw/lpswe
              pair to the mcount code. Only these two instruction would cost us ~120 cycles
              which currently seems not necessary.

              Cc: Vojtech Pavlik <vojtech@suse.cz>
              Cc: Jiri Kosina <jkosina@suse.cz>
              Cc: Jiri Slaby <jslaby@suse.cz>
              Cc: Steven Rostedt <rostedt@goodmis.org>
              Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
              Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>


Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
Acked-by: John Jolly <jjolly@suse.de>
---
 arch/s390/Kconfig              |    1 
 arch/s390/include/asm/ftrace.h |    4 +++
 arch/s390/kernel/ftrace.c      |    7 ++++++
 arch/s390/kernel/mcount64.S    |   43 +++++++++++++++++++++++++++++------------
 4 files changed, 43 insertions(+), 12 deletions(-)

--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -24,4 +24,8 @@ static inline unsigned long ftrace_call_
 #define MCOUNT_INSN_SIZE  22
 #endif
 
+#ifdef CONFIG_64BIT
+#define ARCH_SUPPORTS_FTRACE_OPS 1
+#endif
+
 #endif /* _ASM_S390_FTRACE_H */
--- a/arch/s390/Kconfig
+++ b/arch/s390/Kconfig
@@ -112,6 +112,7 @@ config S390
 	select HAVE_C_RECORDMCOUNT
 	select HAVE_DEBUG_KMEMLEAK
 	select HAVE_DYNAMIC_FTRACE
+	select HAVE_DYNAMIC_FTRACE_WITH_REGS if 64BIT
 	select HAVE_FTRACE_MCOUNT_RECORD
 	select HAVE_FUNCTION_GRAPH_TRACER
 	select HAVE_FUNCTION_TRACER
--- a/arch/s390/kernel/ftrace.c
+++ b/arch/s390/kernel/ftrace.c
@@ -107,6 +107,13 @@ asm(
 
 #endif /* CONFIG_64BIT */
 
+#ifdef CONFIG_64BIT
+int ftrace_modify_call(struct dyn_ftrace *rec, unsigned long old_addr,
+		       unsigned long addr)
+{
+	return 0;
+}
+#endif
 
 int ftrace_make_nop(struct module *mod, struct dyn_ftrace *rec,
 		    unsigned long addr)
--- a/arch/s390/kernel/mcount64.S
+++ b/arch/s390/kernel/mcount64.S
@@ -8,28 +8,47 @@
 #include <linux/linkage.h>
 #include <asm/asm-offsets.h>
 #include <asm/ftrace.h>
+#include <asm/ptrace.h>
 
 	.section .kprobes.text, "ax"
 
 ENTRY(ftrace_stub)
 	br	%r14
 
+#define STACK_FRAME_SIZE  (STACK_FRAME_OVERHEAD + __PT_SIZE)
+#define STACK_PARENT_IP   (STACK_FRAME_SIZE + 8)
+#define STACK_PTREGS	  (STACK_FRAME_OVERHEAD)
+#define STACK_PTREGS_GPRS (STACK_PTREGS + __PT_GPRS)
+#define STACK_PTREGS_PSW  (STACK_PTREGS + __PT_PSW)
+
 ENTRY(_mcount)
 #ifdef CONFIG_DYNAMIC_FTRACE
 	br	%r14
 
 ENTRY(ftrace_caller)
+	.globl	ftrace_regs_caller
+	.set	ftrace_regs_caller,ftrace_caller
 #endif
-	stmg	%r2,%r5,32(%r15)
-	stg	%r14,112(%r15)
 	lgr	%r1,%r15
-	aghi	%r15,-160
+	aghi	%r15,-STACK_FRAME_SIZE
 	stg	%r1,__SF_BACKCHAIN(%r15)
+	stg	%r1,(STACK_PTREGS_GPRS+15*8)(%r15)
+	stmg	%r0,%r13,STACK_PTREGS_GPRS(%r15)
+	stg	%r14,(STACK_PTREGS_PSW+8)(%r15)
+#ifdef CONFIG_HAVE_MARCH_Z196_FEATURES
+	aghik	%r2,%r14,-MCOUNT_INSN_SIZE
+	lgrl	%r4,function_trace_op
+	lgrl	%r14,ftrace_trace_function
+#else
 	lgr	%r2,%r14
-	lg	%r3,168(%r15)
 	aghi	%r2,-MCOUNT_INSN_SIZE
+	larl	%r4,function_trace_op
+	lg	%r4,0(%r4)
 	larl	%r14,ftrace_trace_function
 	lg	%r14,0(%r14)
+#endif
+	lg	%r3,STACK_PARENT_IP(%r15)
+	la	%r5,STACK_PTREGS(%r15)
 	basr	%r14,%r14
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 # The j instruction gets runtime patched to a nop instruction.
@@ -37,16 +56,16 @@ ENTRY(ftrace_caller)
 #	j	.+4
 ENTRY(ftrace_graph_caller)
 	j	ftrace_graph_caller_end
-	lg	%r2,168(%r15)
-	lg	%r3,272(%r15)
+	lg	%r2,STACK_PARENT_IP(%r15)
+	lg	%r3,(STACK_PTREGS_PSW+8)(%r15)
 	brasl	%r14,prepare_ftrace_return
-	stg	%r2,168(%r15)
+	stg	%r2,STACK_PARENT_IP(%r15)
 ftrace_graph_caller_end:
 	.globl	ftrace_graph_caller_end
 #endif
-	aghi	%r15,160
-	lmg	%r2,%r5,32(%r15)
-	lg	%r14,112(%r15)
+	lmg	%r0,%r13,STACK_PTREGS_GPRS(%r15)
+	lg	%r14,(STACK_PTREGS_PSW+8)(%r15)
+	aghi	%r15,STACK_FRAME_SIZE
 	br	%r14
 
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
@@ -54,10 +73,10 @@ ftrace_graph_caller_end:
 ENTRY(return_to_handler)
 	stmg	%r2,%r5,32(%r15)
 	lgr	%r1,%r15
-	aghi	%r15,-160
+	aghi	%r15,-STACK_FRAME_OVERHEAD
 	stg	%r1,__SF_BACKCHAIN(%r15)
 	brasl	%r14,ftrace_return_to_handler
-	aghi	%r15,160
+	aghi	%r15,STACK_FRAME_OVERHEAD
 	lgr	%r14,%r2
 	lmg	%r2,%r5,32(%r15)
 	br	%r14
