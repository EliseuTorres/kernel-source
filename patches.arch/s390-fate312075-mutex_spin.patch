From: Gerald Schaefer <geraldsc@de.ibm.com>
Subject: mutex: Introduce arch_mutex_cpu_relax()
References: bnc#700080,LTC#70031,FATE#312075
Patch-mainline: Yes

Description: The spinning mutex implementation uses cpu_relax() in busy loops
             as a compiler barrier. Depending on the architecture, cpu_relax()
             may do more than needed in this specific mutex spin loops. On
             System z we also give up the time slice of the virtual cpu in
             cpu_relax(), which prevents effective spinning on the mutex.

             This patch replaces cpu_relax() in the spinning mutex code with
             arch_mutex_cpu_relax(), which can be defined by each architecture
             that selects HAVE_ARCH_MUTEX_CPU_RELAX. The default is still
             cpu_relax(), so this patch should not affect other architectures
             than System z for now.

             git 34b133f8e94e39ff3cf4d1c1f67f2e07cdc3d54e
                 fa188ae1657d6edc7963d524ce9a0650fe725242

Acked-by: John Jolly <jjolly@suse.de>
---
 arch/Kconfig                  |    3 +++
 arch/s390/Kconfig             |    2 +-
 arch/s390/include/asm/mutex.h |    2 ++
 include/linux/mutex.h         |    4 ++++
 kernel/mutex.c                |    2 +-
 kernel/sched.c                |    3 ++-
 6 files changed, 13 insertions(+), 3 deletions(-)

--- a/arch/Kconfig
+++ b/arch/Kconfig
@@ -124,6 +124,9 @@ config HAVE_DMA_API_DEBUG
 config HAVE_DEFAULT_NO_SPIN_MUTEXES
 	bool
 
+config HAVE_ARCH_MUTEX_CPU_RELAX
+	bool
+
 source "kernel/gcov/Kconfig"
 
 config FAIR_SLEEPERS_ON_BY_DEFAULT
--- a/arch/s390/Kconfig
+++ b/arch/s390/Kconfig
@@ -87,7 +87,7 @@ config S390
 	select HAVE_SYSCALL_TRACEPOINTS
 	select HAVE_DYNAMIC_FTRACE
 	select HAVE_FUNCTION_GRAPH_TRACER
-	select HAVE_DEFAULT_NO_SPIN_MUTEXES
+	select HAVE_ARCH_MUTEX_CPU_RELAX
 	select HAVE_OPROFILE
 	select HAVE_KPROBES
 	select HAVE_KRETPROBES
--- a/arch/s390/include/asm/mutex.h
+++ b/arch/s390/include/asm/mutex.h
@@ -7,3 +7,5 @@
  */
 
 #include <asm-generic/mutex-dec.h>
+
+#define arch_mutex_cpu_relax()	barrier()
--- a/include/linux/mutex.h
+++ b/include/linux/mutex.h
@@ -152,4 +152,8 @@ extern int mutex_trylock(struct mutex *l
 extern void mutex_unlock(struct mutex *lock);
 extern int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock);
 
+#ifndef CONFIG_HAVE_ARCH_MUTEX_CPU_RELAX
+#define arch_mutex_cpu_relax()	cpu_relax()
+#endif
+
 #endif
--- a/kernel/mutex.c
+++ b/kernel/mutex.c
@@ -209,7 +209,7 @@ __mutex_lock_common(struct mutex *lock,
 		 * memory barriers as we'll eventually observe the right
 		 * values at the cost of a few extra spins.
 		 */
-		cpu_relax();
+		arch_mutex_cpu_relax();
 	}
 #endif
 	spin_lock_mutex(&lock->wait_lock, flags);
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -75,6 +75,7 @@
 
 #include <asm/tlb.h>
 #include <asm/irq_regs.h>
+#include <asm/mutex.h>
 
 #include "sched_cpupri.h"
 
@@ -5900,7 +5901,7 @@ int mutex_spin_on_owner(struct mutex *lo
 		if (task_thread_info(rq->curr) != owner || need_resched())
 			return 0;
 
-		cpu_relax();
+		arch_mutex_cpu_relax();
 	}
 
 	return 1;
