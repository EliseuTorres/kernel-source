From: Mike Travis <travis@sgi.com>
Subject: UV: Optimize UV NMIs
References: bnc#744655
Patch-mainline: Submitted 2 Feb 2012

In order to determine if an incoming NMI was generated externally
by the BMC, the UV BIOS sets a flag in an MMR register.  Reading
this register though is expensive especially when all CPU's on
a blade share the same MMR.  This patch reduces the number of
readers to only the first CPU to enter the NMI handler.  The other
problem was all CPUs on a blade were clearing the flag, when only
one of the needs to rewrite the MMR.

This greatly reduces the overhead to enter either KDB or print the
trace, and also removes the possibility of some of the CPUs not
entering into KDB correctly, leaving their state somewhat undefined,
as some where not entering after KDB generated an IPI NMI.

Signed-off-by: Mike Travis <travis@sgi.com>
Acked-by: Jeff Mahoney <jeffm@suse.com>
---
 arch/x86/kernel/apic/x2apic_uv_x.c |  103 ++++++++++++++++++++++++++-----------
 1 file changed, 74 insertions(+), 29 deletions(-)

--- linux-3.0.orig/arch/x86/kernel/apic/x2apic_uv_x.c
+++ linux-3.0/arch/x86/kernel/apic/x2apic_uv_x.c
@@ -44,7 +44,18 @@
 #define UVH_NMI_MMR				UVH_SCRATCH5
 #define UVH_NMI_MMR_CLEAR			(UVH_NMI_MMR + 8)
 #define UV_NMI_PENDING_MASK			(1UL << 63)
-DEFINE_PER_CPU(unsigned long, cpu_last_nmi_count);
+
+static inline int check_nmi_mmr(void)
+{
+	return (uv_read_local_mmr(UVH_NMI_MMR & UV_NMI_PENDING_MASK) != 0);
+}
+
+static inline void clear_nmi_mmr(void)
+{
+	uv_write_local_mmr(UVH_NMI_MMR_CLEAR, UV_NMI_PENDING_MASK);
+}
+
+//static DEFINE_PER_CPU(unsigned long, cpu_last_nmi_count);
 
 DEFINE_PER_CPU(int, x2apic_extra_bits);
 
@@ -58,6 +69,7 @@ EXPORT_SYMBOL_GPL(uv_min_hub_revision_id
 unsigned int uv_apicid_hibits;
 EXPORT_SYMBOL_GPL(uv_apicid_hibits);
 static DEFINE_SPINLOCK(uv_nmi_lock);
+static DEFINE_SPINLOCK(uv_nmi_reason_lock);
 
 static struct apic apic_x2apic_uv_x;
 
@@ -672,6 +684,8 @@ void __cpuinit uv_cpu_init(void)
 		set_x2apic_extra_bits(uv_hub_info->pnode);
 }
 
+static DEFINE_PER_CPU(unsigned long, cpu_nmi_count);
+
 /*
  * When an NMI from the BMC is received:
  * 	- call KDB if active
@@ -682,9 +696,9 @@ int uv_handle_nmi(struct notifier_block
  	struct die_args *args = data;
  	struct pt_regs *regs = args->regs;
  	static int controlling_cpu = -1;
-	unsigned long real_uv_nmi;
+	static int in_uv_nmi = -1;
+	static atomic_t nr_nmi_cpus;
 	int bid, handled = 0;
-	int saved_console_loglevel;
 
 	if (reason != DIE_NMIUNKNOWN && reason != DIE_NMI)
 		return NOTIFY_OK;
@@ -693,18 +707,40 @@ int uv_handle_nmi(struct notifier_block
 		/* do nothing if entering the crash kernel */
 		return NOTIFY_OK;
 
+	/* note which NMI this one is */
+	percpu_inc(cpu_nmi_count);
+
 	/*
 	 * Each blade has an MMR that indicates when an NMI has been sent
 	 * to cpus on the blade. If an NMI is detected, atomically
 	 * clear the MMR and update a per-blade NMI count used to
 	 * cause each cpu on the blade to notice a new NMI.
+	 *
+	 * We optimize access to the MMR as the read operation is expensive
+	 * and only the first CPU to enter this function needs to read the
+	 * register and set a flag indicating we are indeed servicing an
+	 * external BMC NMI.
+	 *
+	 * Once it's determined whether or not this is a real NMI, the last
+	 * responding CPU clears the flag for the next NMI.
 	 */
-	bid = uv_numa_blade_id();
-	real_uv_nmi = (uv_read_local_mmr(UVH_NMI_MMR) & UV_NMI_PENDING_MASK);
+	if (in_uv_nmi < 0) {
+		spin_lock(&uv_nmi_reason_lock);
+		if (in_uv_nmi < 0) {
+			int nc = num_online_cpus();
+			atomic_set(&nr_nmi_cpus, nc);
+			in_uv_nmi = check_nmi_mmr();
+		}
+		spin_unlock(&uv_nmi_reason_lock);
+	}
 
-	if (likely(!real_uv_nmi))
+	if (likely(in_uv_nmi == 0)) {
+		if (atomic_sub_and_test(1, &nr_nmi_cpus) == 0)
+			in_uv_nmi = -1;
 		return NOTIFY_OK;
+	}
 
+	/* If we are here, we are processing a real BMC NMI */
 #ifdef CONFIG_KDB
 	if (kdb_on) {
 		spin_lock(&uv_nmi_lock);
@@ -723,32 +759,41 @@ int uv_handle_nmi(struct notifier_block
  	}
 #endif
 
-	spin_lock(&uv_blade_info[bid].nmi_lock);
-	uv_blade_info[bid].nmi_count++;
-	uv_write_local_mmr(UVH_NMI_MMR_CLEAR, UV_NMI_PENDING_MASK);
-	spin_unlock(&uv_blade_info[bid].nmi_lock);
-
-	if (likely(__get_cpu_var(cpu_last_nmi_count) ==
-						uv_blade_info[bid].nmi_count))
-		return NOTIFY_STOP;
+	/* Only one cpu per blade needs to clear the MMR BMC NMI flag */
+	bid = uv_numa_blade_id();
+	if (uv_blade_info[bid].nmi_count < __get_cpu_var(cpu_nmi_count)) {
+		spin_lock(&uv_blade_info[bid].nmi_lock);
+		if (uv_blade_info[bid].nmi_count < __get_cpu_var(cpu_nmi_count)) {
+			uv_blade_info[bid].nmi_count = __get_cpu_var(cpu_nmi_count);
+			clear_nmi_mmr();
+		}
+		spin_unlock(&uv_blade_info[bid].nmi_lock);
+	}
 
-	if (handled)
-		return NOTIFY_STOP;
+	/* If not handled by KDB, then print process trace on each cpu */
+	if (!handled) {
+		int saved_console_loglevel = console_loglevel;
 
-	__get_cpu_var(cpu_last_nmi_count) = uv_blade_info[bid].nmi_count;
+		/*
+		 * Use a lock so only one cpu prints at a time.
+		 * This prevents intermixed output.  We can reuse the
+		 * uv_nmi_lock since if KDB was called, then all the
+		 * CPUs have exited KDB, and if it was not called,
+		 * then the lock was not used.
+		 */
+		spin_lock(&uv_nmi_lock);
+		pr_err("== UV NMI process trace NMI %lu: ==\n",
+						__get_cpu_var(cpu_nmi_count));
+		regs = args->regs;
+		console_loglevel = 6;
+		show_regs(regs);
+		console_loglevel = saved_console_loglevel;
+		spin_unlock(&uv_nmi_lock);
+	}
 
-	/*
-	 * Use a lock so only one cpu prints at a time.
-	 * This prevents intermixed output.
-	 */
-	spin_lock(&uv_nmi_lock);
-	pr_err("== UV NMI process trace cpu %u: ==\n", smp_processor_id());
-	regs = args->regs;
-	saved_console_loglevel = console_loglevel;
-	console_loglevel = 6;
-	show_regs(regs);
-	console_loglevel = saved_console_loglevel;
-	spin_unlock(&uv_nmi_lock);
+	/* last cpu resets the "in nmi" flag */
+	if (atomic_sub_and_test(1, &nr_nmi_cpus) == 0)
+		in_uv_nmi = -1;
 
 	return NOTIFY_STOP;
 }

