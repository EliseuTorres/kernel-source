From: Mike Travis <travis@sgi.com>
Subject: [PATCH] x86_64, UV: Update NMI handler for UV1000/2000 systems
Date: Wed Jun 13 16:07:57 PDT 2012
References: bnc#746509, bnc#744655
Patch-mainline: Never
Signed-off-by: Tony Jones <tonyj@suse.de>
Patch-replaces: uv-optimize-uv-nmis

	When the CMC 'power nmi" command is issued, it directs all the
	BMCs on each of the blades to set an NMI flag in the local UV
	HUB MMR.  It then signals the blade with the ICH to broadcast
	the NMI signal via QPI/NumaLink.  Because of this asynchronous
	operation, it's possible on very large UV1000 systems for the
	NMI signal to arrive at a blade before the BMC has had a chance
	to set the NMI flag in the MMR.

	In addition, when 'perf top' is running there are literally
	millions of NMIs being generated per second (measured 4-8 million
	with 4096 cpus spread across the 128 blades).  Reading the HUB
	MMR for each of these causes a serious slowdown in the system
	performance as this steals time from the HUB's primary task of
	directing NumaLink traffic by causing extra packets to be
	sent to and reply received for these QPI transactions.

	We alleviate the problem by registering our notifier at a lower
	priority than PERF so it will "handle" almost all of the NMI
	events occuring.  PERF does have a problem with back to back
	NMIs on the same cpu, so it processes these via the NMIUNKNOWN
	notifier chain.

	Therefore we only register the UV NMI handler on the NMIUNKNOWN
	chain so PERF has had a chance to "handle" the missed NMI
	perf events prior to entering the UV NMI handler.

	On the off chance that a blade missed setting the NMI flag in
	the HUB MMR, we use a system-wide NMI flag in global memory that
	is set by the first cpu to detect the UV NMI signal.  All other
	flags are kept on the local node to lessen NumaLink overhead.

Signed-of-by: Mike Travis <travis@sgi.com>

---
 arch/x86/include/asm/uv/uv_hub.h   |   18 ++-
 arch/x86/kernel/apic/x2apic_uv_x.c |  201 ++++++++++++++++++++++++++++++-------
 2 files changed, 181 insertions(+), 38 deletions(-)

--- linux.orig/arch/x86/include/asm/uv/uv_hub.h
+++ linux/arch/x86/include/asm/uv/uv_hub.h
@@ -475,8 +475,8 @@ struct uv_blade_info {
 	unsigned short	nr_online_cpus;
 	unsigned short	pnode;
 	short		memory_nid;
-	spinlock_t	nmi_lock;
-	unsigned long	nmi_count;
+	spinlock_t	nmi_lock;	/* obsolete, see uv_hub_nmi */
+	unsigned long	nmi_count;	/* obsolete, see uv_hub_nmi */
 };
 extern struct uv_blade_info *uv_blade_info;
 extern short *uv_node_to_blade;
@@ -549,6 +549,20 @@ static inline int uv_num_possible_blades
 	return uv_possible_blades;
 }
 
+/*
+ * Support NMIs on a per hub basis
+ * This avoids excessive NUMA traffic when many NMIs are occurring.
+ */
+struct uv_hub_nmi {
+	spinlock_t	nmi_lock;	/* locks this structure */
+	atomic_t	in_nmi;		/* flag this node in UV NMI IRQ */
+	atomic_t	nmi_queries;	/* count of NMI queries of this hub */
+	atomic_t	read_mmr_count;	/* count of MMR reads */
+	atomic_t	nmi_count;	/* count of true UV NMIs */
+	unsigned long	nmi_value;	/* last value read from NMI MMR */
+	int		cpu_owner;	/* last locker of this struct */
+};
+
 /* Update SCIR state */
 static inline void uv_set_scir_bits(unsigned char value)
 {
--- linux.orig/arch/x86/kernel/apic/x2apic_uv_x.c
+++ linux/arch/x86/kernel/apic/x2apic_uv_x.c
@@ -26,6 +26,7 @@
 #include <linux/delay.h>
 #include <linux/crash_dump.h>
 #include <linux/lkdb.h>
+#include <linux/nmi.h>
 
 #include <asm/uv/uv_mmrs.h>
 #include <asm/uv/uv_hub.h>
@@ -44,7 +45,6 @@
 #define UVH_NMI_MMR				UVH_SCRATCH5
 #define UVH_NMI_MMR_CLEAR			(UVH_NMI_MMR + 8)
 #define UV_NMI_PENDING_MASK			(1UL << 63)
-DEFINE_PER_CPU(unsigned long, cpu_last_nmi_count);
 
 DEFINE_PER_CPU(int, x2apic_extra_bits);
 
@@ -60,6 +60,7 @@ EXPORT_SYMBOL_GPL(uv_apicid_hibits);
 static DEFINE_SPINLOCK(uv_nmi_lock);
 
 static struct apic apic_x2apic_uv_x;
+static struct uv_hub_nmi **hub_nmi_list;
 
 static unsigned long __init uv_early_read_mmr(unsigned long addr)
 {
@@ -673,48 +674,163 @@ void __cpuinit uv_cpu_init(void)
 }
 
 /*
- * When an NMI from the BMC is received:
+ * UV handler for NMI
+ *
+ * Handle the system-wide NMI event generated by the CMC 'power nmi' command.
+ *
+ */
+
+/* system global flags */
+static atomic_t	in_uv_nmi;		/* system global NMI flag */
+static unsigned long uv_nmi_count;	/* UV NMI sequence number */
+static int uv_nmi_cpu = -1;		/* cpu first discovering the UV NMI */
+
+/* UV NMI sequence number per cpu */
+static DEFINE_PER_CPU(unsigned long, uv_cpu_last_nmi_count);
+
+/* this cpu to hub nmi info pointer */
+static DEFINE_PER_CPU(struct uv_hub_nmi *, __uv_hub_nmi);
+static inline struct uv_hub_nmi *cpu_hub_nmi(void)
+{
+	return __this_cpu_read(__uv_hub_nmi);
+}
+
+/* Read MMR and check if NMI was set by BMC */
+static inline int uv_mmr_nmi_set(struct uv_hub_nmi *hub_nmi)
+{
+	hub_nmi->nmi_value = uv_read_local_mmr(UVH_NMI_MMR);
+
+	atomic_inc(&hub_nmi->read_mmr_count);
+
+	return !!(hub_nmi->nmi_value & UV_NMI_PENDING_MASK);
+}
+
+/* Set local hub "in nmi" flag, returns non-zero if first */
+static inline int uv_set_in_nmi(struct uv_hub_nmi *hub_nmi)
+{
+	int first = atomic_add_unless(&hub_nmi->in_nmi, 1, 1);
+
+	if (first)
+		atomic_inc(&hub_nmi->nmi_count);
+
+	return first;
+}
+
+/* Check if this is a UV global NMI */
+static int uv_check_nmi(struct uv_hub_nmi *hub_nmi)
+{
+	int cpu = smp_processor_id();
+	int nmi = 0;
+	unsigned long flags;
+
+	local_irq_save(flags);
+
+	atomic_inc(&hub_nmi->nmi_queries);
+	do {
+		/* check hub in_nmi flag */
+		nmi = atomic_read(&hub_nmi->in_nmi);
+		if (nmi)
+			break;
+
+		spin_lock(&hub_nmi->nmi_lock);
+		hub_nmi->cpu_owner = cpu;
+
+		/* check hub in_nmi flag (locked) */
+		nmi = atomic_read(&hub_nmi->in_nmi);
+		if (nmi) {
+			spin_unlock(&hub_nmi->nmi_lock);
+			break;
+		}
+
+		/* check hub MMR NMI flag */
+		if (uv_mmr_nmi_set(hub_nmi)) {
+			nmi = 1;
+			if (uv_set_in_nmi(hub_nmi) &&
+				atomic_add_unless(&in_uv_nmi, 1, 1)) {
+				uv_nmi_cpu = cpu;
+				uv_nmi_count++;
+			}
+		}
+
+		/*
+		 * check system-wide in_uv_nmi flag
+		 * (this check because on large UV1000 systems, the NMI signal
+		 * may arrive before the BMC has set this hub's NMI flag in
+		 * the MMR.)
+		 */
+		if (!nmi) {
+			if ((nmi = atomic_read(&in_uv_nmi)))
+				uv_set_in_nmi(hub_nmi);
+		}
+
+		spin_unlock(&hub_nmi->nmi_lock);
+	} while (0);
+
+	local_irq_restore(flags);
+	return nmi;
+}
+
+/* Need to reset the NMI MMR register, but only once per blade. */
+static inline void uv_clear_nmi(void)
+{
+	struct uv_hub_nmi *hub_nmi = cpu_hub_nmi();
+	unsigned long flags;
+	int in_nmi;
+
+	local_irq_save(flags);
+
+	in_nmi = (atomic_dec_if_positive(&hub_nmi->in_nmi) == 0);
+	if (in_nmi)
+		uv_write_local_mmr(UVH_NMI_MMR_CLEAR, UV_NMI_PENDING_MASK);
+
+	local_irq_restore(flags);
+}
+
+/*
+ * UV NMI handler
  * 	- call KDB if active
- * 	- print a stack trace if kdb is not active.
+ * 	- print a stack trace for each cpu if kdb is not active.
+ *
+ * We only respond on the DIE_NMIUNKOWN chain to allow perf to handle any
+ * back to back NMIs that it may have missed.  This prevents false positives,
+ * which causes us to needlessly read the NMI MMR adding unecessary traffic
+ * to the UV HUB, and thus degrading the performance of live NumaLink traffic.
+ *
  */
 int uv_handle_nmi(struct notifier_block *self, unsigned long reason, void *data)
 {
  	struct die_args *args = data;
  	struct pt_regs *regs = args->regs;
- 	static int controlling_cpu = -1;
-	unsigned long real_uv_nmi;
-	int bid, handled = 0;
 	int saved_console_loglevel;
-
-	if (reason != DIE_NMIUNKNOWN && reason != DIE_NMI)
-		return NOTIFY_OK;
+	struct uv_hub_nmi *hub_nmi = cpu_hub_nmi();
+	int cpu = smp_processor_id();
+	int handled = 0;
 
 	if (in_crash_kexec)
 		/* do nothing if entering the crash kernel */
 		return NOTIFY_OK;
 
-	/*
-	 * Each blade has an MMR that indicates when an NMI has been sent
-	 * to cpus on the blade. If an NMI is detected, atomically
-	 * clear the MMR and update a per-blade NMI count used to
-	 * cause each cpu on the blade to notice a new NMI.
-	 */
-	bid = uv_numa_blade_id();
-	real_uv_nmi = (uv_read_local_mmr(UVH_NMI_MMR) & UV_NMI_PENDING_MASK);
-
-	if (likely(!real_uv_nmi))
+	if (reason != DIE_NMIUNKNOWN || !uv_check_nmi(hub_nmi))
+		/* not UV global system NMI */
 		return NOTIFY_OK;
 
+	/* track sequential NMI events */
+	percpu_write(uv_cpu_last_nmi_count, atomic_read(&hub_nmi->nmi_count));
+
 #ifdef CONFIG_KDB
 	if (kdb_on) {
-		spin_lock(&uv_nmi_lock);
+		static int controlling_cpu = -1;
+		unsigned long flags;
+
+		spin_lock_irqsave(&uv_nmi_lock, flags);
 		if (controlling_cpu == -1) {
-			controlling_cpu = smp_processor_id();
-			spin_unlock(&uv_nmi_lock);
+			controlling_cpu = cpu;
+			spin_unlock_irqrestore(&uv_nmi_lock, flags);
 			(void)kdb(LKDB_REASON_NMI, reason, regs);
+			atomic_set(&in_uv_nmi, 0);
 			controlling_cpu = -1;
 		} else {
-			spin_unlock(&uv_nmi_lock);
+			spin_unlock_irqrestore(&uv_nmi_lock, flags);
 			(void)kdb(LKDB_REASON_ENTER_SLAVE, reason, regs);
 			while (controlling_cpu != -1)
 				cpu_relax();
@@ -723,27 +839,24 @@ int uv_handle_nmi(struct notifier_block
  	}
 #endif
 
-	spin_lock(&uv_blade_info[bid].nmi_lock);
-	uv_blade_info[bid].nmi_count++;
-	uv_write_local_mmr(UVH_NMI_MMR_CLEAR, UV_NMI_PENDING_MASK);
-	spin_unlock(&uv_blade_info[bid].nmi_lock);
-
-	if (likely(__get_cpu_var(cpu_last_nmi_count) ==
-						uv_blade_info[bid].nmi_count))
-		return NOTIFY_STOP;
-
+	/* Clear MMR NMI flag on each blade */
+	uv_clear_nmi();
 	if (handled)
 		return NOTIFY_STOP;
 
-	__get_cpu_var(cpu_last_nmi_count) = uv_blade_info[bid].nmi_count;
-
 	/*
 	 * Use a lock so only one cpu prints at a time.
 	 * This prevents intermixed output.
 	 */
 	spin_lock(&uv_nmi_lock);
-	pr_err("== UV NMI process trace cpu %u: ==\n", smp_processor_id());
-	regs = args->regs;
+
+	/* clear global NMI flag, since not cleared above */
+	if (cpu == uv_nmi_cpu) {
+		atomic_set(&in_uv_nmi, 0);
+		uv_nmi_cpu = -1;
+	}
+
+	pr_err("== UV NMI process trace cpu %u: ==\n", cpu);
 	saved_console_loglevel = console_loglevel;
 	console_loglevel = 6;
 	show_regs(regs);
@@ -909,6 +1022,22 @@ void __init uv_system_init(void)
 		uv_node_to_blade[nid] = blade;
 	}
 
+	/* Setup hub nmi info */
+	bytes = sizeof(void *) * (1 << NODES_SHIFT);
+	hub_nmi_list = kzalloc(bytes, GFP_KERNEL);
+	pr_info("UV: NMI hub list @ 0x%p (%d)\n", hub_nmi_list, bytes);
+	BUG_ON(!hub_nmi_list);
+	bytes = sizeof(struct uv_hub_nmi);
+	for_each_present_cpu(cpu) {
+		nid = cpu_to_node(cpu);
+		if (hub_nmi_list[nid] == NULL) {
+			hub_nmi_list[nid] = kzalloc_node(bytes,
+							GFP_KERNEL, nid);
+			BUG_ON(!hub_nmi_list[nid]);
+		}
+		per_cpu(__uv_hub_nmi, cpu) = hub_nmi_list[nid];
+	}
+
 	map_gru_high(max_pnode);
 	map_mmr_high(max_pnode);
 	map_mmioh_high(max_pnode & pnode_io_mask);
