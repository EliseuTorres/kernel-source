From: Heiko Carstens <heiko.carstens@de.ibm.com>
Subject: s390/ftrace: enforce DYNAMIC_FTRACE if FUNCTION_TRACER is selected
Patch-mainline: v3.18-rc1
Git-commit: 5d6a0163494c78ad7b6de733c8793e66b5da9212
References: bnc#903279, LTC#118177

Description:  kernel: reduce function tracer overhead
Symptom:      The kernel uses more cpu cycles for each function being
              executed.
Problem:      The kGraft feature requires to instrument the kernel. In
              order to do that the kernel gets compiled with the function
              tracer enabled which causes the compiler to emit code that
              adds an "mcount" call to the prologue of each function.
              This code will be modified by the kernel for function tracing.
              However the implementation was not optimal, since even if
              disabled each function stored a value on the stack and
              afterwards contained an unconditional branch which skipped
              the rest of the mcount prologue code.
              So more instructions than necessary will be executed which
              results in a reduced performance.
Solution:     Patch the mcount prologue code so that only a single
              instruction of the mcount code will be executed. It's either
              a branch that skips the rest of the mcount prologue code or
              a branch to the function tracer.
Reproduction: Compile the kernel with and without function tracer enabled
              and compare cpu time spent in the kernel for identical
              workloads.

Upstream-Description:

              s390/ftrace: enforce DYNAMIC_FTRACE if FUNCTION_TRACER is selected

              We have too many combinations for function tracing. Lets simply stick to
              the most advanced option, so we don't have to care of other combinations.

              This means we always select DYNAMIC_FTRACE if FUNCTION_TRACER is selected.

              In the s390 Makefile also remove CONFIG_FTRACE_SYSCALLS since that
              functionality got moved to architecture independent code in the meantime.

              Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
              Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>


Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
Acked-by: John Jolly <jjolly@suse.de>
---
 arch/s390/Kconfig           |    1 +
 arch/s390/kernel/Makefile   |    4 +---
 arch/s390/kernel/ftrace.c   |    6 ------
 arch/s390/kernel/mcount.S   |    2 --
 arch/s390/kernel/mcount64.S |    2 --
 5 files changed, 2 insertions(+), 13 deletions(-)

--- a/arch/s390/Kconfig
+++ b/arch/s390/Kconfig
@@ -97,6 +97,7 @@ config S390
 	select ARCH_WANT_IPC_PARSE_VERSION
 	select BUILDTIME_EXTABLE_SORT
 	select CLONE_BACKWARDS2
+	select DYNAMIC_FTRACE if FUNCTION_TRACER
 	select GENERIC_CLOCKEVENTS
 	select GENERIC_CPU_DEVICES if !SMP
 	select GENERIC_SMP_IDLE_THREAD
--- a/arch/s390/kernel/ftrace.c
+++ b/arch/s390/kernel/ftrace.c
@@ -17,8 +17,6 @@
 #include <asm/asm-offsets.h>
 #include "entry.h"
 
-#ifdef CONFIG_DYNAMIC_FTRACE
-
 void ftrace_disable_code(void);
 void ftrace_enable_insn(void);
 
@@ -143,8 +141,6 @@ int __init ftrace_dyn_arch_init(void *da
 	return 0;
 }
 
-#endif /* CONFIG_DYNAMIC_FTRACE */
-
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 /*
  * Hook the return address and push it in the stack of return addresses
@@ -171,7 +167,6 @@ out:
 	return parent;
 }
 
-#ifdef CONFIG_DYNAMIC_FTRACE
 /*
  * Patch the kernel code at ftrace_graph_caller location. The instruction
  * there is branch relative and save to prepare_ftrace_return. To disable
@@ -221,5 +216,4 @@ int ftrace_disable_ftrace_graph_caller(v
 }
 
 #endif /* CONFIG_64BIT */
-#endif /* CONFIG_DYNAMIC_FTRACE */
 #endif /* CONFIG_FUNCTION_GRAPH_TRACER */
--- a/arch/s390/kernel/Makefile
+++ b/arch/s390/kernel/Makefile
@@ -54,9 +54,7 @@ obj-$(CONFIG_COMPAT)		+= compat_linux.o
 obj-$(CONFIG_STACKTRACE)	+= stacktrace.o
 obj-$(CONFIG_KPROBES)		+= kprobes.o
 obj-$(CONFIG_FUNCTION_TRACER)	+= $(if $(CONFIG_64BIT),mcount64.o,mcount.o)
-obj-$(CONFIG_DYNAMIC_FTRACE)	+= ftrace.o
-obj-$(CONFIG_FUNCTION_GRAPH_TRACER) += ftrace.o
-obj-$(CONFIG_FTRACE_SYSCALLS)  += ftrace.o
+obj-$(CONFIG_FUNCTION_TRACER)	+= ftrace.o
 obj-$(CONFIG_CRASH_DUMP)	+= crash_dump.o
 
 ifdef CONFIG_64BIT
--- a/arch/s390/kernel/mcount64.S
+++ b/arch/s390/kernel/mcount64.S
@@ -22,13 +22,11 @@ ENTRY(ftrace_stub)
 #define STACK_PTREGS_PSW  (STACK_PTREGS + __PT_PSW)
 
 ENTRY(_mcount)
-#ifdef CONFIG_DYNAMIC_FTRACE
 	br	%r14
 
 ENTRY(ftrace_caller)
 	.globl	ftrace_regs_caller
 	.set	ftrace_regs_caller,ftrace_caller
-#endif
 	lgr	%r1,%r15
 	aghi	%r15,-STACK_FRAME_SIZE
 	stg	%r1,__SF_BACKCHAIN(%r15)
--- a/arch/s390/kernel/mcount.S
+++ b/arch/s390/kernel/mcount.S
@@ -15,11 +15,9 @@ ENTRY(ftrace_stub)
 	br	%r14
 
 ENTRY(_mcount)
-#ifdef CONFIG_DYNAMIC_FTRACE
 	br	%r14
 
 ENTRY(ftrace_caller)
-#endif
 	stm	%r2,%r5,16(%r15)
 	bras	%r1,1f
 0:	.long	ftrace_trace_function
