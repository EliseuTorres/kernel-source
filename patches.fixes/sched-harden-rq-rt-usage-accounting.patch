Subject: Sched: harden rq rt usage accounting
From: Mike Galbraith <mgalbraith@suse.de>
Date: Mon Dec 10 09:45:36 CET 2012
Patch-mainline: never
References: bnc#769685, bnc#788590

Try to survive should rt usage tracking go goofy.  In bnc#769685, the value of
available (RAX: 00000400000002c8) looks suspiciously similar to a bit flip, but
in any case, try to survive.  In the event of a timewarp, or should a cpu
manage to not run an rt task or be ticked for long enough, values could become
very large.  In bnc#788590, a TSC lept very far forward, causing rq->clock to
leap past rq->age_stamp, killing tracking, making rt usage to appear to grow
endlessly.  Any such event leading to huge values risks overflowing 32 bit
divisor in scale_rt_power(), so ensure values are scalable, and if a timewarp
occurs, ensure that rt usage tracking survives it.

Signed-off-by: Mike Galbraith <mgalbraith@suse.de>
---
 kernel/sched.c      |    7 +++++++
 kernel/sched_fair.c |   32 +++++++++++++++++++++++---------
 2 files changed, 30 insertions(+), 9 deletions(-)

--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -3803,6 +3803,13 @@ static void update_cpu_load(struct rq *t
 	}
 
 	sched_avg_update(this_rq);
+
+#ifdef CONFIG_SMP
+	if (unlikely(this_rq->clock - this_rq->age_stamp > sched_avg_period())) {
+		this_rq->age_stamp = this_rq->clock;
+		this_rq->rt_avg = 0;
+	}
+#endif
 }
 
 static void update_cpu_load_active(struct rq *this_rq)
--- a/kernel/sched_fair.c
+++ b/kernel/sched_fair.c
@@ -3293,7 +3293,7 @@ unsigned long __weak arch_scale_smt_powe
 unsigned long scale_rt_power(int cpu)
 {
 	struct rq *rq = cpu_rq(cpu);
-	u64 total, available, age_stamp, avg;
+	u64 total, available, age_stamp, avg, clock;
 
 	/*
 	 * Since we're reading these variables without serialization make sure
@@ -3301,20 +3301,34 @@ unsigned long scale_rt_power(int cpu)
 	 */
 	age_stamp = ACCESS_ONCE(rq->age_stamp);
 	avg = ACCESS_ONCE(rq->rt_avg);
+	clock = ACCESS_ONCE(rq->clock);
 
-	total = sched_avg_period() + (rq->clock - age_stamp);
+	total = available = sched_avg_period() + (clock - age_stamp);
+	total >>= SCHED_POWER_SHIFT;
 
-	if (unlikely(total < avg)) {
-		/* Ensures that power won't end up being negative */
-		available = 0;
-	} else {
-		available = total - avg;
+	/* RT usage tracking looks fishy, report anomaly and restore sanity */
+	if (unlikely(total > (u32)~0)) {
+		if (printk_ratelimit())
+			printk(KERN_ERR "scale_rt_power: clock:%Lx age:%Lx, avg:%Lx\n",
+		               clock, age_stamp, avg);
+		return SCHED_LOAD_SCALE;
 	}
 
-	if (unlikely((s64)total < SCHED_POWER_SCALE))
+	/* Our RT usage is minimal, don't bother */
+	if (avg < total)
+		return SCHED_LOAD_SCALE;
+
+	available -= avg;
+
+	if (unlikely(total < SCHED_LOAD_SCALE))
 		total = SCHED_POWER_SCALE;
 
-	total >>= SCHED_POWER_SHIFT;
+	/*
+	 * We're fully RT bound.  Note: avg > available is possible given no
+	 * synchronization, especially in a heavily loaded -rt kernel, cast.
+	 */
+	if (unlikely((s64)available < 2*total))
+		return 1;
 
 	return div_u64(available, total);
 }
