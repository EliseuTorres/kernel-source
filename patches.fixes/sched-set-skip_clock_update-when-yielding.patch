Subject: sched: Set skip_clock_update in yield_task_fair()
From: Mike Galbraith <mgalbraith@suse.de>
Date: Tue, 22 Nov 2011 15:21:26 +0100
Git-commit: 916671c08b7808aebec87cc56c85788e665b3c6b
Patch-mainline: v3.3-rc1
References: Scheduler enhancements for I7 (bnc#754690)

This is another case where we are on our way to schedule(),
so can save a useless clock update and resulting microscopic
vruntime update.

Signed-off-by: Mike Galbraith <efault@gmx.de>
Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
Link: http://lkml.kernel.org/r/1321971686.6855.18.camel@marge.simson.net
Signed-off-by: Ingo Molnar <mingo@elte.hu>
Acked-by: Mike Galbraith <mgalbraith@suse.de>

---
 kernel/sched.c      |    8 +++++++-
 kernel/sched_fair.c |    6 ++++++
 2 files changed, 13 insertions(+), 1 deletion(-)

--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -5826,7 +5826,13 @@ bool __sched yield_to(struct task_struct
 		 */
 		if (preempt && rq != p_rq)
 			resched_task(p_rq->curr);
-	}
+	} else
+		/*
+		 * We might have set it in task_yield_fair(), but are
+		 * not going to schedule(), so don't want to skip
+		 * the next update.
+		 */
+		rq->skip_clock_update = 0;
 
 out:
 	double_rq_unlock(rq, p_rq);
--- a/kernel/sched_fair.c
+++ b/kernel/sched_fair.c
@@ -2657,6 +2657,12 @@ static void yield_task_fair(struct rq *r
 		 * Update run-time statistics of the 'current'.
 		 */
 		update_curr(cfs_rq);
+		/*
+		 * Tell update_rq_clock() that we've just updated,
+		 * so we don't do microscopic update in schedule()
+		 * and double the fastpath cost.
+		 */
+		 rq->skip_clock_update = 1;
 	}
 
 	set_skip_buddy(se);
