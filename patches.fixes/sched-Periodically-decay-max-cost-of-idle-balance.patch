Subject: sched/balancing: Periodically decay max cost of idle balance
From: Jason Low <jason.low2@hp.com>
Date: Fri, 13 Sep 2013 11:26:53 -0700
Git-commit: f48627e686a69f5215cb0761e731edb3d9859dd9
Patch-mainline: v3.12
References: bnc#849256

This patch builds on patch 2 and periodically decays that max value to
do idle balancing per sched domain by approximately 1% per second. Also
decay the rq's max_idle_balance_cost value.

Signed-off-by: Jason Low <jason.low2@hp.com>
Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1379096813-3032-4-git-send-email-jason.low2@hp.com
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Acked-by: Mike Galbraith <mgalbraith@suse.de>
---
 arch/x86/include/asm/topology.h |    1 +
 include/linux/sched.h           |    2 ++
 include/linux/topology.h        |    4 ++++
 kernel/sched_fair.c             |   34 +++++++++++++++++++++++++++++++++-
 4 files changed, 40 insertions(+), 1 deletion(-)

--- a/arch/x86/include/asm/topology.h
+++ b/arch/x86/include/asm/topology.h
@@ -129,6 +129,7 @@ extern void setup_node_to_cpumask_map(vo
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\
 	.max_newidle_lb_cost	= 0,					\
+	.next_decay_max_lb_cost	= jiffies,				\
 }
 
 #ifdef CONFIG_X86_64
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -970,7 +970,9 @@ struct sched_domain {
 
 	u64 last_update;
 #ifndef __GENKSYMS__
+	/* idle_balance() stats */
 	u64 max_newidle_lb_cost;
+	unsigned long next_decay_max_lb_cost;
 #endif
 
 #ifdef CONFIG_SCHEDSTATS
--- a/include/linux/topology.h
+++ b/include/linux/topology.h
@@ -109,6 +109,7 @@ int arch_update_cpu_topology(void);
 	.balance_interval	= 1,					\
 	.smt_gain		= 1178,	/* 15% */			\
 	.max_newidle_lb_cost	= 0,					\
+	.next_decay_max_lb_cost	= jiffies,				\
 }
 #endif
 #endif /* CONFIG_SCHED_SMT */
@@ -142,6 +143,7 @@ int arch_update_cpu_topology(void);
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\
 	.max_newidle_lb_cost	= 0,					\
+        .next_decay_max_lb_cost = jiffies,                              \
 }
 #endif
 #endif /* CONFIG_SCHED_MC */
@@ -176,6 +178,7 @@ int arch_update_cpu_topology(void);
 	.last_balance		= jiffies,				\
 	.balance_interval	= 1,					\
 	.max_newidle_lb_cost	= 0,					\
+        .next_decay_max_lb_cost = jiffies,                              \
 }
 #endif
 
@@ -203,6 +206,7 @@ int arch_update_cpu_topology(void);
 	.last_balance		= jiffies,				\
 	.balance_interval	= 64,					\
 	.max_newidle_lb_cost	= 0,					\
+        .next_decay_max_lb_cost = jiffies,                              \
 }
 
 #ifdef CONFIG_SCHED_BOOK
--- a/kernel/sched_fair.c
+++ b/kernel/sched_fair.c
@@ -4639,15 +4639,39 @@ static void rebalance_domains(int cpu, e
 	/* Earliest time when we have to do rebalance again */
 	unsigned long next_balance = jiffies + 60*HZ;
 	int update_next_balance = 0;
-	int need_serialize;
+	int need_serialize, need_decay = 0;
+	u64 max_cost = 0;
 
 	update_shares(cpu);
 
 	rcu_read_lock();
 	for_each_domain(cpu, sd) {
+		/*
+		 * Decay the newidle max times here because this is a regular
+		 * visit to all the domains. Decay ~1% per second.
+		 */
+		if (time_after(jiffies, sd->next_decay_max_lb_cost)) {
+			sd->max_newidle_lb_cost =
+				(sd->max_newidle_lb_cost * 253) / 256;
+			sd->next_decay_max_lb_cost = jiffies + HZ;
+			need_decay = 1;
+		}
+		max_cost += sd->max_newidle_lb_cost;
+
 		if (!(sd->flags & SD_LOAD_BALANCE))
 			continue;
 
+		/*
+		 * Stop the load balance at this level. There is another
+		 * CPU in our sched group which is doing load balancing more
+		 * actively.
+		 */
+		if (!balance) {
+			if (need_decay)
+				continue;
+			break;
+		}
+
 		interval = sd->balance_interval;
 		if (idle != CPU_IDLE)
 			interval *= sd->busy_factor;
@@ -4689,6 +4713,14 @@ static void rebalance_domains(int cpu, e
 		if (!balance)
 			break;
 	}
+	if (need_decay) {
+		/*
+		 * Ensure the rq-wide value also decays but keep it at a
+		 * reasonable floor to avoid funnies with rq->avg_idle.
+		 */
+		rq->max_idle_balance_cost =
+			max((u64)sysctl_sched_migration_cost, max_cost);
+	}
 	rcu_read_unlock();
 
 	/*
