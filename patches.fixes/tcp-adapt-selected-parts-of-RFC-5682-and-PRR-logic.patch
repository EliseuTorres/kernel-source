From: Michal Kubecek <mkubecek@suse.cz>
Date: Thu, 3 Jul 2014 13:40:56 +0200
Subject: tcp: adapt selected parts of RFC 5682 and PRR logic
Patch-mainline: Never, mainline (3.10) solution not backportable
References: bnc#879921

Disabling cwnd moderation improves recovery from a single packet
loss but if another loss occurs before recovery from the first
is complete, ssthresh is reduced based on current cwnd which can
still be very low. If reno congestion control is used, cwnd
grows very slowly once it exceeds ssthresh (congestion avoidance
phase) so that letting ssthresh drop very low can be devastating
for the subsequent recovery.

To address this issue, selected parts of revised logic were
picked from RFC 5682 (3.10) and Proportional Rate Reduction
(3.2) implementation:

1. Disallow entering F-RTO from TCP_CA_Recovery state.
2. Allow entering Recovery from F-RTO.
3. Do not return to TCP_CA_Disorder if F-RTO is entered in
   TCP_CA_Loss state.
4. Allow slow-start style cwnd growth in TCP_CA_Recovery state.

To avoid potential regressions, these modifications are active
only if tcp_bnc879921_hack is set to value of 2 or more.

Signed-off-by: Michal Kubecek <mkubecek@suse.cz>
---
 Documentation/networking/ip-sysctl.txt | 28 +++++++------
 net/ipv4/tcp_input.c                   | 74 ++++++++++++++++++++++++++++++----
 2 files changed, 82 insertions(+), 20 deletions(-)

diff --git a/Documentation/networking/ip-sysctl.txt b/Documentation/networking/ip-sysctl.txt
index 973c0d7..b332057 100644
--- a/Documentation/networking/ip-sysctl.txt
+++ b/Documentation/networking/ip-sysctl.txt
@@ -170,18 +170,22 @@ tcp_base_mss - INTEGER
 	Path MTU discovery (MTU probing).  If MTU probing is enabled,
 	this is the initial MSS used by the connection.
 
-tcp_bnc879921_hack - BOOLEAN
-	Disables congestion window moderation when retransmitting lost
-	packets. When set to FALSE (default), cwnd is moderated in TCP_CA_Loss
-	state so that if some packets are lost, transmission is limited to
-	(usually) three packets per roundtrip until all lost packets are
-	retransmitted and acknowledged. In a "long fat network", this can
-	severely throttle the throughput for rather long time: in bnc#879921,
-	a short (0.3 s) drop took up to 40 seconds to recover from. The
-	moderation was removed in 3.10 mainline kernel as part of much bigger
-	change. While the tests show no problems related to cwnd moderation
-	removal, it is sysctl switchable with the original behaviour preserved
-	as default in order to avoid any risk of regressions.
+tcp_bnc879921_hack - INTEGER
+	0 (default) preserves 3.0 congestion control
+
+	1 or higher disables congestion window moderation when retransmitting
+	lost packets. Normally, cwnd is moderated in TCP_CA_Loss state so that
+	if some packets are lost, transmission is limited to (usually) three
+	packets per roundtrip until all lost packets are retransmitted and
+	acknowledged. In a "long fat network", this can severely throttle the
+	throughput for rather long time: in reported case, a short (0.3 s)
+	drop took up to 40 seconds to recover from.
+
+	2 or higher activates selected part of new congestion control logic
+	introduced by Proportional Rate Reduction and RFC 5682 implementation.
+	These improve recovery behaviour if another loss is encountered before
+	the connection is fully recovered from previous one, especially if
+	reno congestion control is used.
 
 tcp_congestion_control - STRING
 	Set the congestion control algorithm to be used for new
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index c071360..e1270a0 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -128,6 +128,16 @@ int sysctl_tcp_bnc879921_hack __read_mostly;
 #define TCP_REMNANT (TCP_FLAG_FIN|TCP_FLAG_URG|TCP_FLAG_SYN|TCP_FLAG_PSH)
 #define TCP_HP_BITS (~(TCP_RESERVED_BITS|TCP_FLAG_PSH))
 
+static inline bool bnc879921_hack_cwnd(void)
+{
+	return unlikely(sysctl_tcp_bnc879921_hack >= 1);
+}
+
+static inline bool bnc879921_hack_frto(void)
+{
+	return unlikely(sysctl_tcp_bnc879921_hack >= 2);
+}
+
 /* Adapt the MSS value used to make delayed ack decision to the
  * real world.
  */
@@ -2032,6 +2042,15 @@ int tcp_use_frto(struct sock *sk)
 	if (icsk->icsk_mtup.probe_size)
 		return 0;
 
+	/* if activated via sysctl, imitate RFC 5682 logic here: do not enter
+	 * F-RTO if recovery is already in progress except for repeated
+	 * timeouts with the same SND.UNA (bnc#879921)
+	 */
+	if (bnc879921_hack_frto() && icsk->icsk_ca_state > TCP_CA_Disorder &&
+	    after(tp->high_seq, tp->snd_una) && !icsk->icsk_retransmits &&
+	    icsk->icsk_ca_state != TCP_CA_Loss)
+		return 0;
+
 	if (tcp_is_sackfrto(tp))
 		return 1;
 
@@ -2086,8 +2105,9 @@ void tcp_enter_frto(struct sock *sk)
 		 * RFC4138 should be more specific on what to do, even though
 		 * RTO is quite unlikely to occur after the first Cumulative ACK
 		 * due to back-off and complexity of triggering events ...
+		 * If RFC 5682 bits are activated, always use the real value.
 		 */
-		if (tp->frto_counter) {
+		if (tp->frto_counter && !bnc879921_hack_frto()) {
 			u32 stored_cwnd;
 			stored_cwnd = tp->snd_cwnd;
 			tp->snd_cwnd = 2;
@@ -2131,7 +2151,11 @@ void tcp_enter_frto(struct sock *sk)
 	} else {
 		tp->frto_highmark = tp->snd_nxt;
 	}
-	tcp_set_ca_state(sk, TCP_CA_Disorder);
+	/* If parts of RFC 5682 logic are activated, do not return to Disorder
+	 * if already in CWR, Recovery or Loss (bnc#879921)
+	 */
+	if (!bnc879921_hack_frto() || icsk->icsk_ca_state == TCP_CA_Open)
+		tcp_set_ca_state(sk, TCP_CA_Disorder);
 	tp->high_seq = tp->snd_nxt;
 	tp->frto_counter = 1;
 }
@@ -2437,8 +2461,11 @@ static int tcp_time_to_recover(struct sock *sk)
 	struct tcp_sock *tp = tcp_sk(sk);
 	__u32 packets_out;
 
-	/* Do not perform any recovery during F-RTO algorithm */
-	if (tp->frto_counter)
+	/* Do not perform any recovery during F-RTO algorithm
+	 * unless parts of RFC 5682 logic are activated via sysctl
+	 * (bnc#879921)
+	 */
+	if (tp->frto_counter && !bnc879921_hack_frto())
 		return 0;
 
 	/* Trick#1: The loss is proven. */
@@ -2642,6 +2669,24 @@ static void tcp_cwnd_down(struct sock *sk, int flag)
 	}
 }
 
+/* Adapt (simplified version of) more efficient cwnd handling
+ * in TCP_CA_Recovery state from PRR implementation.
+ */
+static void bnc879921_tcp_cwnd_reduction(struct tcp_sock *tp,
+					 int prior_unsacked, int fast_rexmit)
+{
+	unsigned int in_flight = tcp_packets_in_flight(tp);
+	int sndcnt = 0;
+	int newly_acked_sacked = prior_unsacked -
+				 (tp->packets_out - tp->sacked_out);
+
+	if (in_flight <= tp->snd_ssthresh)
+		sndcnt = min_t(int, newly_acked_sacked + 1,
+			       tp->snd_ssthresh - in_flight);
+	sndcnt = max_t(int, sndcnt, (fast_rexmit ? 1 : 0));
+	tp->snd_cwnd = in_flight + sndcnt;
+}
+
 /* Nothing was retransmitted or returned timestamp is less
  * than timestamp of the first retransmission.
  */
@@ -2977,7 +3022,8 @@ EXPORT_SYMBOL(tcp_simple_retransmit);
  * It does _not_ decide what to send, it is made in function
  * tcp_xmit_retransmit_queue().
  */
-static void tcp_fastretrans_alert(struct sock *sk, int pkts_acked, int flag)
+static void tcp_fastretrans_alert(struct sock *sk, int pkts_acked, int flag,
+				  const int prior_unsacked)
 {
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	struct tcp_sock *tp = tcp_sk(sk);
@@ -3070,7 +3116,7 @@ static void tcp_fastretrans_alert(struct sock *sk, int pkts_acked, int flag)
 		if (tcp_is_reno(tp) && flag & FLAG_SND_UNA_ADVANCED)
 			tcp_reset_reno_sack(tp);
 		if (!tcp_try_undo_loss(sk)) {
-			if (!sysctl_tcp_bnc879921_hack)
+			if (!bnc879921_hack_cwnd())
 				tcp_moderate_cwnd(tp);
 			tcp_xmit_retransmit_queue(sk);
 			return;
@@ -3114,6 +3160,11 @@ static void tcp_fastretrans_alert(struct sock *sk, int pkts_acked, int flag)
 
 		NET_INC_STATS_BH(sock_net(sk), mib_idx);
 
+		/* If parts of RFC 5682 logic are activated via sysctl, make
+		 * sure we leave F-RTO before entering Recovery (bnc#879921)
+		 */
+		if (bnc879921_hack_frto())
+			tp->frto_counter = 0;
 		tp->high_seq = tp->snd_nxt;
 		tp->prior_ssthresh = 0;
 		tp->undo_marker = tp->snd_una;
@@ -3134,7 +3185,13 @@ static void tcp_fastretrans_alert(struct sock *sk, int pkts_acked, int flag)
 
 	if (do_lost || (tcp_is_fack(tp) && tcp_head_timedout(sk)))
 		tcp_update_scoreboard(sk, fast_rexmit);
-	tcp_cwnd_down(sk, flag);
+	/* If activated via sysctl, use more efficient cwnd handling in
+	 * Recovery (bnc#879921)
+	 */
+	if (bnc879921_hack_frto())
+		bnc879921_tcp_cwnd_reduction(tp, prior_unsacked, fast_rexmit);
+	else
+		tcp_cwnd_down(sk, flag);
 	tcp_xmit_retransmit_queue(sk);
 }
 
@@ -3692,6 +3749,7 @@ static int tcp_ack(struct sock *sk, struct sk_buff *skb, int flag)
 	u32 prior_in_flight;
 	u32 prior_fackets;
 	int prior_packets;
+	const int prior_unsacked = tp->packets_out - tp->sacked_out;
 	int frto_cwnd = 0;
 
 	/* If the ack is older than previous acks
@@ -3787,7 +3845,7 @@ static int tcp_ack(struct sock *sk, struct sk_buff *skb, int flag)
 		    tcp_may_raise_cwnd(sk, flag))
 			tcp_cong_avoid(sk, ack, prior_in_flight);
 		tcp_fastretrans_alert(sk, prior_packets - tp->packets_out,
-				      flag);
+				      flag, prior_unsacked);
 	} else {
 		if ((flag & FLAG_DATA_ACKED) && !frto_cwnd)
 			tcp_cong_avoid(sk, ack, prior_in_flight);
-- 
1.8.4.5

