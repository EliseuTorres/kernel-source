Subject: sched: Only queue remote wakeups when crossing cache boundaries part2
From: Mike Galbraith <efault@gmx.de>
Date: Tue Jun 12 06:14:01 CEST 2012
Patch-mainline: never
References: References: Scheduler enhancements for I7 (bnc#754690)

Cleanup.

Add the rest of 518cd623 to bring in sd_llc_id, and take 39be35012 the
ttwu_share_cache()->cpus_share_cache() rename for later use while here.

Signed-off-by: Mike Galbraith <mgalbraith@suse.de>

---
 kernel/sched.c      |   32 ++++++++++++++++++++------------
 kernel/sched_fair.c |    2 +-
 2 files changed, 21 insertions(+), 13 deletions(-)

--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -1884,6 +1884,9 @@ static inline struct sched_domain *highe
 	return hsd;
 }
 
+DECLARE_PER_CPU(struct sched_domain *, sd_llc);
+DECLARE_PER_CPU(int, sd_llc_id);
+
 #else /* CONFIG_SMP */
 
 /*
@@ -2811,17 +2814,9 @@ static int ttwu_activate_remote(struct t
 }
 #endif /* __ARCH_WANT_INTERRUPTS_ON_CTXSW */
 
-/*
- * Keep a unique ID per domain (we use the first cpu number in
- * the cpumask of the domain), this allows us to quickly tell if
- * two cpus are in the same cache domain, see ttwu_share_cache().
- */
-DEFINE_PER_CPU(int, sd_top_spr_id);
-
-static inline int ttwu_share_cache(int this_cpu, int that_cpu)
+static inline int cpus_share_cache(int this_cpu, int that_cpu)
 {
-	return per_cpu(sd_top_spr_id, this_cpu) ==
-		per_cpu(sd_top_spr_id, that_cpu);
+	return per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu);
 }
 #endif /* CONFIG_SMP */
 
@@ -2830,7 +2825,7 @@ static void ttwu_queue(struct task_struc
 	struct rq *rq = cpu_rq(cpu);
 
 #if defined(CONFIG_SMP)
-	if (sched_feat(TTWU_QUEUE) && !ttwu_share_cache(smp_processor_id(), cpu)) {
+	if (sched_feat(TTWU_QUEUE) && !cpus_share_cache(smp_processor_id(), cpu)) {
 		sched_clock_cpu(cpu); /* sync clocks x-cpu */
 		ttwu_queue_remote(p, cpu);
 		return;
@@ -7070,6 +7065,18 @@ static void destroy_sched_domains(struct
 		destroy_sched_domain(sd, cpu);
 }
 
+/*
+ * Keep a special pointer to the highest sched_domain that has
+ * SD_SHARE_PKG_RESOURCE set (Last Level Cache Domain) for this
+ * allows us to avoid some pointer chasing select_idle_sibling().
+ *
+ * Also keep a unique ID per domain (we use the first cpu number in
+ * the cpumask of the domain), this allows us to quickly tell if
+ * two cpus are in the same cache domain, see cpus_share_cache().
+ */
+DEFINE_PER_CPU(struct sched_domain *, sd_llc);
+DEFINE_PER_CPU(int, sd_llc_id);
+
 static void update_top_cache_domain(int cpu)
 {
 	struct sched_domain *sd;
@@ -7079,7 +7086,8 @@ static void update_top_cache_domain(int
 	if (sd)
 		id = cpumask_first(sched_domain_span(sd));
 
-	per_cpu(sd_top_spr_id, cpu) = id;
+	rcu_assign_pointer(per_cpu(sd_llc, cpu), sd);
+	per_cpu(sd_llc_id, cpu) = id;
 }
 
 /*
--- a/kernel/sched_fair.c
+++ b/kernel/sched_fair.c
@@ -2279,7 +2279,7 @@ static int select_idle_sibling(struct ta
 	/*
 	 * Otherwise, iterate the domains and find an elegible idle cpu.
 	 */
-	sd = highest_flag_domain(target, SD_SHARE_PKG_RESOURCES);
+	sd = rcu_dereference(per_cpu(sd_llc, target));
 	for_each_lower_domain(sd) {
 		sg = sd->groups;
 		do {
