From: Mel Gorman <mgorman@suse.de>
Date: Thu, 21 Feb 2013 11:19:28 +0000
Subject: [PATCH] Have mmu_notifiers use SRCU so they may safely schedule kabi
 compatability

References: bnc#578046, bnc#786814, FATE#306952
Patch-mainline: No, never

SP2 merged support for sleeping MMU notifiers that differed from what was
merged upstream. SP2 used a per-mm srcu_struct but upstream noted that
this could have considerable memory overhead. The upstream patches are
now included in SP2 but unfortunately this patch is necessary to convert
back to per-mm SCRU to preserve KABI compatability.

Signed-off-by: Mel Gorman <mgorman@suse.de>
---
 include/linux/mmu_notifier.h |    2 ++
 mm/mmu_notifier.c            |   43 +++++++++++++++++++++++++-----------------
 2 files changed, 28 insertions(+), 17 deletions(-)

diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h
index ee2baf0..23c3c86 100644
--- a/include/linux/mmu_notifier.h
+++ b/include/linux/mmu_notifier.h
@@ -20,6 +20,8 @@ struct mmu_notifier_ops;
 struct mmu_notifier_mm {
 	/* all mmu notifiers registerd in this mm are queued in this list */
 	struct hlist_head list;
+	/* srcu structure for this mm */
+	struct srcu_struct srcu;
 	/* to serialize the list modifications and hlist_unhashed */
 	spinlock_t lock;
 };
diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c
index 88fa54d..db970dd 100644
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@ -45,7 +45,7 @@ void __mmu_notifier_release(struct mm_struct *mm)
 	 * ->release() callouts this function makes have
 	 * returned.
 	 */
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	spin_lock(&mm->mmu_notifier_mm->lock);
 	while (unlikely(!hlist_empty(&mm->mmu_notifier_mm->list))) {
 		mn = hlist_entry(mm->mmu_notifier_mm->list.first,
@@ -73,7 +73,7 @@ void __mmu_notifier_release(struct mm_struct *mm)
 	 * All callouts to ->release() which we have done are complete.
 	 * Allow synchronize_srcu() in mmu_notifier_unregister() to complete
 	 */
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 
 	/*
 	 * mmu_notifier_unregister() may have unlinked a notifier and may
@@ -98,12 +98,12 @@ int __mmu_notifier_clear_flush_young(struct mm_struct *mm,
 	struct hlist_node *n;
 	int young = 0, id;
 
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, n, &mm->mmu_notifier_mm->list, hlist) {
 		if (mn->ops->clear_flush_young)
 			young |= mn->ops->clear_flush_young(mn, mm, address);
 	}
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 
 	return young;
 }
@@ -113,9 +113,10 @@ int __mmu_notifier_test_young(struct mm_struct *mm,
 {
 	struct mmu_notifier *mn;
 	struct hlist_node *n;
-	int young = 0, id;
+	int young = 0;
+	int id;
 
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, n, &mm->mmu_notifier_mm->list, hlist) {
 		if (mn->ops->test_young) {
 			young = mn->ops->test_young(mn, mm, address);
@@ -123,7 +124,7 @@ int __mmu_notifier_test_young(struct mm_struct *mm,
 				break;
 		}
 	}
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 
 	return young;
 }
@@ -135,7 +136,7 @@ void __mmu_notifier_change_pte(struct mm_struct *mm, unsigned long address,
 	struct hlist_node *n;
 	int id;
 
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, n, &mm->mmu_notifier_mm->list, hlist) {
 		if (mn->ops->change_pte)
 			mn->ops->change_pte(mn, mm, address, pte);
@@ -146,7 +147,7 @@ void __mmu_notifier_change_pte(struct mm_struct *mm, unsigned long address,
 		else if (mn->ops->invalidate_page)
 			mn->ops->invalidate_page(mn, mm, address);
 	}
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 }
 
 void __mmu_notifier_invalidate_page(struct mm_struct *mm,
@@ -156,12 +157,12 @@ void __mmu_notifier_invalidate_page(struct mm_struct *mm,
 	struct hlist_node *n;
 	int id;
 
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, n, &mm->mmu_notifier_mm->list, hlist) {
 		if (mn->ops->invalidate_page)
 			mn->ops->invalidate_page(mn, mm, address);
 	}
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 }
 
 void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,
@@ -171,12 +172,12 @@ void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,
 	struct hlist_node *n;
 	int id;
 
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, n, &mm->mmu_notifier_mm->list, hlist) {
 		if (mn->ops->invalidate_range_start)
 			mn->ops->invalidate_range_start(mn, mm, start, end);
 	}
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 }
 
 void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,
@@ -186,12 +187,12 @@ void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,
 	struct hlist_node *n;
 	int id;
 
-	id = srcu_read_lock(&srcu);
+	id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 	hlist_for_each_entry_rcu(mn, n, &mm->mmu_notifier_mm->list, hlist) {
 		if (mn->ops->invalidate_range_end)
 			mn->ops->invalidate_range_end(mn, mm, start, end);
 	}
-	srcu_read_unlock(&srcu, id);
+	srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 }
 
 static int do_mmu_notifier_register(struct mmu_notifier *mn,
@@ -214,6 +215,10 @@ static int do_mmu_notifier_register(struct mmu_notifier *mn,
 	if (unlikely(!mmu_notifier_mm))
 		goto out;
 
+	ret = init_srcu_struct(&mmu_notifier_mm->srcu);
+	if (unlikely(ret))
+		goto out_kfree;
+
 	if (take_mmap_sem)
 		down_write(&mm->mmap_sem);
 	ret = mm_take_all_locks(mm);
@@ -244,6 +249,9 @@ static int do_mmu_notifier_register(struct mmu_notifier *mn,
 out_cleanup:
 	if (take_mmap_sem)
 		up_write(&mm->mmap_sem);
+	if (mmu_notifier_mm)
+		cleanup_srcu_struct(&mmu_notifier_mm->srcu);
+out_kfree:
 	/* kfree() does nothing if mmu_notifier_mm is NULL */
 	kfree(mmu_notifier_mm);
 out:
@@ -284,6 +292,7 @@ EXPORT_SYMBOL_GPL(__mmu_notifier_register);
 void __mmu_notifier_mm_destroy(struct mm_struct *mm)
 {
 	BUG_ON(!hlist_empty(&mm->mmu_notifier_mm->list));
+	cleanup_srcu_struct(&mm->mmu_notifier_mm->srcu);
 	kfree(mm->mmu_notifier_mm);
 	mm->mmu_notifier_mm = LIST_POISON1; /* debug */
 }
@@ -309,7 +318,7 @@ void mmu_notifier_unregister(struct mmu_notifier *mn, struct mm_struct *mm)
 		/*
 		 * Ensure we synchronize up with __mmu_notifier_release().
 		 */
-		id = srcu_read_lock(&srcu);
+		id = srcu_read_lock(&mm->mmu_notifier_mm->srcu);
 
 		hlist_del_rcu(&mn->hlist);
 		spin_unlock(&mm->mmu_notifier_mm->lock);
@@ -320,7 +329,7 @@ void mmu_notifier_unregister(struct mmu_notifier *mn, struct mm_struct *mm)
 		/*
 		 * Allow __mmu_notifier_release() to complete.
 		 */
-		srcu_read_unlock(&srcu, id);
+		srcu_read_unlock(&mm->mmu_notifier_mm->srcu, id);
 	} else
 		spin_unlock(&mm->mmu_notifier_mm->lock);
 
