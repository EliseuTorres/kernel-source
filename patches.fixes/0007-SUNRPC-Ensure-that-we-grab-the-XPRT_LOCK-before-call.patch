Git-commit: 43cedbf0e8dfb9c5610eb7985d5f21263e313802
From: Trond Myklebust <Trond.Myklebust@netapp.com>
Date: Sun, 17 Jul 2011 16:01:03 -0400
Subject: [PATCH] SUNRPC: Ensure that we grab the XPRT_LOCK before calling xprt_alloc_slot
Patch-mainline: v3.1
References: bnc#800907

This throttles the allocation of new slots when the socket is busy
reconnecting and/or is out of buffer space.

Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>
Acked-by: NeilBrown <neilb@suse.de>

---
 net/sunrpc/xprt.c     |   88 ++++++++++++++++++++++++++++++++++++--------------
 net/sunrpc/xprtsock.c |    2 +
 2 files changed, 66 insertions(+), 24 deletions(-)

--- linux-3.0-SLE11-SP2.orig/net/sunrpc/xprt.c
+++ linux-3.0-SLE11-SP2/net/sunrpc/xprt.c
@@ -191,10 +191,9 @@ EXPORT_SYMBOL_GPL(xprt_load_transport);
  * transport connects from colliding with writes.  No congestion control
  * is provided.
  */
-int xprt_reserve_xprt(struct rpc_task *task)
+static int xprt_reserve_xprt2(struct rpc_xprt *xprt, struct rpc_task *task)
 {
 	struct rpc_rqst *req = task->tk_rqstp;
-	struct rpc_xprt	*xprt = req->rq_xprt;
 
 	if (test_and_set_bit(XPRT_LOCKED, &xprt->state)) {
 		if (task == xprt->snd_task)
@@ -202,8 +201,10 @@ int xprt_reserve_xprt(struct rpc_task *t
 		goto out_sleep;
 	}
 	xprt->snd_task = task;
-	req->rq_bytes_sent = 0;
-	req->rq_ntrans++;
+	if (req != NULL) {
+		req->rq_bytes_sent = 0;
+		req->rq_ntrans++;
+	}
 
 	return 1;
 
@@ -212,12 +213,16 @@ out_sleep:
 			task->tk_pid, xprt);
 	task->tk_timeout = 0;
 	task->tk_status = -EAGAIN;
-	if (req->rq_ntrans)
+	if (req != NULL && req->rq_ntrans)
 		rpc_sleep_on(&xprt->resend, task, NULL);
 	else
 		rpc_sleep_on(&xprt->sending, task, NULL);
 	return 0;
 }
+int xprt_reserve_xprt(struct rpc_task *task)
+{
+	return xprt_reserve_xprt2(task->tk_xprt, task);
+}
 EXPORT_SYMBOL_GPL(xprt_reserve_xprt);
 
 static void xprt_clear_locked(struct rpc_xprt *xprt)
@@ -239,9 +244,8 @@ static void xprt_clear_locked(struct rpc
  * integrated into the decision of whether a request is allowed to be
  * woken up and given access to the transport.
  */
-int xprt_reserve_xprt_cong(struct rpc_task *task)
+static int xprt_reserve_xprt_cong2(struct rpc_xprt *xprt, struct rpc_task *task)
 {
-	struct rpc_xprt	*xprt = task->tk_xprt;
 	struct rpc_rqst *req = task->tk_rqstp;
 
 	if (test_and_set_bit(XPRT_LOCKED, &xprt->state)) {
@@ -249,12 +253,14 @@ int xprt_reserve_xprt_cong(struct rpc_ta
 			return 1;
 		goto out_sleep;
 	}
+	if (req == NULL) {
+		xprt->snd_task = task;
+		return 1;
+	}
 	if (__xprt_get_cong(xprt, task)) {
 		xprt->snd_task = task;
-		if (req) {
-			req->rq_bytes_sent = 0;
-			req->rq_ntrans++;
-		}
+		req->rq_bytes_sent = 0;
+		req->rq_ntrans++;
 		return 1;
 	}
 	xprt_clear_locked(xprt);
@@ -262,12 +268,16 @@ out_sleep:
 	dprintk("RPC: %5u failed to lock transport %p\n", task->tk_pid, xprt);
 	task->tk_timeout = 0;
 	task->tk_status = -EAGAIN;
-	if (req && req->rq_ntrans)
+	if (req != NULL && req->rq_ntrans)
 		rpc_sleep_on(&xprt->resend, task, NULL);
 	else
 		rpc_sleep_on(&xprt->sending, task, NULL);
 	return 0;
 }
+int xprt_reserve_xprt_cong(struct rpc_task *task)
+{
+	return xprt_reserve_xprt_cong2(task->tk_xprt, task);
+}
 EXPORT_SYMBOL_GPL(xprt_reserve_xprt_cong);
 
 static inline int xprt_lock_write(struct rpc_xprt *xprt, struct rpc_task *task)
@@ -275,7 +285,13 @@ static inline int xprt_lock_write(struct
 	int retval;
 
 	spin_lock_bh(&xprt->transport_lock);
-	retval = xprt->ops->reserve_xprt(task);
+	if (xprt->ops->reserve_xprt == xprt_reserve_xprt)
+		retval = xprt_reserve_xprt2(xprt, task);
+	else if (xprt->ops->reserve_xprt == xprt_reserve_xprt_cong)
+		retval = xprt_reserve_xprt_cong2(xprt, task);
+	else
+		/* Must be rdma, for which this is safe */
+		retval = xprt->ops->reserve_xprt(task);
 	spin_unlock_bh(&xprt->transport_lock);
 	return retval;
 }
@@ -291,7 +307,7 @@ static void __xprt_lock_write_next(struc
 	task = rpc_wake_up_next(&xprt->resend);
 	if (!task) {
 		task = rpc_wake_up_next(&xprt->sending);
-		if (!task)
+		if (task == NULL)
 			goto out_unlock;
 	}
 
@@ -310,6 +326,7 @@ out_unlock:
 static void __xprt_lock_write_next_cong(struct rpc_xprt *xprt)
 {
 	struct rpc_task *task;
+	struct rpc_rqst *req;
 
 	if (test_and_set_bit(XPRT_LOCKED, &xprt->state))
 		return;
@@ -318,16 +335,19 @@ static void __xprt_lock_write_next_cong(
 	task = rpc_wake_up_next(&xprt->resend);
 	if (!task) {
 		task = rpc_wake_up_next(&xprt->sending);
-		if (!task)
+		if (task == NULL)
 			goto out_unlock;
 	}
+
+	req = task->tk_rqstp;
+	if (req == NULL) {
+		xprt->snd_task = task;
+		return;
+	}
 	if (__xprt_get_cong(xprt, task)) {
-		struct rpc_rqst *req = task->tk_rqstp;
 		xprt->snd_task = task;
-		if (req) {
-			req->rq_bytes_sent = 0;
-			req->rq_ntrans++;
-		}
+		req->rq_bytes_sent = 0;
+		req->rq_ntrans++;
 		return;
 	}
 out_unlock:
@@ -845,6 +865,7 @@ int xprt_prepare_transmit(struct rpc_tas
 	struct rpc_rqst	*req = task->tk_rqstp;
 	struct rpc_xprt	*xprt = req->rq_xprt;
 	int err = 0;
+	int ret;
 
 	dprintk("RPC: %5u xprt_prepare_transmit\n", task->tk_pid);
 
@@ -853,7 +874,14 @@ int xprt_prepare_transmit(struct rpc_tas
 		err = req->rq_reply_bytes_recvd;
 		goto out_unlock;
 	}
-	if (!xprt->ops->reserve_xprt(task))
+	if (xprt->ops->reserve_xprt == xprt_reserve_xprt)
+		ret = xprt_reserve_xprt2(xprt, task);
+	else if (xprt->ops->reserve_xprt == xprt_reserve_xprt_cong)
+		ret = xprt_reserve_xprt_cong2(xprt, task);
+	else
+		/* Must be rdma, for which this is safe */
+		ret = xprt->ops->reserve_xprt(task);
+	if (!ret)
 		err = -EAGAIN;
 out_unlock:
 	spin_unlock_bh(&xprt->transport_lock);
@@ -934,8 +962,6 @@ static void xprt_alloc_slot(struct rpc_t
 	struct rpc_xprt	*xprt = task->tk_xprt;
 
 	task->tk_status = 0;
-	if (task->tk_rqstp)
-		return;
 	if (!list_empty(&xprt->free)) {
 		struct rpc_rqst	*req = list_entry(xprt->free.next, struct rpc_rqst, rq_list);
 		list_del_init(&req->rq_list);
@@ -945,7 +971,6 @@ static void xprt_alloc_slot(struct rpc_t
 	}
 	dprintk("RPC:       waiting for request slot\n");
 	task->tk_status = -EAGAIN;
-	task->tk_timeout = 0;
 	rpc_sleep_on(&xprt->backlog, task, NULL);
 }
 
@@ -1002,10 +1027,25 @@ void xprt_reserve(struct rpc_task *task)
 {
 	struct rpc_xprt	*xprt = task->tk_xprt;
 
+	task->tk_status = 0;
+	if (task->tk_rqstp != NULL)
+		return;
+
+	/* Note: grabbing the xprt_lock_write() here is not strictly needed,
+	 * but ensures that we throttle new slot allocation if the transport
+	 * is congested (e.g. if reconnecting or if we're out of socket
+	 * write buffer space).
+	 */
+	task->tk_timeout = 0;
+	task->tk_status = -EAGAIN;
+	if (!xprt_lock_write(xprt, task))
+		return;
+
 	task->tk_status = -EIO;
 	spin_lock(&xprt->reserve_lock);
 	xprt_alloc_slot(task);
 	spin_unlock(&xprt->reserve_lock);
+	xprt_release_write(xprt, task);
 }
 
 static inline __be32 xprt_alloc_xid(struct rpc_xprt *xprt)
--- linux-3.0-SLE11-SP2.orig/net/sunrpc/xprtsock.c
+++ linux-3.0-SLE11-SP2/net/sunrpc/xprtsock.c
@@ -753,6 +753,8 @@ static void xs_tcp_release_xprt(struct r
 	if (task == NULL)
 		goto out_release;
 	req = task->tk_rqstp;
+	if (req == NULL)
+		goto out_release;
 	if (req->rq_bytes_sent == 0)
 		goto out_release;
 	if (req->rq_bytes_sent == req->rq_snd_buf.len)
