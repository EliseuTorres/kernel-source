From: Vincent Guittot <vincent.guittot@linaro.org>
Date: Tue, 11 Mar 2014 17:26:06 +0100
Subject: [PATCH] sched: Remove double calculation in fix_small_imbalance()
Git-commit: a2cd42601b474b957e1a5fe3692bcf7f9363bd51
Patch-mainline: v3.15-rc1
References:

The tmp value has been already calculated in:

  scaled_busy_load_per_task =
		(busiest->load_per_task * SCHED_POWER_SCALE) /
		busiest->group_power;

Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1394555166-22894-1-git-send-email-vincent.guittot@linaro.org
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Acked-by: Mike Galbraith <mgalbraith@suse.de>

---
 kernel/sched/fair.c |    6 ++----
 1 file changed, 2 insertions(+), 4 deletions(-)

--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5822,12 +5822,10 @@ void fix_small_imbalance(struct lb_env *
 	pwr_now /= SCHED_POWER_SCALE;
 
 	/* Amount of load we'd subtract */
-	tmp = (busiest->load_per_task * SCHED_POWER_SCALE) /
-		busiest->group_power;
-	if (busiest->avg_load > tmp) {
+	if (busiest->avg_load > scaled_busy_load_per_task) {
 		pwr_move += busiest->group_power *
 			    min(busiest->load_per_task,
-				busiest->avg_load - tmp);
+				busiest->avg_load - scaled_busy_load_per_task);
 	}
 
 	/* Amount of load we'd add */
