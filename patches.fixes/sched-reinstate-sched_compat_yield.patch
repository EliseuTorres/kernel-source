Subject: sched: reinstate sched_compat_yield
From: Mike Galbraith <mgalbraith@suse.de>
Date: Fri Oct  7 08:06:43 CEST 2011
Patch-mainline: never
References: bnc#722449

Restore aggressive yield option to fix up partner performance regressions.

Quoting bugzilla:

Older software relies on the old yield behavior. And many IBM software,
especially the one used on System z in production, are  based on older Java
versions that have this problem. It's no question that newer software should
not be written that way. 
So removing the possibility to switch to the old behavior will break production
customers

Signed-off-by: Mike Galbraith <mgalbraith@suse.de>

---
 include/linux/sched.h |    2 ++
 kernel/sched_fair.c   |   33 +++++++++++++++++++++++++++++++--
 kernel/sysctl.c       |    7 +++++++
 3 files changed, 40 insertions(+), 2 deletions(-)

--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2008,6 +2008,8 @@ int sched_rt_handler(struct ctl_table *t
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 
+extern unsigned int sysctl_sched_compat_yield;
+
 #ifdef CONFIG_SCHED_AUTOGROUP
 extern unsigned int sysctl_sched_autogroup_enabled;
 
--- a/kernel/sched_fair.c
+++ b/kernel/sched_fair.c
@@ -70,6 +70,14 @@ static unsigned int sched_nr_latency = 8
 unsigned int sysctl_sched_child_runs_first __read_mostly;
 
 /*
+ * sys_sched_yield() compat mode
+ *
+ * This option switches the agressive yield implementation of the
+ * old scheduler back on.
+ */
+unsigned int sysctl_sched_compat_yield __read_mostly;
+
+/*
  * SCHED_OTHER wake-up granularity.
  * (default: 5 msec * (1 + ilog(ncpus)), units: nanoseconds)
  *
@@ -433,7 +441,6 @@ static struct sched_entity *__pick_next_
 	return rb_entry(next, struct sched_entity, run_node);
 }
 
-#ifdef CONFIG_SCHED_DEBUG
 static struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq)
 {
 	struct rb_node *last = rb_last(&cfs_rq->tasks_timeline);
@@ -444,6 +451,7 @@ static struct sched_entity *__pick_last_
 	return rb_entry(last, struct sched_entity, run_node);
 }
 
+#ifdef CONFIG_SCHED_DEBUG
 /**************************************************************
  * Scheduling class statistics methods:
  */
@@ -2629,7 +2637,7 @@ static void yield_task_fair(struct rq *r
 {
 	struct task_struct *curr = rq->curr;
 	struct cfs_rq *cfs_rq = task_cfs_rq(curr);
-	struct sched_entity *se = &curr->se;
+	struct sched_entity *se = &curr->se, *rightmost;
 
 	/*
 	 * Are we the only task in the tree?
@@ -2648,6 +2656,27 @@ static void yield_task_fair(struct rq *r
 	}
 
 	set_skip_buddy(se);
+
+	if (likely(!sysctl_sched_compat_yield))
+		return;
+
+	/*
+	 * Find the rightmost entry in the rbtree:
+	 */
+	rightmost = __pick_last_entity(cfs_rq);
+
+	/*
+	 * Already in the rightmost position?
+	 */
+	if (unlikely(!rightmost || entity_before(rightmost, se)))
+		return;
+
+	/*
+	 * Minimally necessary key value to be last in the tree:
+	 * Upon rescheduling, sched_class::put_prev_task() will place
+	 * 'current' within the tree based on its new key value.
+	 */
+	se->vruntime = rightmost->vruntime + 1;
 }
 
 static bool yield_to_task_fair(struct rq *rq, struct task_struct *p, bool preempt)
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -368,6 +368,13 @@ static struct ctl_table kern_table[] = {
 		.mode		= 0644,
 		.proc_handler	= sched_rt_handler,
 	},
+	{
+		.procname	= "sched_compat_yield",
+		.data		= &sysctl_sched_compat_yield,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
 #ifdef CONFIG_SCHED_AUTOGROUP
 	{
 		.procname	= "sched_autogroup_enabled",
