From: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Subject: /proc/stat: scalability of irq num per cpu
References: bnc#648112
Git-commit: f2c66cd8eeddedb440f33bc0f5cec1ed7ae376cb
Patch-mainline: v2.6.37-rc1

/proc/stat shows the total number of all interrupts to each cpu. But when
the number of IRQs are very large, it take very long time and 'cat /proc/stat'
takes more than 10 secs. This is because sum of all irq events are counted
when /proc/stat is read. This patch adds "sum of all irq" counter percpu
and reduce read costs.

[rjw: Modified to avoid kABI breakage.]

Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Acked-by: Jack Steiner <steiner@sgi.com>
Signed-off-by: Rafael J. Wysocki <rjw@suse.de>
---
 fs/proc/stat.c              |    4 +---
 include/linux/kernel_stat.h |   18 ++++++++++++++++--
 kernel/sched.c              |    4 ++++
 3 files changed, 21 insertions(+), 5 deletions(-)

Index: linux-2.6.32-SLE11-SP1/include/linux/kernel_stat.h
===================================================================
--- linux-2.6.32-SLE11-SP1.orig/include/linux/kernel_stat.h
+++ linux-2.6.32-SLE11-SP1/include/linux/kernel_stat.h
@@ -37,9 +37,13 @@ struct kernel_stat {
 
 DECLARE_PER_CPU(struct kernel_stat, kstat);
 
+DECLARE_PER_CPU(unsigned long, kstat_irqs_sum);
+
 #define kstat_cpu(cpu)	per_cpu(kstat, cpu)
+#define kstat_irqs_sum_cpu(cpu)	per_cpu(kstat_irqs_sum, cpu)
 /* Must have preemption disabled for this to be meaningful. */
 #define kstat_this_cpu	__get_cpu_var(kstat)
+#define kstat_irqs_sum_this_cpu	__get_cpu_var(kstat_irqs_sum)
 
 extern unsigned long long nr_context_switches(void);
 
@@ -53,6 +57,7 @@ static inline void kstat_incr_irqs_this_
 					    struct irq_desc *desc)
 {
 	kstat_this_cpu.irqs[irq]++;
+	kstat_irqs_sum_this_cpu++;
 }
 
 static inline unsigned int kstat_irqs_cpu(unsigned int irq, int cpu)
@@ -64,8 +69,10 @@ static inline unsigned int kstat_irqs_cp
 extern unsigned int kstat_irqs_cpu(unsigned int irq, int cpu);
 #define kstat_irqs_this_cpu(DESC) \
 	((DESC)->kstat_irqs[smp_processor_id()])
-#define kstat_incr_irqs_this_cpu(irqno, DESC) \
-	((DESC)->kstat_irqs[smp_processor_id()]++)
+#define kstat_incr_irqs_this_cpu(irqno, DESC) do {\
+	((DESC)->kstat_irqs[smp_processor_id()]++);\
+	kstat_irqs_sum_this_cpu++; } while (0)
+
 
 #endif
 
@@ -93,6 +100,13 @@ static inline unsigned int kstat_irqs(un
 	return sum;
 }
 
+/*
+ * Number of interrupts per cpu, since bootup
+ */
+static inline unsigned int kstat_cpu_irqs_sum(unsigned int cpu)
+{
+	return kstat_irqs_sum_cpu(cpu);
+}
 
 /*
  * Lock/unlock the current runqueue - to extract task statistics:
Index: linux-2.6.32-SLE11-SP1/kernel/sched.c
===================================================================
--- linux-2.6.32-SLE11-SP1.orig/kernel/sched.c
+++ linux-2.6.32-SLE11-SP1/kernel/sched.c
@@ -5037,6 +5037,10 @@ DEFINE_PER_CPU(struct kernel_stat, kstat
 
 EXPORT_PER_CPU_SYMBOL(kstat);
 
+DEFINE_PER_CPU(unsigned long, kstat_irqs_sum);
+
+EXPORT_PER_CPU_SYMBOL(kstat_irqs_sum);
+
 /*
  * Return any ns on the sched_clock that have not yet been accounted in
  * @p in case that task is currently running.
Index: linux-2.6.32-SLE11-SP1/fs/proc/stat.c
===================================================================
--- linux-2.6.32-SLE11-SP1.orig/fs/proc/stat.c
+++ linux-2.6.32-SLE11-SP1/fs/proc/stat.c
@@ -51,9 +51,7 @@ static int show_stat(struct seq_file *p,
 		softirq = cputime64_add(softirq, kstat_cpu(i).cpustat.softirq);
 		steal = cputime64_add(steal, kstat_cpu(i).cpustat.steal);
 		guest = cputime64_add(guest, kstat_cpu(i).cpustat.guest);
-		for_each_irq_nr(j) {
-			sum += kstat_irqs_cpu(j, i);
-		}
+		sum += kstat_cpu_irqs_sum(i);
 		sum += arch_irq_stat_cpu(i);
 
 		for (j = 0; j < NR_SOFTIRQS; j++) {
