From 0a8df02669e696873d42aa8343be230cb4155ab9 Mon Sep 17 00:00:00 2001
From: Michal Kubecek <mkubecek@suse.cz>
Date: Mon, 29 Oct 2012 13:54:33 +0100
Subject: inetpeer: make unused_peers list per-netns
Patch-mainline: Never, SLE specific fixup
References: bnc#779969

As we have separate per-netns inet_peer trees now, cleanup of
unused entries needs to be done per namespace as well. Otherwise
cleanup_once() wouldn't know which tree to unlink the entry
from.

Signed-off-by: Michal Kubecek <mkubecek@suse.cz>
---
 include/net/inetpeer.h |    7 +++-
 net/ipv4/inetpeer.c    |   94 ++++++++++++++++++++++++------------------------
 2 files changed, 53 insertions(+), 48 deletions(-)

diff --git a/include/net/inetpeer.h b/include/net/inetpeer.h
index d2dd781..83cf2b0 100644
--- a/include/net/inetpeer.h
+++ b/include/net/inetpeer.h
@@ -27,12 +27,17 @@ struct inetpeer_addr {
 	__u16				family;
 };
 
+struct inet_peer_base;
+
 struct inet_peer {
 	/* group together avl_left,avl_right,v4daddr to speedup lookups */
 	struct inet_peer __rcu	*avl_left, *avl_right;
 	struct inetpeer_addr	daddr;
 	__u32			avl_height;
-	struct list_head	unused;
+	union {
+		struct list_head	unused;
+		struct inet_peer_base	*base;
+	};
 	__u32			dtime;		/* the time of last use of not
 						 * referenced entries */
 	atomic_t		refcnt;
diff --git a/net/ipv4/inetpeer.c b/net/ipv4/inetpeer.c
index 0b28d2e..2ef9e14 100644
--- a/net/ipv4/inetpeer.c
+++ b/net/ipv4/inetpeer.c
@@ -81,11 +81,14 @@ static const struct inet_peer peer_fake_node = {
 };
 
 struct inet_peer_base {
-	struct inet_peer __rcu *root;
-	seqlock_t	lock;
-	int		total;
+	struct inet_peer __rcu	*root;
+	seqlock_t		lock;
+	struct list_head	unused_peers_list;
 };
 
+atomic_t peers_total;
+DEFINE_SPINLOCK(unused_peers_lock);
+
 #define PEER_MAXDEPTH 40 /* sufficient for about 2^27 nodes */
 
 /* Exported for sysctl_net_ipv4.  */
@@ -96,14 +99,6 @@ int inet_peer_maxttl __read_mostly = 10 * 60 * HZ;	/* usual time to live: 10 min
 int inet_peer_gc_mintime __read_mostly = 10 * HZ;
 int inet_peer_gc_maxtime __read_mostly = 120 * HZ;
 
-static struct {
-	struct list_head	list;
-	spinlock_t		lock;
-} unused_peers = {
-	.list			= LIST_HEAD_INIT(unused_peers.list),
-	.lock			= __SPIN_LOCK_UNLOCKED(unused_peers.lock),
-};
-
 static void peer_check_expire(unsigned long dummy);
 static DEFINE_TIMER(peer_periodic_timer, peer_check_expire, 0, 0);
 
@@ -116,6 +111,7 @@ static int __net_init inetpeer_net_init(struct net *net)
 		return -ENOMEM;
 
 	net->ipv4.peers->root = peer_avl_empty_rcu;
+	INIT_LIST_HEAD(&net->ipv4.peers->unused_peers_list);
 	seqlock_init(&net->ipv4.peers->lock);
 
 	net->ipv6.peers = kzalloc(sizeof(struct inet_peer_base),
@@ -124,6 +120,7 @@ static int __net_init inetpeer_net_init(struct net *net)
 		goto out_ipv6;
 
 	net->ipv6.peers->root = peer_avl_empty_rcu;
+	INIT_LIST_HEAD(&net->ipv6.peers->unused_peers_list);
 	seqlock_init(&net->ipv6.peers->lock);
 
 	return 0;
@@ -180,11 +177,13 @@ void __init inet_initpeers(void)
 }
 
 /* Called with or without local BH being disabled. */
-static void unlink_from_unused(struct inet_peer *p)
+static void unlink_from_unused(struct inet_peer_base *base,
+			       struct inet_peer *p)
 {
-	spin_lock_bh(&unused_peers.lock);
-	list_del_init(&p->unused);
-	spin_unlock_bh(&unused_peers.lock);
+	spin_lock_bh(&unused_peers_lock);
+	list_del(&p->unused);
+	p->base = base;
+	spin_unlock_bh(&unused_peers_lock);
 }
 
 static int addr_compare(const struct inetpeer_addr *a,
@@ -434,7 +433,7 @@ static void unlink_from_pool(struct inet_peer *p, struct inet_peer_base *base,
 			delp[1] = &t->avl_left; /* was &p->avl_left */
 		}
 		peer_avl_rebalance(stack, stackptr, base);
-		base->total--;
+		atomic_dec(&peers_total);
 		do_free = 1;
 	}
 	write_sequnlock_bh(&base->lock);
@@ -458,37 +457,35 @@ static struct inet_peer_base *family_to_base(struct net *net,
 	return (family == AF_INET ? net->ipv4.peers : net->ipv6.peers);
 }
 
-static struct inet_peer_base *peer_to_base(struct inet_peer *p)
-{
-	return family_to_base(p->daddr.family);
-}
-
 /* May be called with local BH enabled. */
-static int cleanup_once(unsigned long ttl, struct inet_peer __rcu **stack[PEER_MAXDEPTH])
+static int cleanup_once(struct inet_peer_base *base,
+			unsigned long ttl,
+			struct inet_peer __rcu **stack[PEER_MAXDEPTH])
 {
 	struct inet_peer *p = NULL;
 
 	/* Remove the first entry from the list of unused nodes. */
-	spin_lock_bh(&unused_peers.lock);
-	if (!list_empty(&unused_peers.list)) {
+	spin_lock_bh(&unused_peers_lock);
+	if (!list_empty(&base->unused_peers_list)) {
 		__u32 delta;
 
-		p = list_first_entry(&unused_peers.list, struct inet_peer, unused);
+		p = list_first_entry(&base->unused_peers_list, struct inet_peer, unused);
 		delta = (__u32)jiffies - p->dtime;
 
 		if (delta < ttl) {
 			/* Do not prune fresh entries. */
-			spin_unlock_bh(&unused_peers.lock);
+			spin_unlock_bh(&unused_peers_lock);
 			return -1;
 		}
 
 		list_del_init(&p->unused);
+		p->base = base;
 
 		/* Grab an extra reference to prevent node disappearing
 		 * before unlink_from_pool() call. */
 		atomic_inc(&p->refcnt);
 	}
-	spin_unlock_bh(&unused_peers.lock);
+	spin_unlock_bh(&unused_peers_lock);
 
 	if (p == NULL)
 		/* It means that the total number of USED entries has
@@ -496,7 +493,7 @@ static int cleanup_once(unsigned long ttl, struct inet_peer __rcu **stack[PEER_M
 		 * happen because of entry limits in route cache. */
 		return -1;
 
-	unlink_from_pool(p, peer_to_base(p), stack);
+	unlink_from_pool(p, base, stack);
 	return 0;
 }
 
@@ -525,7 +522,7 @@ found:		/* The existing node has been found.
 		 * Remove the entry from unused list if it was there.
 		 */
 		if (newrefcnt == 1)
-			unlink_from_unused(p);
+			unlink_from_unused(base, p);
 		return p;
 	}
 
@@ -556,26 +553,20 @@ found:		/* The existing node has been found.
 		p->pmtu_expires = 0;
 		p->pmtu_orig = 0;
 		memset(&p->redirect_learned, 0, sizeof(p->redirect_learned));
-		INIT_LIST_HEAD(&p->unused);
-
+		p->base = base;
 
 		/* Link the node. */
 		link_to_pool(p, base);
-		base->total++;
+		atomic_inc(&peers_total);
 	}
 	write_sequnlock_bh(&base->lock);
 
-	if (base->total >= inet_peer_threshold)
+	if (atomic_read(&peers_total) >= inet_peer_threshold)
 		/* Remove one less-recently-used entry. */
-		cleanup_once(0, stack);
+		cleanup_once(base, 0, stack);
 
 	return p;
 }
-
-static int compute_total(void)
-{
-	return v4_peers.total + v6_peers.total;
-}
 EXPORT_SYMBOL_GPL(inet_getpeer);
 
 /* Called with local BH disabled. */
@@ -584,23 +575,31 @@ static void peer_check_expire(unsigned long dummy)
 	unsigned long now = jiffies;
 	int ttl, total;
 	struct inet_peer __rcu **stack[PEER_MAXDEPTH];
+	struct net *net;
 
-	total = compute_total();
+	total = atomic_read(&peers_total);
 	if (total >= inet_peer_threshold)
 		ttl = inet_peer_minttl;
 	else
 		ttl = inet_peer_maxttl
 				- (inet_peer_maxttl - inet_peer_minttl) / HZ *
 					total / inet_peer_threshold * HZ;
-	while (!cleanup_once(ttl, stack)) {
-		if (jiffies != now)
-			break;
+	for_each_net(net) {
+		while (!cleanup_once(net->ipv4.peers, ttl, stack)) {
+			if (jiffies != now)
+				goto too_long;
+		}
+		while (!cleanup_once(net->ipv6.peers, ttl, stack)) {
+			if (jiffies != now)
+				goto too_long;
+		}
 	}
+too_long:
 
 	/* Trigger the timer after inet_peer_gc_mintime .. inet_peer_gc_maxtime
 	 * interval depending on the total number of entries (more entries,
 	 * less interval). */
-	total = compute_total();
+	total = atomic_read(&peers_total);
 	if (total >= inet_peer_threshold)
 		peer_periodic_timer.expires = jiffies + inet_peer_gc_mintime;
 	else
@@ -615,10 +614,11 @@ void inet_putpeer(struct inet_peer *p)
 {
 	local_bh_disable();
 
-	if (atomic_dec_and_lock(&p->refcnt, &unused_peers.lock)) {
-		list_add_tail(&p->unused, &unused_peers.list);
+	if (atomic_dec_and_lock(&p->refcnt, &unused_peers_lock)) {
+		struct inet_peer_base *base = p->base;
+		list_add_tail(&p->unused, &base->unused_peers_list);
 		p->dtime = (__u32)jiffies;
-		spin_unlock(&unused_peers.lock);
+		spin_unlock(&unused_peers_lock);
 	}
 
 	local_bh_enable();
-- 
1.7.10.4

