From: Mike Galbraith <mgalbraith@suse.de>
Date: Mon Dec  6 11:25:16 CET 2010
Subject: sched: debug divide by zero bug in find_busiest_group()
Patch-mainline: never
References: bnc#630970, bnc#661605

Based on patchlet from PeterZ.

Code inspection reveals nothing, the bug is not repeatable, yet some
customer boxen are exploding at random intervals varying from weeks to
months between events.  This has also been reported against mainline.

Let's not explode, and try to get some useful info.

Signed-off-by: Mike Galbraith <mgalbraith@suse.de>
---
 kernel/sched.c |   58 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++-
 1 file changed, 57 insertions(+), 1 deletion(-)

Index: linux-2.6.32-SLE11-SP1/kernel/sched.c
===================================================================
--- linux-2.6.32-SLE11-SP1.orig/kernel/sched.c
+++ linux-2.6.32-SLE11-SP1/kernel/sched.c
@@ -3704,6 +3704,18 @@ unsigned long scale_rt_power(int cpu)
 	total = sched_avg_period() + (rq->clock - rq->age_stamp);
 	available = total - rq->rt_avg;
 
+	if (unlikely(total < rq->rt_avg)) {
+		if (printk_ratelimit()) {
+			printk(KERN_ERR "scale_rt_power: period: %lu clock: %lu age_stamp: %lu rt_avg: %lu\n",
+				sched_avg_period(), rq->clock, rq->age_stamp, rq->rt_avg);
+		}
+
+		/* Ensures that power won't end up being negative */
+		available = 0;
+	} else {
+		available = total - rq->rt_avg;
+	}
+
 	if (unlikely((s64)total < SCHED_LOAD_SCALE))
 		total = SCHED_LOAD_SCALE;
 
@@ -3717,6 +3729,7 @@ static void update_cpu_power(struct sche
 	unsigned long weight = sd->span_weight;
 	unsigned long power = SCHED_LOAD_SCALE;
 	struct sched_group *sdg = sd->groups;
+	unsigned long scale_rt;
 
 	if (sched_feat(ARCH_POWER))
 		power *= arch_scale_freq_power(sd, cpu);
@@ -3734,12 +3747,22 @@ static void update_cpu_power(struct sche
 		power >>= SCHED_LOAD_SHIFT;
 	}
 
-	power *= scale_rt_power(cpu);
+	scale_rt = scale_rt_power(cpu);
+	power *= scale_rt;
 	power >>= SCHED_LOAD_SHIFT;
 
 	if (!power)
 		power = 1;
 
+	if ((int)power <= 0) {
+		if (printk_ratelimit()) {
+			WARN_ON_ONCE(1);
+			printk(KERN_ERR "update_cpu_power: cpu_power = %lu; scale_rt = %lu\n",
+					power, scale_rt);
+		}
+		power = 1;
+	}
+
 	sdg->cpu_power = power;
 }
 
@@ -3762,6 +3785,12 @@ static void update_group_power(struct sc
 		group = group->next;
 	} while (group != child->groups);
 
+	if ((int)power <= 0) {
+		WARN_ON_ONCE(1);
+		printk(KERN_ERR "update_group_power: cpu_power = %lu\n", power);
+		power = 1;
+	}
+
 	sdg->cpu_power = power;
 }
 
@@ -3839,6 +3868,14 @@ static inline void update_sg_lb_stats(st
 		return;
 	}
 
+	if ((int)group->cpu_power <= 0) {
+		if (printk_ratelimit()) {
+			WARN_ON_ONCE(1);
+			printk(KERN_ERR "update_sg_lb_stats: group->cpu_power = %d\n", group->cpu_power);
+		}
+		return;
+	}
+
 	/* Adjust by relative CPU power of the group */
 	sgs->avg_load = (sgs->group_load * SCHED_LOAD_SCALE) / group->cpu_power;
 
@@ -3957,6 +3994,16 @@ static inline void fix_small_imbalance(s
 
 	scaled_busy_load_per_task = sds->busiest_load_per_task
 						 * SCHED_LOAD_SCALE;
+
+	if ((int)sds->busiest->cpu_power <= 0) {
+		if (printk_ratelimit()) {
+			WARN_ON_ONCE(1);
+			printk(KERN_ERR "fix_small_imbalance: sds->busiest->cpu_power = %d\n",
+				sds->busiest->cpu_power);
+		}
+		return;
+	}
+
 	scaled_busy_load_per_task /= sds->busiest->cpu_power;
 
 	if (sds->max_load - sds->this_load + scaled_busy_load_per_task >=
@@ -4038,6 +4085,15 @@ static inline void calculate_imbalance(s
 
 		load_above_capacity *= (SCHED_LOAD_SCALE * SCHED_LOAD_SCALE);
 
+		if ((int)sds->busiest->cpu_power <= 0) {
+			if (printk_ratelimit()) {
+				WARN_ON_ONCE(1);
+				printk(KERN_ERR "calculate_imbalance: sds->busiest->cpu_power = %d\n",
+						sds->busiest->cpu_power);
+				return;
+			}
+		}
+
 		load_above_capacity /= sds->busiest->cpu_power;
 	}
 
