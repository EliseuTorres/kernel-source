From: Mel Gorman <mel@csn.ul.ie>
Subject: Reduce page allocation stalls
References: bnc#561811
Patch-mainline: no (not yet)

[Modified by Nick from Mel's post to linux-mm]

Under heavy memory pressure, the page allocator may call congestion_wait()
to wait for IO congestion to clear or a timeout. This is not as sensible
a choice as it first appears. There is no guarantee that BLK_RW_ASYNC is
even congested as the pressure could have been due to a large number of
SYNC reads and the allocator waits for the entire timeout, possibly uselessly.

At the point of congestion_wait(), the allocator is struggling to get the
pages it needs and it should back off. This patch puts the allocator to sleep
on a zone->pressure_wq for either a timeout or until a direct reclaimer or
kswapd brings the zone over the low watermark, whichever happens first.

Signed-off-by: Mel Gorman <mel@csn.ul.ie>
Signed-off-by: Nick Piggin <npiggin@suse.de>
---
 mm/page_alloc.c |   51 +++++++++++++++++++++++++++++++++++++++++++++++----
 1 file changed, 47 insertions(+), 4 deletions(-)

Index: linux-2.6.27/mm/page_alloc.c
===================================================================
--- linux-2.6.27.orig/mm/page_alloc.c
+++ linux-2.6.27/mm/page_alloc.c
@@ -170,6 +170,43 @@ EXPORT_SYMBOL(nr_online_nodes);
 
 int page_group_by_mobility_disabled __read_mostly;
 
+static wait_queue_head_t watermark_wq __read_mostly = __WAIT_QUEUE_HEAD_INITIALIZER(watermark_wq);
+
+static inline void watermark_check_zone(struct zone *zone)
+{
+	/* If no process is waiting, nothing to do */
+	if (likely(!waitqueue_active(&watermark_wq)))
+		return;
+
+	/* Check if the high watermark is ok for order 0 */
+	if (zone_watermark_ok(zone, 0, low_wmark_pages(zone), 0, 0))
+		wake_up_interruptible(&watermark_wq);
+}
+
+/**
+ * watermark_wait - Wait for watermark to go above low
+ * @timeout: Wait until watermark is reached or this timeout is reached
+ *
+ * Waits for up to @timeout jiffies for watermark on a zone to be reached
+ */
+static long watermark_wait(long timeout)
+{
+	long ret;
+	DEFINE_WAIT(wait);
+
+	prepare_to_wait(&watermark_wq, &wait, TASK_INTERRUPTIBLE);
+
+	/*
+	 * The use of io_schedule_timeout() here means that it gets
+	 * accounted for as IO waiting. This may or may not be the case
+	 * but at least this way it gets picked up by vmstat
+	 */
+	ret = io_schedule_timeout(timeout);
+	finish_wait(&watermark_wq, &wait);
+
+	return ret;
+}
+
 static void set_pageblock_migratetype(struct page *page, int migratetype)
 {
 
@@ -565,6 +602,8 @@ static void free_pcppages_bulk(struct zo
 		} while (--count && --batch_free && !list_empty(list));
 	}
 	spin_unlock(&zone->lock);
+	/* A batch of pages have been freed so check zone pressure */
+	watermark_check_zone(zone);
 }
 
 static void free_one_page(struct zone *zone, struct page *page, int order,
@@ -577,6 +616,8 @@ static void free_one_page(struct zone *z
 	__mod_zone_page_state(zone, NR_FREE_PAGES, 1 << order);
 	__free_one_page(page, zone, order, migratetype);
 	spin_unlock(&zone->lock);
+	/* A batch of pages have been freed so check zone pressure */
+	watermark_check_zone(zone);
 }
 
 static void __free_pages_ok(struct page *page, unsigned int order)
@@ -1727,8 +1768,10 @@ __alloc_pages_high_priority(gfp_t gfp_ma
 			zonelist, high_zoneidx, ALLOC_NO_WATERMARKS,
 			preferred_zone, migratetype);
 
-		if (!page && gfp_mask & __GFP_NOFAIL)
-			congestion_wait(BLK_RW_ASYNC, HZ/50);
+		if (!page && gfp_mask & __GFP_NOFAIL) {
+			/* If still failing, wait for pressure on zone to relieve */
+			watermark_wait(HZ/50);
+		}
 	} while (!page && (gfp_mask & __GFP_NOFAIL));
 
 	return page;
@@ -1898,8 +1941,8 @@ rebalance:
 	/* Check if we should retry the allocation */
 	pages_reclaimed += did_some_progress;
 	if (should_alloc_retry(gfp_mask, order, pages_reclaimed)) {
-		/* Wait for some write requests to complete then retry */
-		congestion_wait(BLK_RW_ASYNC, HZ/50);
+		/* Too much pressure, back off a bit at let reclaimers do work */
+		watermark_wait(HZ/50);
 		goto rebalance;
 	}
 
