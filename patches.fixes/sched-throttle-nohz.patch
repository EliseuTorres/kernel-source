Subject: sched: ratelimit nohz
From: Mike Galbraith <mgalbraith@suse.de>
Date: Mon Jun 11 18:21:05 CEST 2012
Patch-mainline: never
References: Scheduler enhancements for I7 (bnc#754690)

Entering nohz code on every micro-idle is costing ~10% throughput for netperf
TCP_RR when scheduling cross-cpu.

The higher the context switch rate, the more nohz entry costs.  With this patch
and some cycle recovery patches in my tree, max cross cpu context switch rate is
improved by ~16%, a large portion of which of which is this ratelimiting.

Note: a similar patch was in mainline briefly, but was reverted due to one
complaint wrt laptop using more power.  The earlier version raised ticks/s
on my Q6600 bof from ~85c to ~128, this version does not, and also blocks
the mb() in rcu_needs_cpu().

Signed-off-by: Mike Galbraith <mgalbraith@suse.de>

---
 include/linux/sched.h    |    7 ++++++-
 kernel/sched.c           |    5 +++++
 kernel/sched_fair.c      |    2 +-
 kernel/time/tick-sched.c |    4 ++--
 4 files changed, 14 insertions(+), 4 deletions(-)

--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -274,8 +274,13 @@ extern cpumask_var_t nohz_cpu_mask;
 #if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ)
 extern void select_nohz_load_balancer(int stop_tick);
 extern int get_nohz_timer_target(void);
+extern int sched_needs_cpu(int cpu);
 #else
 static inline void select_nohz_load_balancer(int stop_tick) { }
+static inline int sched_needs_cpu(int cpu)
+{
+	return 0;
+}
 #endif
 
 /*
@@ -1970,6 +1975,7 @@ extern unsigned int sysctl_sched_latency
 extern unsigned int sysctl_sched_min_granularity;
 extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_child_runs_first;
+extern unsigned int sysctl_sched_migration_cost;
 
 enum sched_tunable_scaling {
 	SCHED_TUNABLESCALING_NONE,
@@ -1980,7 +1986,6 @@ enum sched_tunable_scaling {
 extern enum sched_tunable_scaling sysctl_sched_tunable_scaling;
 
 #ifdef CONFIG_SCHED_DEBUG
-extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
 extern unsigned int sysctl_sched_time_avg;
 extern unsigned int sysctl_timer_migration;
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -1417,6 +1417,11 @@ static inline bool got_nohz_idle_kick(vo
 	return idle_cpu(smp_processor_id()) && this_rq()->nohz_balance_kick;
 }
 
+int sched_needs_cpu(int cpu)
+{
+	return  cpu_rq(cpu)->avg_idle < sysctl_sched_migration_cost;
+}
+
 #else /* CONFIG_NO_HZ */
 
 static inline bool got_nohz_idle_kick(void)
--- a/kernel/sched_fair.c
+++ b/kernel/sched_fair.c
@@ -88,7 +88,7 @@ unsigned int sysctl_sched_compat_yield _
 unsigned int sysctl_sched_wakeup_granularity = 2500000UL;
 unsigned int normalized_sysctl_sched_wakeup_granularity = 2500000UL;
 
-const_debug unsigned int sysctl_sched_migration_cost = 500000UL;
+unsigned int __read_mostly sysctl_sched_migration_cost = 500000UL;
 
 /*
  * The exponential sliding  window over which load is averaged for shares
--- a/kernel/time/tick-sched.c
+++ b/kernel/time/tick-sched.c
@@ -323,8 +323,8 @@ void tick_nohz_stop_sched_tick(int inidl
 		time_delta = timekeeping_max_deferment();
 	} while (read_seqretry(&xtime_lock, seq));
 
-	if (rcu_needs_cpu(cpu) || printk_needs_cpu(cpu) ||
-	    arch_needs_cpu(cpu)) {
+	if (sched_needs_cpu(cpu) || rcu_needs_cpu(cpu) ||
+	    printk_needs_cpu(cpu) || arch_needs_cpu(cpu)) {
 		next_jiffies = last_jiffies + 1;
 		delta_jiffies = 1;
 	} else {
