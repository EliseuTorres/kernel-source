From: Miklos Szeredi <mszeredi@suse.cz>
Subject: mm: prevent concurrent unmap_mapping_range() on the same inode
Patch-mainline: never

This was fixed in 2aa15890f3c upstream, but this backported causes
kABI breakage.

So we revert that the patch in:
patches.kabi/revert-mm-prevent-concurrent-unmap_mapping_range-on-.patch

and introduce this, a bit uglier, fix instead.

Signed-off-by: Miklos Szeredi <mszeredi@suse.cz>
Acked-by: Jiri Slaby <jslaby@suse.cz>
---
 include/linux/pagemap.h |    1 +
 mm/memory.c             |   14 ++++++++++++++
 2 files changed, 15 insertions(+)

Index: linux.git/include/linux/pagemap.h
===================================================================
--- linux.git.orig/include/linux/pagemap.h	2010-11-26 10:52:17.000000000 +0100
+++ linux.git/include/linux/pagemap.h	2010-12-11 13:39:32.000000000 +0100
@@ -24,6 +24,7 @@ enum mapping_flags {
 	AS_ENOSPC	= __GFP_BITS_SHIFT + 1,	/* ENOSPC on async write */
 	AS_MM_ALL_LOCKS	= __GFP_BITS_SHIFT + 2,	/* under mm_take_all_locks() */
 	AS_UNEVICTABLE	= __GFP_BITS_SHIFT + 3,	/* e.g., ramdisk, SHM_LOCK */
+	AS_UNMAPPING	= __GFP_BITS_SHIFT + 4, /* for unmap_mapping_range() */
 };
 
 static inline void mapping_set_error(struct address_space *mapping, int error)
Index: linux.git/mm/memory.c
===================================================================
--- linux.git.orig/mm/memory.c	2010-12-11 13:07:28.000000000 +0100
+++ linux.git/mm/memory.c	2010-12-11 14:09:42.000000000 +0100
@@ -2535,6 +2535,12 @@ static inline void unmap_mapping_range_l
 	}
 }
 
+static int mapping_sleep(void *x)
+{
+	schedule();
+	return 0;
+}
+
 /**
  * unmap_mapping_range - unmap the portion of all mmaps in the specified address_space corresponding to the specified page range in the underlying file.
  * @mapping: the address space containing mmaps to be unmapped.
@@ -2572,6 +2578,9 @@ void unmap_mapping_range(struct address_
 		details.last_index = ULONG_MAX;
 	details.i_mmap_lock = &mapping->i_mmap_lock;
 
+	wait_on_bit_lock(&mapping->flags, AS_UNMAPPING, mapping_sleep,
+			 TASK_UNINTERRUPTIBLE);
+
 	spin_lock(&mapping->i_mmap_lock);
 
 	/* Protect against endless unmapping loops */
@@ -2588,6 +2597,11 @@ void unmap_mapping_range(struct address_
 	if (unlikely(!list_empty(&mapping->i_mmap_nonlinear)))
 		unmap_mapping_range_list(&mapping->i_mmap_nonlinear, &details);
 	spin_unlock(&mapping->i_mmap_lock);
+
+	clear_bit_unlock(AS_UNMAPPING, &mapping->flags);
+	smp_mb__after_clear_bit();
+	wake_up_bit(&mapping->flags, AS_UNMAPPING);
+
 }
 EXPORT_SYMBOL(unmap_mapping_range);
 

