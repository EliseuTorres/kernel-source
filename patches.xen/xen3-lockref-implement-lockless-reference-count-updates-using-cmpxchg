From bc08b449ee14ace4d869adaa1bb35a44ce68d775 Mon Sep 17 00:00:00 2001
From: Linus Torvalds <torvalds@linux-foundation.org>
Date: Mon, 2 Sep 2013 12:12:15 -0700
Subject: lockref: implement lockless reference count updates using cmpxchg()
Patch-mainline: v3.12-rc1
References: FATE#317271

Instead of taking the spinlock, the lockless versions atomically check
that the lock is not taken, and do the reference count update using a
cmpxchg() loop.  This is semantically identical to doing the reference
count update protected by the lock, but avoids the "wait for lock"
contention that you get when accesses to the reference count are
contended.

Note that a "lockref" is absolutely _not_ equivalent to an atomic_t.
Even when the lockref reference counts are updated atomically with
cmpxchg, the fact that they also verify the state of the spinlock means
that the lockless updates can never happen while somebody else holds the
spinlock.

So while "lockref_put_or_lock()" looks a lot like just another name for
"atomic_dec_and_lock()", and both optimize to lockless updates, they are
fundamentally different: the decrement done by atomic_dec_and_lock() is
truly independent of any lock (as long as it doesn't decrement to zero),
so a locked region can still see the count change.

The lockref structure, in contrast, really is a *locked* reference
count.  If you hold the spinlock, the reference count will be stable and
you can modify the reference count without using atomics, because even
the lockless updates will see and respect the state of the lock.

In order to enable the cmpxchg lockless code, the architecture needs to
do three things:

 (1) Make sure that the "arch_spinlock_t" and an "unsigned int" can fit
     in an aligned u64, and have a "cmpxchg()" implementation that works
     on such a u64 data type.

 (2) define a helper function to test for a spinlock being unlocked
     ("arch_spin_value_unlocked()")

 (3) select the "ARCH_USE_CMPXCHG_LOCKREF" config variable in its
     Kconfig file.

This enables it for x86-64 (but not 32-bit, we'd need to make sure
cmpxchg() turns into the proper cmpxchg8b in order to enable it for
32-bit mode).

Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Acked-by: Jeff Mahoney <jeffm@suse.com>
Automatically created from "patches.suse/lockref-implement-lockless-reference-count-updates-using-cmpxchg" by xen-port-patches.py

--- sle11sp3.orig/arch/x86/include/mach-xen/asm/spinlock.h	2012-10-19 09:33:06.000000000 +0200
+++ sle11sp3/arch/x86/include/mach-xen/asm/spinlock.h	2014-06-04 13:51:23.000000000 +0200
@@ -247,6 +247,12 @@ static __always_inline void __ticket_spi
 #undef __ticket_spin_unlock_body
 #endif
 
+static __always_inline int __ticket_spin_value_unlocked(arch_spinlock_t lock)
+{
+	return !(((lock.slock >> TICKET_SHIFT) ^ lock.slock) &
+		 ((1 << TICKET_SHIFT) - 1));
+}
+
 static inline int __ticket_spin_is_locked(arch_spinlock_t *lock)
 {
 	int tmp = ACCESS_ONCE(lock->slock);
@@ -268,6 +274,11 @@ static inline int __ticket_spin_is_conte
 static inline int xen_spinlock_init(unsigned int cpu) { return 0; }
 static inline void xen_spinlock_cleanup(unsigned int cpu) {}
 
+static __always_inline int __byte_spin_value_unlocked(arch_spinlock_t lock)
+{
+	return lock.lock == 0;
+}
+
 static inline int __byte_spin_is_locked(arch_spinlock_t *lock)
 {
 	return lock->lock != 0;
@@ -317,6 +328,11 @@ static inline void __byte_spin_unlock(ar
 
 #endif /* TICKET_SHIFT */
 
+static __always_inline int arch_spin_value_unlocked(arch_spinlock_t lock)
+{
+	return __arch_spin(value_unlocked)(lock);
+}
+
 static inline int arch_spin_is_locked(arch_spinlock_t *lock)
 {
 	return __arch_spin(is_locked)(lock);
