From http://xenbits.xen.org/hg/linux-2.6.18-xen.hg/rev/5108c6901b30
From: xen-devel@lists.xen.org
Patch-mainline: n/a
Subject: netback: shutdown the ring if it contains garbage
References: CVE-2013-0216 XSA-39 bnc#800280

A buggy or malicious frontend should not be able to confuse netback.
If we spot anything which is not as it should be then shutdown the
device and don't try to continue with the ring in a potentially
hostile state. Well behaved and non-hostile frontends will not be
penalised.

As well as making the existing checks for such errors fatal also add a
new check that ensures that there isn't an insane number of requests
on the ring (i.e. more than would fit in the ring). If the ring
contains garbage then previously is was possible to loop over this
insane number, getting an error each time and therefore not generating
any more pending requests and therefore not exiting the loop in
xen_netbk_tx_build_gops for an externded period.

Also turn various netdev_dbg calls which no precipitate a fatal error
into netdev_err, they are rate limited because the device is shutdown
afterwards.

This fixes at least one known DoS/softlockup of the backend domain.

Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
Signed-off-by: Jan Beulich <jbeulich@suse.com>

From http://xenbits.xen.org/hg/linux-2.6.18-xen.hg/rev/ff0befcaac09
From: xen-devel@lists.xen.org
Patch-mainline: n/a
Subject: netback: fix shutting down the ring if it contains garbage

Using rtnl_lock() in tasklet context is not permitted.

This undoes the part of 1219:5108c6901b30 that split off
xenvif_carrier_off() from netif_disconnect().

Signed-off-by: Jan Beulich <jbeulich@suse.com>

--- sle11sp2.orig/drivers/xen/netback/common.h	2012-01-03 00:00:00.000000000 +0100
+++ sle11sp2/drivers/xen/netback/common.h	2013-02-15 13:22:52.000000000 +0100
@@ -77,6 +77,8 @@ typedef struct netif_st {
 	u8 can_queue:1;	/* can queue packets for receiver? */
 	u8 copying_receiver:1;	/* copy packets to receiver?       */
 
+	u8 busted:1;
+
 	/* Allow netif_be_start_xmit() to peek ahead in the rx request ring. */
 	RING_IDX rx_req_cons_peek;
 
@@ -194,7 +196,8 @@ int netif_map(struct backend_info *be, g
 void netif_xenbus_init(void);
 
 #define netif_schedulable(netif)				\
-	(netif_running((netif)->dev) && netback_carrier_ok(netif))
+	(likely(!(netif)->busted) &&				\
+	 netif_running((netif)->dev) &&	netback_carrier_ok(netif))
 
 void netif_schedule_work(netif_t *netif);
 void netif_deschedule_work(netif_t *netif);
--- sle11sp2.orig/drivers/xen/netback/interface.c	2013-02-18 09:50:59.000000000 +0100
+++ sle11sp2/drivers/xen/netback/interface.c	2013-02-15 13:22:52.000000000 +0100
@@ -56,6 +56,10 @@ module_param_named(queue_length, netbk_q
 
 static void __netif_up(netif_t *netif)
 {
+	if (netif->busted) {
+		netif->busted = 0;
+		enable_irq(netif->irq);
+	}
 	enable_irq(netif->irq);
 	netif_schedule_work(netif);
 }
--- sle11sp2.orig/drivers/xen/netback/netback.c	2013-02-18 09:50:59.000000000 +0100
+++ sle11sp2/drivers/xen/netback/netback.c	2013-02-15 13:22:52.000000000 +0100
@@ -853,7 +853,7 @@ void netif_schedule_work(netif_t *netif)
 	RING_FINAL_CHECK_FOR_REQUESTS(&netif->tx, more_to_do);
 #endif
 
-	if (more_to_do) {
+	if (more_to_do && likely(!netif->busted)) {
 		add_to_net_schedule_list_tail(netif);
 		maybe_schedule_tx_action();
 	}
@@ -1028,6 +1028,16 @@ static void netbk_tx_err(netif_t *netif,
 	netif_put(netif);
 }
 
+static void netbk_fatal_tx_err(netif_t *netif)
+{
+	printk(KERN_ERR "%s: fatal error; disabling device\n",
+	       netif->dev->name);
+	netif->busted = 1;
+	disable_irq(netif->irq);
+	netif_deschedule_work(netif);
+	netif_put(netif);
+}
+
 static int netbk_count_requests(netif_t *netif, netif_tx_request_t *first,
 				netif_tx_request_t *txp, int work_to_do)
 {
@@ -1039,19 +1049,25 @@ static int netbk_count_requests(netif_t 
 
 	do {
 		if (frags >= work_to_do) {
-			DPRINTK("Need more frags\n");
+			printk(KERN_ERR "%s: Need more frags\n",
+			       netif->dev->name);
+			netbk_fatal_tx_err(netif);
 			return -frags;
 		}
 
 		if (unlikely(frags >= MAX_SKB_FRAGS)) {
-			DPRINTK("Too many frags\n");
+			printk(KERN_ERR "%s: Too many frags\n",
+			       netif->dev->name);
+			netbk_fatal_tx_err(netif);
 			return -frags;
 		}
 
 		memcpy(txp, RING_GET_REQUEST(&netif->tx, cons + frags),
 		       sizeof(*txp));
 		if (txp->size > first->size) {
-			DPRINTK("Frags galore\n");
+			printk(KERN_ERR "%s: Frag is bigger than frame.\n",
+			       netif->dev->name);
+			netbk_fatal_tx_err(netif);
 			return -frags;
 		}
 
@@ -1059,8 +1075,9 @@ static int netbk_count_requests(netif_t 
 		frags++;
 
 		if (unlikely((txp->offset + txp->size) > PAGE_SIZE)) {
-			DPRINTK("txp->offset: %x, size: %u\n",
-				txp->offset, txp->size);
+			printk(KERN_ERR "%s: txp->offset: %x, size: %u\n",
+			       netif->dev->name, txp->offset, txp->size);
+			netbk_fatal_tx_err(netif);
 			return -frags;
 		}
 	} while ((txp++)->flags & NETTXF_more_data);
@@ -1203,7 +1220,9 @@ int netbk_get_extras(netif_t *netif, str
 
 	do {
 		if (unlikely(work_to_do-- <= 0)) {
-			DPRINTK("Missing extra info\n");
+			printk(KERN_ERR "%s: Missing extra info\n",
+			       netif->dev->name);
+			netbk_fatal_tx_err(netif);
 			return -EBADR;
 		}
 
@@ -1212,7 +1231,9 @@ int netbk_get_extras(netif_t *netif, str
 		if (unlikely(!extra.type ||
 			     extra.type >= XEN_NETIF_EXTRA_TYPE_MAX)) {
 			netif->tx.req_cons = ++cons;
-			DPRINTK("Invalid extra type: %d\n", extra.type);
+			printk(KERN_ERR "%s: Invalid extra type: %d\n",
+			       netif->dev->name, extra.type);
+			netbk_fatal_tx_err(netif);
 			return -EINVAL;
 		}
 
@@ -1223,16 +1244,21 @@ int netbk_get_extras(netif_t *netif, str
 	return work_to_do;
 }
 
-static int netbk_set_skb_gso(struct sk_buff *skb, struct netif_extra_info *gso)
+static int netbk_set_skb_gso(netif_t *netif, struct sk_buff *skb,
+			     struct netif_extra_info *gso)
 {
 	if (!gso->u.gso.size) {
-		DPRINTK("GSO size must not be zero.\n");
+		printk(KERN_ERR "%s: GSO size must not be zero.\n",
+		       netif->dev->name);
+		netbk_fatal_tx_err(netif);
 		return -EINVAL;
 	}
 
 	/* Currently only TCPv4 S.O. is supported. */
 	if (gso->u.gso.type != XEN_NETIF_GSO_TYPE_TCPV4) {
-		DPRINTK("Bad GSO type %d.\n", gso->u.gso.type);
+		printk(KERN_ERR "%s: Bad GSO type %d.\n",
+		       netif->dev->name, gso->u.gso.type);
+		netbk_fatal_tx_err(netif);
 		return -EINVAL;
 	}
 
@@ -1267,9 +1293,30 @@ static void net_tx_action(unsigned long 
 		!list_empty(&net_schedule_list)) {
 		/* Get a netif from the list with work to do. */
 		netif = poll_net_schedule_list();
+		/*
+		 * This can sometimes happen because the test of
+		 * list_empty(net_schedule_list) at the top of the
+		 * loop is unlocked.  Just go back and have another
+		 * look.
+		 */
 		if (!netif)
 			continue;
 
+		if (unlikely(netif->busted)) {
+			netif_put(netif);
+			continue;
+		}
+
+		if (netif->tx.sring->req_prod - netif->tx.req_cons >
+		    NET_TX_RING_SIZE) {
+			printk(KERN_ERR "%s: Impossible number of requests. "
+			       "req_prod %u, req_cons %u, size %lu\n",
+			       netif->dev->name, netif->tx.sring->req_prod,
+			       netif->tx.req_cons, NET_TX_RING_SIZE);
+			netbk_fatal_tx_err(netif);
+			continue;
+		}
+
 		RING_FINAL_CHECK_FOR_REQUESTS(&netif->tx, work_to_do);
 		if (!work_to_do) {
 			netif_put(netif);
@@ -1321,17 +1368,14 @@ static void net_tx_action(unsigned long 
 			work_to_do = netbk_get_extras(netif, extras,
 						      work_to_do);
 			i = netif->tx.req_cons;
-			if (unlikely(work_to_do < 0)) {
-				netbk_tx_err(netif, &txreq, i);
+			if (unlikely(work_to_do < 0))
 				continue;
-			}
 		}
 
 		ret = netbk_count_requests(netif, &txreq, txfrags, work_to_do);
-		if (unlikely(ret < 0)) {
-			netbk_tx_err(netif, &txreq, i - ret);
+		if (unlikely(ret < 0))
 			continue;
-		}
+
 		i += ret;
 
 		if (unlikely(txreq.size < ETH_HLEN)) {
@@ -1342,10 +1386,10 @@ static void net_tx_action(unsigned long 
 
 		/* No crossing a page as the payload mustn't fragment. */
 		if (unlikely((txreq.offset + txreq.size) > PAGE_SIZE)) {
-			DPRINTK("txreq.offset: %x, size: %u, end: %lu\n", 
-				txreq.offset, txreq.size, 
-				(txreq.offset &~PAGE_MASK) + txreq.size);
-			netbk_tx_err(netif, &txreq, i);
+			printk(KERN_ERR "%s: txreq.offset: %x, size: %u, end: %lu\n",
+			       netif->dev->name, txreq.offset, txreq.size,
+			       (txreq.offset & ~PAGE_MASK) + txreq.size);
+			netbk_fatal_tx_err(netif);
 			continue;
 		}
 
@@ -1370,9 +1414,9 @@ static void net_tx_action(unsigned long 
 			struct netif_extra_info *gso;
 			gso = &extras[XEN_NETIF_EXTRA_TYPE_GSO - 1];
 
-			if (netbk_set_skb_gso(skb, gso)) {
+			if (netbk_set_skb_gso(netif, skb, gso)) {
+				/* Failure in netbk_set_skb_gso is fatal. */
 				kfree_skb(skb);
-				netbk_tx_err(netif, &txreq, i);
 				continue;
 			}
 		}
