From: Daniel Vetter <daniel.vetter@ffwll.ch>
Date: Thu Jan 3 09:04:48 2013 -0500
Subject: drm/i915: Implement workaround for broken CS tlb on i830/845
Patch-Mainline: Backported from b45305fce5bb1abec263fcff9d81ebecd6306ede
Git-commit: 44624002918cf7bb74484a9f0203e38741525a6e
Git-repo: git://kernel.opensuse.org/kernel.git
References: bnc #758040
Signed-off-by: Egbert Eich <eich@suse.com>

Backported to SLE11 SP2 kernel.

Now that Chris Wilson demonstrated that the key for stability on early
gen 2 is to simple _never_ exchange the physical backing storage of
batch buffers I've tried a stab at a kernel solution. Doesn't look too
nefarious imho, now that I don't try to be too clever for my own good
any more.

v2: After discussing the various techniques, we've decided to always blit
batches on the suspect devices, but allow userspace to opt out of the
kernel workaround assume full responsibility for providing coherent
batches. The principal reason is that avoiding the blit does improve
performance in a few key microbenchmarks and also in cairo-trace
replays.

Signed-Off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
[danvet:
- Drop the hunk which uses HAS_BROKEN_CS_TLB to implement the ring
  wrap w/a. Suggested by Chris Wilson.
- Also add the ACTHD check from Chris Wilson for the error state
  dumping, so that we still catch batches when userspace opts out of
  the w/a.]
Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
Signed-off-by: Egbert Eich <eich@suse.de>
---
 drivers/gpu/drm/i915/i915_dma.c            |    3 +
 drivers/gpu/drm/i915/i915_drv.h            |    4 +
 drivers/gpu/drm/i915/i915_gem_execbuffer.c |    9 ++-
 drivers/gpu/drm/i915/i915_irq.c            |   12 ++++
 drivers/gpu/drm/i915/intel_ringbuffer.c    |   79 +++++++++++++++++++++++++----
 drivers/gpu/drm/i915/intel_ringbuffer.h    |    4 +
 include/drm/i915_drm.h                     |   10 +++
 7 files changed, 108 insertions(+), 13 deletions(-)

--- linux-3.0.orig/drivers/gpu/drm/i915/i915_dma.c
+++ linux-3.0/drivers/gpu/drm/i915/i915_dma.c
@@ -783,6 +783,9 @@ static int i915_getparam(struct drm_devi
 	case I915_PARAM_HAS_GEN7_SOL_RESET:
 		value = 1;
 		break;
+	case I915_PARAM_HAS_PINNED_BATCHES:
+		value = 1;
+		break;
 	default:
 		DRM_DEBUG_DRIVER("Unknown parameter %d\n",
 				 param->param);
--- linux-3.0.orig/drivers/gpu/drm/i915/i915_drv.h
+++ linux-3.0/drivers/gpu/drm/i915/i915_drv.h
@@ -897,6 +897,7 @@ struct drm_i915_gem_object {
 	 */
 	atomic_t pending_flip;
 };
+#define to_gem_object(obj) (&((struct drm_i915_gem_object *)(obj))->base)
 
 #define to_intel_bo(x) container_of(x, struct drm_i915_gem_object, base)
 
@@ -978,6 +979,9 @@ struct drm_i915_file_private {
 #define HAS_OVERLAY(dev)		(INTEL_INFO(dev)->has_overlay)
 #define OVERLAY_NEEDS_PHYSICAL(dev)	(INTEL_INFO(dev)->overlay_needs_physical)
 
+/* Early gen2 have a totally busted CS tlb and require pinned batches. */
+#define HAS_BROKEN_CS_TLB(dev)		(IS_I830(dev) || IS_845G(dev))
+
 /* With the 945 and later, Y tiling got adjusted so that it was 32 128-byte
  * rows, which changed the alignment requirements and fence programming.
  */
--- linux-3.0.orig/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+++ linux-3.0/drivers/gpu/drm/i915/i915_gem_execbuffer.c
@@ -1031,6 +1031,7 @@ i915_gem_do_execbuffer(struct drm_device
 	u32 exec_start, exec_len;
 	u32 seqno;
 	u32 mask;
+	u32 flags;
 	int ret, mode, i;
 
 	if (!i915_gem_check_execbuffer(args)) {
@@ -1042,6 +1043,10 @@ i915_gem_do_execbuffer(struct drm_device
 	if (ret)
 		return ret;
 
+	flags = 0;
+	if (args->flags & I915_EXEC_IS_PINNED)
+		flags |= I915_DISPATCH_PINNED;
+
 	switch (args->flags & I915_EXEC_RING_MASK) {
 	case I915_EXEC_DEFAULT:
 	case I915_EXEC_RENDER:
@@ -1253,12 +1258,12 @@ i915_gem_do_execbuffer(struct drm_device
 				goto err;
 
 			ret = ring->dispatch_execbuffer(ring,
-							exec_start, exec_len);
+							exec_start, exec_len, flags);
 			if (ret)
 				goto err;
 		}
 	} else {
-		ret = ring->dispatch_execbuffer(ring, exec_start, exec_len);
+		ret = ring->dispatch_execbuffer(ring, exec_start, exec_len, flags);
 		if (ret)
 			goto err;
 	}
--- linux-3.0.orig/drivers/gpu/drm/i915/i915_irq.c
+++ linux-3.0/drivers/gpu/drm/i915/i915_irq.c
@@ -851,6 +851,18 @@ i915_error_first_batchbuffer(struct drm_
 	if (!ring->get_seqno)
 		return NULL;
 
+	if (HAS_BROKEN_CS_TLB(dev_priv->dev)) {
+		u32 acthd = I915_READ(ACTHD);
+
+		if (WARN_ON(ring->id != RCS))
+			return NULL;
+
+		obj = ring->private;
+		if (acthd >= obj->gtt_offset &&
+		    acthd < obj->gtt_offset + obj->base.size)
+			return i915_error_object_create(dev_priv, obj);
+	}
+
 	seqno = ring->get_seqno(ring);
 	list_for_each_entry(obj, &dev_priv->mm.active_list, mm_list) {
 		if (obj->ring != ring)
--- linux-3.0.orig/drivers/gpu/drm/i915/intel_ringbuffer.c
+++ linux-3.0/drivers/gpu/drm/i915/intel_ringbuffer.c
@@ -427,9 +427,14 @@ static int init_render_ring(struct intel
 
 static void render_ring_cleanup(struct intel_ring_buffer *ring)
 {
+	struct drm_device *dev = ring->dev;
+
 	if (!ring->private)
 		return;
 
+	if (HAS_BROKEN_CS_TLB(dev))
+		drm_gem_object_unreference(to_gem_object(ring->private));
+
 	cleanup_pipe_control(ring);
 }
 
@@ -887,8 +892,10 @@ bsd_ring_put_irq(struct intel_ring_buffe
 	spin_unlock(&ring->irq_lock);
 }
 
+/* Just userspace ABI convention to limit the wa batch bo to a resonable size */
+#define I830_BATCH_LIMIT (256*1024)
 static int
-ring_dispatch_execbuffer(struct intel_ring_buffer *ring, u32 offset, u32 length)
+ring_dispatch_execbuffer(struct intel_ring_buffer *ring, u32 offset, u32 length, unsigned flags)
 {
 	int ret;
 
@@ -907,20 +914,51 @@ ring_dispatch_execbuffer(struct intel_ri
 
 static int
 render_ring_dispatch_execbuffer(struct intel_ring_buffer *ring,
-				u32 offset, u32 len)
+				u32 offset, u32 len, unsigned flags)
 {
 	struct drm_device *dev = ring->dev;
 	int ret;
 
 	if (IS_I830(dev) || IS_845G(dev)) {
-		ret = intel_ring_begin(ring, 4);
-		if (ret)
-			return ret;
+		if (flags & I915_DISPATCH_PINNED) {
+			ret = intel_ring_begin(ring, 4);
+			if (ret)
+				return ret;
 
-		intel_ring_emit(ring, MI_BATCH_BUFFER);
-		intel_ring_emit(ring, offset | MI_BATCH_NON_SECURE);
-		intel_ring_emit(ring, offset + len - 8);
-		intel_ring_emit(ring, 0);
+			intel_ring_emit(ring, MI_BATCH_BUFFER);
+			intel_ring_emit(ring, offset | MI_BATCH_NON_SECURE);
+			intel_ring_emit(ring, offset + len - 8);
+			intel_ring_emit(ring, 0);
+		} else {
+			struct drm_i915_gem_object *obj = ring->private;
+			u32 cs_offset = obj->gtt_offset;
+
+			if (len > I830_BATCH_LIMIT)
+				return -ENOSPC;
+
+			ret = intel_ring_begin(ring, 9+3);
+			if (ret)
+				return ret;
+			/* Blit the batch (which has now all relocs applied) to the stable batch
+			 * scratch bo area (so that the CS never stumbles over its tlb
+			 * invalidation bug) ... */
+			intel_ring_emit(ring, XY_SRC_COPY_BLT_CMD |
+					XY_SRC_COPY_BLT_WRITE_ALPHA |
+					XY_SRC_COPY_BLT_WRITE_RGB);
+			intel_ring_emit(ring, BLT_DEPTH_32 | BLT_ROP_GXCOPY | 4096);
+			intel_ring_emit(ring, 0);
+			intel_ring_emit(ring, (DIV_ROUND_UP(len, 4096) << 16) | 1024);
+			intel_ring_emit(ring, cs_offset);
+			intel_ring_emit(ring, 0);
+			intel_ring_emit(ring, 4096);
+			intel_ring_emit(ring, offset);
+			intel_ring_emit(ring, MI_FLUSH);
+
+			/* ... and execute it. */
+			intel_ring_emit(ring, MI_BATCH_BUFFER);
+			intel_ring_emit(ring, cs_offset | MI_BATCH_NON_SECURE);
+			intel_ring_emit(ring, cs_offset + len - 8);
+		}
 	} else {
 		ret = intel_ring_begin(ring, 2);
 		if (ret)
@@ -1297,7 +1335,7 @@ static int gen6_ring_flush(struct intel_
 
 static int
 gen6_ring_dispatch_execbuffer(struct intel_ring_buffer *ring,
-			      u32 offset, u32 len)
+			      u32 offset, u32 len, unsigned flags)
 {
        int ret;
 
@@ -1524,6 +1562,27 @@ int intel_init_render_ring_buffer(struct
 		memset(ring->status_page.page_addr, 0, PAGE_SIZE);
 	}
 
+	/* Workaround batchbuffer to combat CS tlb bug. */
+	if (HAS_BROKEN_CS_TLB(dev)) {
+		struct drm_i915_gem_object *obj;
+		int ret;
+
+		obj = i915_gem_alloc_object(dev, I830_BATCH_LIMIT);
+		if (obj == NULL) {
+			DRM_ERROR("Failed to allocate batch bo\n");
+			return -ENOMEM;
+		}
+
+		ret = i915_gem_object_pin(obj, 0, true);
+		if (ret != 0) {
+			drm_gem_object_unreference(&obj->base);
+			DRM_ERROR("Failed to ping batch bo\n");
+			return ret;
+		}
+
+		ring->private = obj;
+	}
+
 	return intel_init_ring_buffer(dev, ring);
 }
 
--- linux-3.0.orig/drivers/gpu/drm/i915/intel_ringbuffer.h
+++ linux-3.0/drivers/gpu/drm/i915/intel_ringbuffer.h
@@ -73,7 +73,9 @@ struct  intel_ring_buffer {
 				       u32 *seqno);
 	u32		(*get_seqno)(struct intel_ring_buffer *ring);
 	int		(*dispatch_execbuffer)(struct intel_ring_buffer *ring,
-					       u32 offset, u32 length);
+					       u32 offset, u32 length,
+					       unsigned flags);
+#define I915_DISPATCH_PINNED 0x2
 	void		(*cleanup)(struct intel_ring_buffer *ring);
 	int		(*sync_to)(struct intel_ring_buffer *ring,
 				   struct intel_ring_buffer *to,
--- linux-3.0.orig/include/drm/i915_drm.h
+++ linux-3.0/include/drm/i915_drm.h
@@ -292,6 +292,7 @@ typedef struct drm_i915_irq_wait {
 #define I915_PARAM_HAS_EXEC_CONSTANTS	 14
 #define I915_PARAM_HAS_RELAXED_DELTA	 15
 #define I915_PARAM_HAS_GEN7_SOL_RESET	 16
+#define I915_PARAM_HAS_PINNED_BATCHES	 24
 
 typedef struct drm_i915_getparam {
 	int param;
@@ -657,6 +658,15 @@ struct drm_i915_gem_execbuffer2 {
 /** Resets the SO write offset registers for transform feedback on gen7. */
 #define I915_EXEC_GEN7_SOL_RESET	(1<<8)
 
+/** Inform the kernel that the batch is and will always be pinned. This
+ * negates the requirement for a workaround to be performed to avoid
+ * an incoherent CS (such as can be found on 830/845). If this flag is
+ * not passed, the kernel will endeavour to make sure the batch is
+ * coherent with the CS before execution. If this flag is passed,
+ * userspace assumes the responsibility for ensuring the same.
+ */
+#define I915_EXEC_IS_PINNED             (1<<10)
+
 struct drm_i915_gem_pin {
 	/** Handle of the buffer to be pinned. */
 	__u32 handle;
