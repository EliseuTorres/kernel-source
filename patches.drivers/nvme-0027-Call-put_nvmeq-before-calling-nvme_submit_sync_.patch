From: Matthew Wilcox <matthew.r.wilcox@intel.com>
Date: Fri, 4 Feb 2011 16:14:30 -0500
Subject: NVMe: Call put_nvmeq() before calling nvme_submit_sync_cmd()
Git-commit: b1ad37efcafe396ac3944853589688dd0ec3c64e
References: FATE#313627
Patch-Mainline: v3.7-rc1

We can't have preemption disabled when we call schedule().  Accept the
possibility that we'll get preempted, and it'll cost us some cacheline
bounces.

Signed-off-by: Matthew Wilcox <matthew.r.wilcox@intel.com>
Acked-by: Hannes Reinecke <hare@suse.de>
---
 drivers/block/nvme.c |    7 ++++++-
 1 files changed, 6 insertions(+), 1 deletions(-)

diff --git a/drivers/block/nvme.c b/drivers/block/nvme.c
index 4bfed59..1c3cd6c 100644
--- a/drivers/block/nvme.c
+++ b/drivers/block/nvme.c
@@ -842,8 +842,13 @@ static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
 	nvme_setup_prps(&c.common, sg, length);
 
 	nvmeq = get_nvmeq(ns);
-	status = nvme_submit_sync_cmd(nvmeq, &c, &result);
+	/* Since nvme_submit_sync_cmd sleeps, we can't keep preemption
+	 * disabled.  We may be preempted at any point, and be rescheduled
+	 * to a different CPU.  That will cause cacheline bouncing, but no
+	 * additional races since q_lock already protects against other CPUs.
+	 */
 	put_nvmeq(nvmeq);
+	status = nvme_submit_sync_cmd(nvmeq, &c, &result);
 
 	nvme_unmap_user_pages(dev, io.opcode & 1, io.addr, length, sg, nents);
 	put_user(result, &uio->result);
-- 
1.7.4.2

