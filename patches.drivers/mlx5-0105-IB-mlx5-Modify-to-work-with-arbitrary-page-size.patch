From: Yishai Hadas <yishaih@mellanox.com>
Date: Sun, 14 Sep 2014 16:47:55 +0300
Subject: [PATCH 105/117] IB/mlx5: Modify to work with arbitrary page size
Patch-mainline: v3.18-rc1
Git-commit: f39f86971c0cded8c2563e7dfd82c650ca9c0044
References: bsc#923036 FATE#318772

When dealing with umem objects, the driver assumed host page sizes
defined by PAGE_SHIFT.  Modify the code to use arbitrary page shift
provided by umem->page_shift to support different page sizes.

Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
Signed-off-by: Eli Cohen <eli@mellanox.com>
Signed-off-by: Roland Dreier <roland@purestorage.com>
Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
Acked-by: Cho, Yu-Chen <acho@suse.com>
---
 drivers/infiniband/hw/mlx5/mem.c |   18 ++++++++++--------
 1 file changed, 10 insertions(+), 8 deletions(-)

--- a/drivers/infiniband/hw/mlx5/mem.c
+++ b/drivers/infiniband/hw/mlx5/mem.c
@@ -54,8 +54,9 @@ void mlx5_ib_cont_pages(struct ib_umem *
 	int mask;
 	u64 len;
 	u64 pfn;
+	unsigned long page_shift = ilog2(umem->page_size);
 
-	addr = addr >> PAGE_SHIFT;
+	addr = addr >> page_shift;
 	tmp = (unsigned long)addr;
 	m = find_first_bit(&tmp, sizeof(tmp));
 	skip = 1 << m;
@@ -63,8 +64,8 @@ void mlx5_ib_cont_pages(struct ib_umem *
 	i = 0;
 	list_for_each_entry(chunk, &umem->chunk_list, list)
 		for (j = 0; j < chunk->nmap; j++) {
-			len = sg_dma_len(&chunk->page_list[j]) >> PAGE_SHIFT;
-			pfn = sg_dma_address(&chunk->page_list[j]) >> PAGE_SHIFT;
+			len = sg_dma_len(&chunk->page_list[j]) >> page_shift;
+			pfn = sg_dma_address(&chunk->page_list[j]) >> page_shift;
 			for (k = 0; k < len; k++) {
 				if (!(i & mask)) {
 					tmp = (unsigned long)pfn;
@@ -103,14 +104,15 @@ void mlx5_ib_cont_pages(struct ib_umem *
 
 		*ncont = 0;
 	}
-	*shift = PAGE_SHIFT + m;
+	*shift = page_shift + m;
 	*count = i;
 }
 
 void mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 			  int page_shift, __be64 *pas, int umr)
 {
-	int shift = page_shift - PAGE_SHIFT;
+	unsigned long umem_page_shift = ilog2(umem->page_size);
+	int shift = page_shift - umem_page_shift;
 	int mask = (1 << shift) - 1;
 	struct ib_umem_chunk *chunk;
 	int i, j, k;
@@ -121,11 +123,11 @@ void mlx5_ib_populate_pas(struct mlx5_ib
 	i = 0;
 	list_for_each_entry(chunk, &umem->chunk_list, list)
 		for (j = 0; j < chunk->nmap; j++) {
-			len = sg_dma_len(&chunk->page_list[j]) >> PAGE_SHIFT;
+			len = sg_dma_len(&chunk->page_list[j]) >> umem_page_shift;
 			base = sg_dma_address(&chunk->page_list[j]);
 			for (k = 0; k < len; k++) {
 				if (!(i & mask)) {
-					cur = base + (k << PAGE_SHIFT);
+					cur = base + (k << umem_page_shift);
 					if (umr)
 						cur |= 3;
 
@@ -134,7 +136,7 @@ void mlx5_ib_populate_pas(struct mlx5_ib
 						    i >> shift, be64_to_cpu(pas[i >> shift]));
 				}  else
 					mlx5_ib_dbg(dev, "=====> 0x%llx\n",
-						    base + (k << PAGE_SHIFT));
+						    base + (k << umem_page_shift));
 				i++;
 			}
 		}
