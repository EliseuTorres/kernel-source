From: Jiri Slaby <jslaby@suse.cz>
Date: Mon, 12 Mar 2012 15:14:30 +0100
Subject: Revert "i387: don't ever touch TS_USEDFPU directly, use helper
 functions"
Patch-mainline: never

This reverts commit 9221484f11c3902bfc84e18e6c6f50f8739134a7.

Signed-off-by: Jiri Slaby <jslaby@suse.cz>
---
 arch/x86/include/asm/i387.h |   75 ++++++++++++-------------------------------
 arch/x86/kernel/traps.c     |    2 +-
 arch/x86/kernel/xsave.c     |    2 +-
 arch/x86/kvm/vmx.c          |    2 +-
 4 files changed, 23 insertions(+), 58 deletions(-)

diff --git a/arch/x86/include/asm/i387.h b/arch/x86/include/asm/i387.h
index 730d7be..55fb3aa 100644
--- a/arch/x86/include/asm/i387.h
+++ b/arch/x86/include/asm/i387.h
@@ -280,47 +280,6 @@ static inline int restore_fpu_checking(struct task_struct *tsk)
 }
 
 /*
- * Software FPU state helpers. Careful: these need to
- * be preemption protection *and* they need to be
- * properly paired with the CR0.TS changes!
- */
-static inline int __thread_has_fpu(struct thread_info *ti)
-{
-	return ti->status & TS_USEDFPU;
-}
-
-/* Must be paired with an 'stts' after! */
-static inline void __thread_clear_has_fpu(struct thread_info *ti)
-{
-	ti->status &= ~TS_USEDFPU;
-}
-
-/* Must be paired with a 'clts' before! */
-static inline void __thread_set_has_fpu(struct thread_info *ti)
-{
-	ti->status |= TS_USEDFPU;
-}
-
-/*
- * Encapsulate the CR0.TS handling together with the
- * software flag.
- *
- * These generally need preemption protection to work,
- * do try to avoid using these on their own.
- */
-static inline void __thread_fpu_end(struct thread_info *ti)
-{
-	__thread_clear_has_fpu(ti);
-	stts();
-}
-
-static inline void __thread_fpu_begin(struct thread_info *ti)
-{
-	clts();
-	__thread_set_has_fpu(ti);
-}
-
-/*
  * Signal frame handlers...
  */
 extern int save_i387_xstate(void __user *buf);
@@ -328,21 +287,23 @@ extern int restore_i387_xstate(void __user *buf);
 
 static inline void __unlazy_fpu(struct task_struct *tsk)
 {
-	if (__thread_has_fpu(task_thread_info(tsk))) {
+	if (task_thread_info(tsk)->status & TS_USEDFPU) {
 		__save_init_fpu(tsk);
-		__thread_fpu_end(task_thread_info(tsk));
+		task_thread_info(tsk)->status &= ~TS_USEDFPU;
+		stts();
 	} else
 		tsk->fpu_counter = 0;
 }
 
 static inline void __clear_fpu(struct task_struct *tsk)
 {
-	if (__thread_has_fpu(task_thread_info(tsk))) {
+	if (task_thread_info(tsk)->status & TS_USEDFPU) {
 		/* Ignore delayed exceptions from user space */
 		asm volatile("1: fwait\n"
 			     "2:\n"
 			     _ASM_EXTABLE(1b, 2b));
-		__thread_fpu_end(task_thread_info(tsk));
+		task_thread_info(tsk)->status &= ~TS_USEDFPU;
+		stts();
 	}
 }
 
@@ -350,14 +311,14 @@ static inline void __clear_fpu(struct task_struct *tsk)
  * Were we in an interrupt that interrupted kernel mode?
  *
  * We can do a kernel_fpu_begin/end() pair *ONLY* if that
- * pair does nothing at all: the thread must not have fpu (so
+ * pair does nothing at all: TS_USEDFPU must be clear (so
  * that we don't try to save the FPU state), and TS must
  * be set (so that the clts/stts pair does nothing that is
  * visible in the interrupted kernel thread).
  */
 static inline bool interrupted_kernel_fpu_idle(void)
 {
-	return !__thread_has_fpu(current_thread_info()) &&
+	return !(current_thread_info()->status & TS_USEDFPU) &&
 		(read_cr0() & X86_CR0_TS);
 }
 
@@ -395,9 +356,9 @@ static inline void kernel_fpu_begin(void)
 
 	WARN_ON_ONCE(!irq_fpu_usable());
 	preempt_disable();
-	if (__thread_has_fpu(me)) {
+	if (me->status & TS_USEDFPU) {
 		__save_init_fpu(me->task);
-		__thread_clear_has_fpu(me);
+		me->status &= ~TS_USEDFPU;
 		/* We do 'stts()' in kernel_fpu_end() */
 	} else
 		clts();
@@ -461,21 +422,24 @@ static inline void irq_ts_restore(int TS_state)
  */
 static inline int user_has_fpu(void)
 {
-	return __thread_has_fpu(current_thread_info());
+	return current_thread_info()->status & TS_USEDFPU;
 }
 
 static inline void user_fpu_end(void)
 {
 	preempt_disable();
-	__thread_fpu_end(current_thread_info());
+	current_thread_info()->status &= ~TS_USEDFPU;
+	stts();
 	preempt_enable();
 }
 
 static inline void user_fpu_begin(void)
 {
 	preempt_disable();
-	if (!user_has_fpu())
-		__thread_fpu_begin(current_thread_info());
+	if (!user_has_fpu()) {
+		clts();
+		current_thread_info()->status |= TS_USEDFPU;
+	}
 	preempt_enable();
 }
 
@@ -484,10 +448,11 @@ static inline void user_fpu_begin(void)
  */
 static inline void save_init_fpu(struct task_struct *tsk)
 {
-	WARN_ON_ONCE(!__thread_has_fpu(task_thread_info(tsk)));
+	WARN_ON_ONCE(!(task_thread_info(tsk)->status & TS_USEDFPU));
 	preempt_disable();
 	__save_init_fpu(tsk);
-	__thread_fpu_end(task_thread_info(tsk));
+	task_thread_info(tsk)->status &= ~TS_USEDFPU;
+	stts();
 	preempt_enable();
 }
 
diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 326476d..5878de3 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -734,7 +734,7 @@ void __math_state_restore(void)
 		return;
 	}
 
-	__thread_set_has_fpu(thread);	/* clts in caller! */
+	thread->status |= TS_USEDFPU;	/* So we fnsave on switch_to() */
 	tsk->fpu_counter++;
 }
 
diff --git a/arch/x86/kernel/xsave.c b/arch/x86/kernel/xsave.c
index a0bcd0d..86f1f09 100644
--- a/arch/x86/kernel/xsave.c
+++ b/arch/x86/kernel/xsave.c
@@ -47,7 +47,7 @@ void __sanitize_i387_state(struct task_struct *tsk)
 	if (!fx)
 		return;
 
-	BUG_ON(__thread_has_fpu(task_thread_info(tsk)));
+	BUG_ON(task_thread_info(tsk)->status & TS_USEDFPU);
 
 	xstate_bv = tsk->thread.fpu.state->xsave.xsave_hdr.xstate_bv;
 
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index 6da2bae..d48ec60 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -948,7 +948,7 @@ static void __vmx_load_host_state(struct vcpu_vmx *vmx)
 #ifdef CONFIG_X86_64
 	wrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_host_kernel_gs_base);
 #endif
-	if (__thread_has_fpu(current_thread_info()))
+	if (current_thread_info()->status & TS_USEDFPU)
 		clts();
 	load_gdt(&__get_cpu_var(host_gdt));
 }
-- 
1.7.9.2

