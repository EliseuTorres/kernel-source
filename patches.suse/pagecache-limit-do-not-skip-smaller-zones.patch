From: Michal Hocko <mhocko@suse.cz>
Subject: pagecache limit: Do not skip over small zones that easily
Patch-mainline: never
References: bnc#925881

Small LRU protection in shrink_all_zones seems to be overly eager.
8*SWAP_CLUSTER_MAX with DEF_PRIORITY is 4G worth of memory needed for a single
page to reclaim. Moreover this doesn't make much sense in general. The reclaim
targed might be achieved from several zones so excluding a single based on the
whole target is simply wrong.

This patch removes the nr_pages based LRU protection. It still tries to protect
zones with smaller LRUs and doesn't allow to scan more than 1/4 of the LRU
list. If no page could be reclaimed we bail out early even without scanning
our target.

The patch should help with long stalls caused by all the page cache consumers
hitting on few zones with a sufficiently large LRU. With a throttling mechanism
introduced by patches.suse/pagecache-limit-reduce-zone-lrulock-bouncing.patch
this means that other consumers would go to sleep just to wake up little bit
later just to find out the situation is basically similar.

Signed-off-by: Michal Hocko <mhocko@suse.cz>

---
 mm/vmscan.c |   67 +++++++++++++++++++++++++++++++++---------------------------
 1 file changed, 37 insertions(+), 30 deletions(-)

--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -3123,6 +3123,39 @@ static void pagecache_reclaim_unlock_zon
  */
 DECLARE_WAIT_QUEUE_HEAD(pagecache_reclaim_wq);
 
+static int pagecache_shrink_lru(unsigned long nr_pages, struct zone *zone, enum lru_list l,
+		struct scan_control *sc, int prio)
+{
+	enum zone_stat_item ls = NR_LRU_BASE + l;
+	unsigned long lru_pages = zone_page_state(zone, ls);
+	unsigned long nr_to_scan;
+	unsigned long nr_reclaimed = 0;
+
+	/* Do not try to scan the whole LRU list */
+	nr_to_scan = min(nr_pages, lru_pages >> 2);
+
+	while (nr_to_scan > 0) {
+		unsigned long reclaimed;
+		unsigned long batch;
+
+		/*
+		 * shrink_list takes lru_lock with IRQ off so we
+		 * should be careful about really huge nr_to_scan
+		 */
+		batch = min_t(unsigned long, nr_to_scan, SWAP_CLUSTER_MAX);
+		reclaimed = shrink_list(l, batch, zone, sc, prio);
+
+		/* Do not lose much time if we are not able to reclaim anything. */
+		if (!reclaimed)
+			break;
+
+		nr_reclaimed += reclaimed;
+		nr_to_scan -= batch;
+	}
+
+	return nr_reclaimed;
+}
+
 /*
  * We had to resurect this function for __shrink_page_cache (upstream has
  * removed it and reworked shrink_all_memory by 7b51755c).
@@ -3166,42 +3199,16 @@ static int shrink_all_zones(unsigned lon
 		nr_locked_zones++;
 
 		for_each_evictable_lru(l) {
-			enum zone_stat_item ls = NR_LRU_BASE + l;
-			unsigned long lru_pages = zone_page_state(zone, ls);
 
 			/* For pass = 0, we don't shrink the active list */
 			if (pass == 0 && (l == LRU_ACTIVE_ANON ||
 						l == LRU_ACTIVE_FILE))
 				continue;
 
-			/* Original code relied on nr_saved_scan which is no
-			 * longer present so we are just considering LRU pages.
-			 * This means that the zone has to have quite large
-			 * LRU list for default priority and minimum nr_pages
-			 * size (8*SWAP_CLUSTER_MAX). In the end we will tend
-			 * to reclaim more from large zones wrt. small.
-			 * This should be OK because shrink_page_cache is called
-			 * when we are getting to short memory condition so
-			 * LRUs tend to be large.
-			 */
-			if (((lru_pages >> prio) + 1) >= nr_pages || pass > 3) {
-				unsigned long nr_to_scan;
-
-				nr_to_scan = min(nr_pages, lru_pages);
-				/* shrink_list takes lru_lock with IRQ off so we
-				 * should be careful about really huge nr_to_scan
-				 */
-				while (nr_to_scan > 0) {
-					unsigned long batch = min_t(unsigned long, nr_to_scan, SWAP_CLUSTER_MAX);
-
-					nr_reclaimed += shrink_list(l, batch, zone,
-									sc, prio);
-					if (nr_reclaimed >= nr_pages) {
-						pagecache_reclaim_unlock_zone(zone);
-						goto out_wakeup;
-					}
-					nr_to_scan -= batch;
-				}
+			nr_reclaimed += pagecache_shrink_lru(nr_pages, zone, l, sc, prio);
+			if (nr_reclaimed >= nr_pages) {
+				pagecache_reclaim_unlock_zone(zone);
+				goto out_wakeup;
 			}
 		}
 		pagecache_reclaim_unlock_zone(zone);
