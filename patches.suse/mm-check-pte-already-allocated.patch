From: Mel Gorman <mgorman@suse.de>
Date: Fri, 15 Apr 2011 09:57:45 +0100
Subject: [PATCH] mm: Check if PTE is already allocated during page fault
References: THP core (fate #311931)
Git-commit: e410d853e77fe942fd39ae1410b5cbedf2871f80

With transparent hugepage support, handle_mm_fault() has to be careful
when allocating a PTE.  It uses __pte_alloc() instead of pte_alloc_map
as pte_offset_map() cannot be used on a huge PMD. Care is taken
to ensure it a normal PMD is established before pte_offset_map()
is called.

pte_alloc_map() is smart enough to check if a PTE is already present
before calling __pte_alloc but this check was lost during the
change. As a consequence, PTEs are being allocated even when a PTE
is already in place incurring extra overhead from allocation and the
taking of the page table lock.  The newly allocate PTE does get cleaned
up but it's a performance hit which is visible in page_test from aim9.

This patch simply readds the check normally done by pte_alloc_map to
check if the PTE needs to be allocated before taking the page table
lock.

Signed-off-by: Mel Gorman <mgorman@suse.de>
---
 mm/memory.c |    2 +-
 1 files changed, 1 insertions(+), 1 deletions(-)

diff --git a/mm/memory.c b/mm/memory.c
index be433db..56e6a99 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3237,7 +3237,7 @@ int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	 * run pte_offset_map on the pmd, if an huge pmd could
 	 * materialize from under us from a different thread.
 	 */
-	if (unlikely(__pte_alloc(mm, vma, pmd, address)))
+	if (unlikely(pmd_none(*pmd)) && __pte_alloc(mm, vma, pmd, address))
 		return VM_FAULT_OOM;
 	/* if an huge pmd materialized from under us just retry later */
 	if (unlikely(pmd_trans_huge(*pmd)))

