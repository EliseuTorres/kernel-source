From: Cliff Wickman <cpw@sgi.com>
Subject: perfmon2
References: FATE#303968
Patch-mainline: never

This patch implements the perfmon2 performance monitoring interface.  
http://perfmon2.sourceforge.net. Required userspace is libpfm and pfmon

Signed-off-by: Tony Jones <tonyj@suse.de>

---
 Documentation/ABI/testing/sysfs-perfmon      |   87 
 Documentation/ABI/testing/sysfs-perfmon-fmt  |   18 
 Documentation/ABI/testing/sysfs-perfmon-pmu  |   48 
 Documentation/kernel-parameters.txt          |    3 
 Documentation/perfmon2-debugfs.txt           |  126 
 Documentation/perfmon2.txt                   |  221 
 MAINTAINERS                                  |    8 
 Makefile                                     |    1 
 arch/ia64/Kconfig                            |   10 
 arch/ia64/Makefile                           |    1 
 arch/ia64/configs/generic_defconfig          |   12 
 arch/ia64/include/asm/Kbuild                 |    4 
 arch/ia64/include/asm/hw_irq.h               |    2 
 arch/ia64/include/asm/perfmon.h              |  302 -
 arch/ia64/include/asm/perfmon_compat.h       |  167 
 arch/ia64/include/asm/perfmon_default_smpl.h |  121 
 arch/ia64/include/asm/perfmon_kern.h         |  354 +
 arch/ia64/include/asm/processor.h            |   10 
 arch/ia64/include/asm/system.h               |   18 
 arch/ia64/include/asm/thread_info.h          |    4 
 arch/ia64/include/asm/unistd.h               |   14 
 arch/ia64/kernel/Makefile                    |    3 
 arch/ia64/kernel/entry.S                     |   12 
 arch/ia64/kernel/irq_ia64.c                  |    7 
 arch/ia64/kernel/perfmon.c                   | 6840 ---------------------------
 arch/ia64/kernel/perfmon_default_smpl.c      |  296 -
 arch/ia64/kernel/perfmon_generic.h           |   45 
 arch/ia64/kernel/perfmon_itanium.h           |  115 
 arch/ia64/kernel/perfmon_mckinley.h          |  187 
 arch/ia64/kernel/perfmon_montecito.h         |  269 -
 arch/ia64/kernel/process.c                   |   98 
 arch/ia64/kernel/ptrace.c                    |    8 
 arch/ia64/kernel/setup.c                     |    3 
 arch/ia64/kernel/smpboot.c                   |   10 
 arch/ia64/kernel/sys_ia64.c                  |    8 
 arch/ia64/lib/Makefile                       |    1 
 arch/ia64/oprofile/perfmon.c                 |   36 
 arch/ia64/perfmon/Kconfig                    |   67 
 arch/ia64/perfmon/Makefile                   |   11 
 arch/ia64/perfmon/perfmon.c                  |  937 +++
 arch/ia64/perfmon/perfmon_compat.c           | 1221 ++++
 arch/ia64/perfmon/perfmon_default_smpl.c     |  273 +
 arch/ia64/perfmon/perfmon_generic.c          |  148 
 arch/ia64/perfmon/perfmon_itanium.c          |  232 
 arch/ia64/perfmon/perfmon_mckinley.c         |  290 +
 arch/ia64/perfmon/perfmon_montecito.c        |  412 +
 include/linux/Kbuild                         |    4 
 include/linux/perfmon.h                      |  213 
 include/linux/perfmon_dfl_smpl.h             |   78 
 include/linux/perfmon_fmt.h                  |   83 
 include/linux/perfmon_kern.h                 |  543 ++
 include/linux/perfmon_pmu.h                  |  193 
 include/linux/sched.h                        |    4 
 include/linux/syscalls.h                     |   30 
 kernel/sched.c                               |    1 
 kernel/sys_ni.c                              |   13 
 perfmon/Makefile                             |   12 
 perfmon/perfmon_activate.c                   |  276 +
 perfmon/perfmon_attach.c                     |  487 +
 perfmon/perfmon_ctx.c                        |  301 +
 perfmon/perfmon_ctxsw.c                      |  337 +
 perfmon/perfmon_debugfs.c                    |  168 
 perfmon/perfmon_dfl_smpl.c                   |  298 +
 perfmon/perfmon_file.c                       |  646 ++
 perfmon/perfmon_fmt.c                        |  219 
 perfmon/perfmon_hotplug.c                    |  158 
 perfmon/perfmon_init.c                       |  130 
 perfmon/perfmon_intr.c                       |  626 ++
 perfmon/perfmon_msg.c                        |  229 
 perfmon/perfmon_pmu.c                        |  640 ++
 perfmon/perfmon_priv.h                       |  178 
 perfmon/perfmon_res.c                        |  425 +
 perfmon/perfmon_rw.c                         |  643 ++
 perfmon/perfmon_sets.c                       |  873 +++
 perfmon/perfmon_smpl.c                       |  888 +++
 perfmon/perfmon_syscalls.c                   | 1067 ++++
 perfmon/perfmon_sysfs.c                      |  525 ++
 77 files changed, 15154 insertions(+), 8194 deletions(-)

Index: linux-2.6.31-master/Documentation/ABI/testing/sysfs-perfmon
===================================================================
--- /dev/null
+++ linux-2.6.31-master/Documentation/ABI/testing/sysfs-perfmon
@@ -0,0 +1,87 @@
+What:		/sys/kernel/perfmon
+Date:		Oct 2008
+KernelVersion:	2.6.27
+Contact:	eranian@gmail.com
+
+Description:	provide the configuration interface for the perfmon subsystems.
+	        The tree contains information about the detected hardware,
+		current state of the subsystem as well as some configuration
+		parameters.
+
+		The tree consists of the following entries:
+
+	/sys/kernel/perfmon/debug (read-write):
+
+		This entry is now obsolete. Perfmon uses pr_XXX() functions
+		for debugging. To enable/disable debugging output, refer to
+		the documentation about CONFIG_DYNAMIC_PRINTK_DEBUG.
+
+	/sys/kernel/perfmon/pmc_max_fast_arg (read-only):
+
+		Number of perfmon syscall arguments copied directly onto the
+   		stack (copy_from_user) for pfm_write_pmcs(). Copying to the
+		stack avoids having to allocate a buffer. The unit is the
+		number of pfarg_pmc_t structures.
+
+	/sys/kernel/perfmon/pmd_max_fast_arg (read-only):
+
+		Number of perfmon syscall arguments copied directly onto the
+   		stack (copy_from_user) for pfm_write_pmds()/pfm_read_pmds().
+		Copying to the stack avoids having to allocate a buffer. The
+		unit is the number of pfarg_pmd_t structures.
+
+	/sys/kernel/perfmon/reset_stats (write-only):
+
+		Reset the statistics collected by perfmon. Stats are available
+		per-cpu via debugfs.
+
+	/sys/kernel/perfmon/smpl_buffer_mem_cur (read-only):
+
+		Reports the amount of memory currently dedicated to sampling
+   		buffers by the kernel. The unit is byte.
+
+   	/sys/kernel/perfmon/smpl_buffer_mem_max (read-write):
+
+		Maximum amount of kernel memory usable for sampling buffers.
+		-1 means everything that is available. Unit is byte.
+
+   	/sys/kernel/perfmon/smpl_buffer_mem_cur (read-only):
+
+		Current utilization of kernel memory in bytes.
+
+   	/sys/kernel/perfmon/sys_group (read-write):
+
+		Users group allowed to create a system-wide perfmon context
+		(session).  -1 means any group.
+
+	/sys/kernel/perfmon/task_group (read-write):
+
+		Users group allowed to create a per-thread context (session).
+   		-1 means any group.
+
+	/sys/kernel/perfmon/sys_sessions_count (read-only):
+
+		Number of system-wide contexts (sessions) currently attached
+		to CPUs.
+
+	/sys/kernel/perfmon/task_sessions_count (read-only):
+
+		Number of per-thread contexts (sessions) currently attached
+		to threads.
+
+   	/sys/kernel/perfmon/version (read-only):
+
+		Perfmon interface revision number.
+
+	/sys/kernel/perfmon/arg_mem_max(read-write):
+
+		Maximum size of vector arguments expressed in bytes.
+		It can be modified but must be at least a page.
+		Default: PAGE_SIZE
+
+	/sys/kernel/perfmon/mode(read-write):
+
+		Bitmask to enable/disable certain perfmon features.
+		Currently defined:
+		- bit 0: if set, then reserved bitfield are ignored on PMC writes
+
Index: linux-2.6.31-master/Documentation/ABI/testing/sysfs-perfmon-fmt
===================================================================
--- /dev/null
+++ linux-2.6.31-master/Documentation/ABI/testing/sysfs-perfmon-fmt
@@ -0,0 +1,18 @@
+What:		/sys/kernel/perfmon/formats
+Date:		2007
+KernelVersion:	2.6.24
+Contact:	eranian@gmail.com
+
+Description:	provide description of available perfmon2 custom sampling buffer formats
+		which are implemented as independent kernel modules. Each formats gets
+		a subdir which a few entries.
+
+		The name of the subdir is the name of the sampling format. The same name
+		must be passed to pfm_create_context() to use the format.
+
+		Each subdir XX contains the following entries:
+
+	/sys/kernel/perfmon/formats/XX/version (read-only):
+
+		Version number of the format in clear text and null terminated.
+
Index: linux-2.6.31-master/Documentation/ABI/testing/sysfs-perfmon-pmu
===================================================================
--- /dev/null
+++ linux-2.6.31-master/Documentation/ABI/testing/sysfs-perfmon-pmu
@@ -0,0 +1,48 @@
+What:		/sys/kernel/perfmon/pmu
+Date:		Nov 2007
+KernelVersion:	2.6.24
+Contact:	eranian@gmail.com
+
+Description:	Provides information about the active PMU description
+		module.  The module contains the mapping of the actual
+		performance counter registers onto the logical PMU exposed by
+		perfmon.  There is at most one PMU description module loaded
+		at any time.
+
+		The sysfs PMU tree provides a description of the mapping for
+		each register. There is one subdir per config and data register
+		along an entry for the name of the PMU model.
+
+		The entries are as follows:
+
+	/sys/kernel/perfmon/pmu_desc/model (read-only):
+
+		Name of the PMU model is clear text and zero terminated.
+
+	Then, for each logical PMU register, XX, gets a subtree with the
+	following entries:
+
+	/sys/kernel/perfmon/pmu_desc/pm*XX/addr (read-only):
+
+		The physical address or index of the actual underlying hardware
+		register.  On Itanium, it corresponds to the index. But on X86
+		processor, this is the actual MSR address.
+
+	/sys/kernel/perfmon/pmu_desc/pm*XX/dfl_val (read-only):
+
+		The default value of the register in hexadecimal.
+
+	/sys/kernel/perfmon/pmu_desc/pm*XX/name (read-only):
+
+		The name of the hardware register.
+
+	/sys/kernel/perfmon/pmu_desc/pm*XX/rsvd_msk (read-only):
+
+		Bitmask of reserved bits, i.e., bits which cannot be changed
+		by applications. When a bit is set, it means the corresponding
+		bit in the actual register is reserved.
+
+	/sys/kernel/perfmon/pmu_desc/pm*XX/width (read-only):
+
+		The width in bits of the registers. This field is only
+		relevant for counter registers.
Index: linux-2.6.31-master/Documentation/kernel-parameters.txt
===================================================================
--- linux-2.6.31-master.orig/Documentation/kernel-parameters.txt
+++ linux-2.6.31-master/Documentation/kernel-parameters.txt
@@ -1998,6 +1998,9 @@ and is between 256 and 4096 characters. 
 			allocator.  This parameter is primarily	for debugging
 			and performance comparison.
 
+	perfmon_debug	[PERFMON] Enables Perfmon debug messages. Needed
+			to see traces of the early startup startup phase.
+
 	pf.		[PARIDE]
 			See Documentation/blockdev/paride.txt.
 
Index: linux-2.6.31-master/Documentation/perfmon2-debugfs.txt
===================================================================
--- /dev/null
+++ linux-2.6.31-master/Documentation/perfmon2-debugfs.txt
@@ -0,0 +1,126 @@
+		The perfmon2 debug and statistics interface
+                ------------------------------------------
+		           Stephane Eranian
+			  <eranian@gmail.com>
+
+The perfmon2 interfaces exports a set of statistics which are used to tune and
+debug the implementation. The data is composed of a set of very simple metrics
+mostly aggregated counts and durations. They instruments key points in the
+perfmon2 code, such as context switch and interrupt handling.
+
+The data is accessible via the debug filesystem (debugfs). Thus you need to
+have the filesystem support enabled in your kernel. Furthermore since, 2.6.25,
+the perfmon2 statistics interface is an optional component. It needs to be
+explicitely enabled in the kernel config file (CONFIG_PERFMON_DEBUG_FS).
+
+To access the data, the debugs filesystem must be mounted. Supposing the mount
+point is /debugfs, you would need to do:
+	$ mount -t debugs none /debugfs
+
+The data is located under the perfmon subdirectory and is organized per CPU.
+For each CPU, the same set of metrics is available, one metric per file in
+clear ASCII text.
+
+The metrics are as follows:
+
+	ctxswin_count (read-only):
+
+		Number of PMU context switch in.
+
+	ctxswin_ns (read-only):
+
+		Number of nanoseconds spent in the PMU context switch in
+		routine.  Dividing this number by the value of ctxswin_count,
+		yields average cost of the PMU context switch in.
+
+	ctxswout_count (read-only):
+
+		Number of PMU context switch out.
+
+	ctxswout_ns (read-only):
+
+		Number of nanoseconds spent in the PMU context switch in
+		routine. Dividing this number by the value of ctxswout_count,
+		yields average cost of the PMU context switch out.
+
+	fmt_handler_calls (read-only):
+
+		Number of calls to the sampling format routine that handles
+		PMU interrupts, i.e., typically the routine that records a
+		sample.
+
+	fmt_handler_ns (read-only):
+
+		Number of nanoseconds spent in the routine that handle PMU
+		interrupt in the sampling format. Dividing this number by
+		the number of calls provided by fmt_handler_calls, yields
+		average time spent in this routine.
+
+	ovfl_intr_all_count (read-only):
+
+		Number of PMU interrupts received by the kernel.
+
+
+	ovfl_intr_nmi_count (read-only):
+
+		Number of Non Maskeable Interrupts (NMI) received by the kernel
+		for perfmon. This is relevant only on X86 hardware.
+
+	ovfl_intr_ns (read-only):
+
+		Number of nanoseconds spent in the perfmon2 PMU interrupt
+		handler routine. Dividing this number of ovfl_intr_all_count
+		yields the average time to handle one PMU interrupt.
+
+	ovfl_intr_regular_count (read-only):
+
+		Number of PMU interrupts which are actually processed by
+		the perfmon interrupt handler. There may be spurious or replay
+		interrupts.
+
+	ovfl_intr_replay_count (read-only):
+
+		Number of PMU interrupts which were replayed on context switch
+		in or on event set switching. Interrupts get replayed when they
+		were in flight at the time monitoring had to be stopped.
+
+	perfmon/ovfl_intr_spurious_count (read-only):
+
+		Number of PMU interrupts which were dropped because there was
+		no active context (session).
+
+	ovfl_notify_count (read-only):
+
+		Number of user level notifications sent. Notifications are
+		appended as messages to the context queue. Notifications may
+		be sent on PMU interrupts.
+
+	pfm_restart_count (read-only):
+
+		Number of times pfm_restart() is called.
+
+	reset_pmds_count (read-only):
+
+		Number of times pfm_reset_pmds() is called.
+
+	set_switch_count (read-only):
+
+		Number of event set switches.
+
+	set_switch_ns (read-only):
+
+		Number of nanoseconds spent in the set switching routine.
+		Dividing this number by set_switch_count yields the average
+		cost of switching sets.
+
+	handle_timeout_count (read-only):
+
+		Number of times the pfm_handle_timeout() routine is called.
+		It is used for timeout-based set switching.
+
+	handle_work_count (read-only):
+
+		Number of times pfm_handle_work() is called. The routine
+		handles asynchronous perfmon2 work for per-thread contexts
+		(sessions).
+
Index: linux-2.6.31-master/Documentation/perfmon2.txt
===================================================================
--- /dev/null
+++ linux-2.6.31-master/Documentation/perfmon2.txt
@@ -0,0 +1,221 @@
+              The perfmon hardware monitoring interface
+              ------------------------------------------
+		           Stephane Eranian
+			  <eranian@gmail.com>
+
+I/ Introduction
+
+   The perfmon interface provides access to the hardware performance counters
+   of major processors. Nowadays, all processors implement some flavor of
+   performance counters which capture micro-architectural level information
+   such as the number of elapsed cycles, number of cache misses, and so on.
+
+   The interface is implemented as a set of new system calls and a set of
+   config files in /sys.
+
+   It is possible to monitor a single thread or a CPU. In either mode,
+   applications can count or sample. System-wide monitoring is supported by
+   running a monitoring session on each CPU. The interface supports event-based
+   sampling where the sampling period is expressed as the number of occurrences
+   of event, instead of just a timeout. This approach provides a better
+   granularity and flexibility.
+
+   For performance reason, it is possible to use a kernel-level sampling buffer
+   to minimize the overhead incurred by sampling. The format of the buffer,
+   what is recorded, how it is recorded, and how it is exported to user is
+   controlled by a kernel module called a sampling format. The current
+   implementation comes with a default format but it is possible to create
+   additional formats. There is an kernel registration interface for formats.
+   Each format is identified by a simple string which a tool can pass when a
+   monitoring session is created.
+
+   The interface also provides support for event set and multiplexing to work
+   around hardware limitations in the number of available counters or in how
+   events can be combined. Each set defines as many counters as the hardware
+   can support. The kernel then multiplexes the sets. The interface supports
+   time-based switching but also overflow-based switching, i.e., after n
+   overflows of designated counters.
+
+   Applications never manipulates the actual performance counter registers.
+   Instead they see a logical Performance Monitoring Unit (PMU) composed of a
+   set of config registers (PMC) and a set of data registers (PMD). Note that
+   PMD are not necessarily counters, they can be buffers. The logical PMU is
+   then mapped onto the actual PMU using a mapping table which is implemented
+   as a kernel module. The mapping is chosen once for each new processor. It is
+   visible in /sys/kernel/perfmon/pmu_desc. The kernel module is automatically
+   loaded on first use.
+
+   A monitoring session is uniquely identified by a file descriptor obtained
+   when the session is created. File sharing semantics apply to access the
+   session inside a process. A session is never inherited across fork. The file
+   descriptor can be used to receive counter overflow notifications or when the
+   sampling buffer is full. It is possible to use poll/select on the descriptor
+   to wait for notifications from multiple sessions. Similarly, the descriptor
+   supports asynchronous notifications via SIGIO.
+
+   Counters are always exported as being 64-bit wide regardless of what the
+   underlying hardware implements.
+
+II/ Kernel compilation
+
+    To enable perfmon, you need to enable CONFIG_PERFMON and also some of the
+    model-specific PMU modules.
+
+III/ OProfile interactions
+
+    The set of features offered by perfmon is rich enough to support migrating
+    Oprofile on top of it. That means that PMU programming and low-level
+    interrupt handling could be done by perfmon. The Oprofile sampling buffer
+    management code in the kernel as well as how samples are exported to users
+    could remain through the use of a sampling format. This is how Oprofile
+    works on Itanium.
+
+    The current interactions with Oprofile are:
+	- on X86: Both subsystems can be compiled into the same kernel. There
+		  is enforced mutual exclusion between the two subsystems. When
+		  there is an Oprofile session, no perfmon session can exist
+		  and vice-versa.
+
+	- On IA-64: Oprofile works on top of perfmon. Oprofile being a
+		    system-wide monitoring tool, the regular per-thread vs.
+		    system-wide session restrictions apply.
+
+	- on PPC: no integration yet. Only one subsystem can be enabled.
+	- on MIPS: no integration yet.  Only one subsystem can be enabled.
+
+IV/ User tools
+
+    We have released a simple monitoring tool to demonstrate the features of
+    the interface. The tool is called pfmon and it comes with a simple helper
+    library called libpfm. The library comes with a set of examples to show
+    how to use the kernel interface. Visit http://perfmon2.sf.net for details.
+
+    There maybe other tools available for perfmon.
+
+V/ How to program?
+
+   The best way to learn how to program perfmon, is to take a look at the
+   source code for the examples in libpfm. The source code is available from:
+
+		http://perfmon2.sf.net
+
+VI/ System calls overview
+
+   The interface is implemented by the following system calls:
+
+   * int pfm_create_context(pfarg_ctx_t *ctx, char *fmt, void *arg, size_t arg_size)
+
+      This function create a perfmon2 context. The type of context is per-thread by
+      default unless PFM_FL_SYSTEM_WIDE is passed in ctx. The sampling format name
+      is passed in fmt. Arguments to the format are passed in arg which is of size
+      arg_size. Upon successful return, the file descriptor identifying the context
+      is returned.
+
+   * int pfm_write_pmds(int fd, pfarg_pmd_t *pmds, int n)
+
+      This function is used to program the PMD registers. It is possible to pass
+      vectors of PMDs.
+
+   * int pfm_write_pmcs(int fd, pfarg_pmc_t *pmds, int n)
+
+      This function is used to program the PMC registers. It is possible to pass
+      vectors of PMDs.
+
+   * int pfm_read_pmds(int fd, pfarg_pmd_t *pmds, int n)
+
+      This function is used to read the PMD registers. It is possible to pass
+      vectors of PMDs.
+
+   * int pfm_load_context(int fd, pfarg_load_t *load)
+
+      This function is used to attach the context to a thread or CPU.
+      Thread means kernel-visible thread (NPTL). The thread identification
+      as obtained by gettid must be passed to load->load_target.
+
+      To operate on another thread (not self), it is mandatory that the thread
+      be stopped via ptrace().
+
+      To attach to a CPU, the CPU number must be specified in load->load_target
+      AND the call must be issued on that CPU. To monitor a CPU, a thread MUST
+      be pinned on that CPU.
+
+      Until the context is attached, the actual counters are not accessed.
+
+   * int pfm_unload_context(int fd)
+
+     The context is detached for the thread or CPU is was attached to.
+     As a consequence monitoring is stopped.
+
+     When monitoring another thread, the thread MUST be stopped via ptrace()
+     for this function to succeed.
+
+   * int pfm_start(int fd, pfarg_start_t *st)
+
+     Start monitoring. The context must be attached for this function to succeed.
+     Optionally, it is possible to specify the event set on which to start using the
+     st argument, otherwise just pass NULL.
+
+     When monitoring another thread, the thread MUST be stopped via ptrace()
+     for this function to succeed.
+
+   * int pfm_stop(int fd)
+
+     Stop monitoring. The context must be attached for this function to succeed.
+
+     When monitoring another thread, the thread MUST be stopped via ptrace()
+     for this function to succeed.
+
+
+   * int pfm_create_evtsets(int fd, pfarg_setdesc_t *sets, int n)
+
+     This function is used to create or change event sets. By default set 0 exists.
+     It is possible to create/change multiple sets in one call.
+
+     The context must be detached for this call to succeed.
+
+     Sets are identified by a 16-bit integer. They are sorted based on this
+     set and switching occurs in a round-robin fashion.
+
+   * int pfm_delete_evtsets(int fd, pfarg_setdesc_t *sets, int n)
+
+     Delete event sets. The context must be detached for this call to succeed.
+
+
+   * int pfm_getinfo_evtsets(int fd, pfarg_setinfo_t *sets, int n)
+
+     Retrieve information about event sets. In particular it is possible
+     to get the number of activation of a set. It is possible to retrieve
+     information about multiple sets in one call.
+
+
+   * int pfm_restart(int fd)
+
+     Indicate to the kernel that the application is done processing an overflow
+     notification. A consequence of this call could be that monitoring resumes.
+
+   * int read(fd, pfm_msg_t *msg, sizeof(pfm_msg_t))
+
+   the regular read() system call can be used with the context file descriptor to
+   receive overflow notification messages. Non-blocking read() is supported.
+
+   Each message carry information about the overflow such as which counter overflowed
+   and where the program was (interrupted instruction pointer).
+
+   * int close(int fd)
+
+   To destroy a context, the regular close() system call is used.
+
+
+VII/ /sys interface overview
+
+   Refer to Documentation/ABI/testing/sysfs-perfmon-* for a detailed description
+   of the sysfs interface of perfmon2.
+
+VIII/ debugfs interface overview
+
+  Refer to Documentation/perfmon2-debugfs.txt for a detailed description of the
+  debug and statistics interface of perfmon2.
+
+IX/ Documentation
+
+   Visit http://perfmon2.sf.net
Index: linux-2.6.31-master/MAINTAINERS
===================================================================
--- linux-2.6.31-master.orig/MAINTAINERS
+++ linux-2.6.31-master/MAINTAINERS
@@ -4094,6 +4094,14 @@ F:	arch/*/lib/perf_event.c
 F:	arch/*/kernel/perf_callchain.c
 F:	tools/perf/
 
+PERFMON SUBSYSTEM
+P:	Stephane Eranian
+M:	eranian@gmail.com
+L:	perfmon2-devel@lists.sf.net
+W:	http://perfmon2.sf.net
+T:	git kernel.org:/pub/scm/linux/kernel/git/eranian/linux-2.6
+S:	Maintained
+
 PERSONALITY HANDLING
 M:	Christoph Hellwig <hch@infradead.org>
 L:	linux-abi-devel@lists.sourceforge.net
Index: linux-2.6.31-master/Makefile
===================================================================
--- linux-2.6.31-master.orig/Makefile
+++ linux-2.6.31-master/Makefile
@@ -672,6 +672,7 @@ export mod_strip_cmd
 ifeq ($(KBUILD_EXTMOD),)
 core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/ block/
 core-$(CONFIG_KDB) += kdb/
+core-$(CONFIG_PERFMON) += perfmon/
 
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
Index: linux-2.6.31-master/arch/ia64/Kconfig
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/Kconfig
+++ linux-2.6.31-master/arch/ia64/Kconfig
@@ -531,14 +531,6 @@ config IA64_CPE_MIGRATE
 	  build this functionality as a kernel loadable module.  Installing
 	  the module will turn on the functionality.
 
-config PERFMON
-	bool "Performance monitor support"
-	help
-	  Selects whether support for the IA-64 performance monitor hardware
-	  is included in the kernel.  This makes some kernel data-structures a
-	  little bigger and slows down execution a bit, but it is generally
-	  a good idea to turn this on.  If you're unsure, say Y.
-
 config IA64_PALINFO
 	tristate "/proc/pal support"
 	help
@@ -610,6 +602,8 @@ source "drivers/firmware/Kconfig"
 
 source "fs/Kconfig.binfmt"
 
+source "arch/ia64/perfmon/Kconfig"
+
 endmenu
 
 menu "Power management and ACPI options"
Index: linux-2.6.31-master/arch/ia64/Makefile
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/Makefile
+++ linux-2.6.31-master/arch/ia64/Makefile
@@ -54,6 +54,7 @@ core-$(CONFIG_IA64_HP_ZX1)	+= arch/ia64/
 core-$(CONFIG_IA64_HP_ZX1_SWIOTLB) += arch/ia64/dig/
 core-$(CONFIG_IA64_XEN_GUEST)	+= arch/ia64/dig/
 core-$(CONFIG_IA64_SGI_SN2)	+= arch/ia64/sn/
+core-$(CONFIG_PERFMON)		+= arch/ia64/perfmon/
 core-$(CONFIG_IA64_SGI_UV)	+= arch/ia64/uv/
 core-$(CONFIG_KVM) 		+= arch/ia64/kvm/
 core-$(CONFIG_XEN)		+= arch/ia64/xen/
Index: linux-2.6.31-master/arch/ia64/configs/generic_defconfig
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/configs/generic_defconfig
+++ linux-2.6.31-master/arch/ia64/configs/generic_defconfig
@@ -209,7 +209,6 @@ CONFIG_IA32_SUPPORT=y
 CONFIG_COMPAT=y
 CONFIG_COMPAT_FOR_U64_ALIGNMENT=y
 CONFIG_IA64_MCA_RECOVERY=y
-CONFIG_PERFMON=y
 CONFIG_IA64_PALINFO=y
 # CONFIG_IA64_MC_ERR_INJECT is not set
 CONFIG_SGI_SN=y
@@ -236,6 +235,17 @@ CONFIG_BINFMT_ELF=y
 CONFIG_BINFMT_MISC=m
 
 #
+# Hardware Performance Monitoring support
+#
+CONFIG_PERFMON=y
+CONFIG_PERFMON_DEBUG=y
+CONFIG_IA64_PERFMON_COMPAT=y
+CONFIG_IA64_PERFMON_GENERIC=m
+CONFIG_IA64_PERFMON_ITANIUM=m
+CONFIG_IA64_PERFMON_MCKINLEY=m
+CONFIG_IA64_PERFMON_MONTECITO=m
+
+#
 # Power management and ACPI options
 #
 CONFIG_PM=y
Index: linux-2.6.31-master/arch/ia64/include/asm/Kbuild
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/include/asm/Kbuild
+++ linux-2.6.31-master/arch/ia64/include/asm/Kbuild
@@ -4,10 +4,12 @@ header-y += break.h
 header-y += fpu.h
 header-y += ia64regs.h
 header-y += intel_intrin.h
-header-y += perfmon_default_smpl.h
 header-y += ptrace_offsets.h
 header-y += rse.h
 header-y += ucontext.h
+header-y += perfmon.h
+header-y += perfmon_compat.h
+header-y += perfmon_default_smpl.h
 
 unifdef-y += gcc_intrin.h
 unifdef-y += intrinsics.h
Index: linux-2.6.31-master/arch/ia64/include/asm/hw_irq.h
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/include/asm/hw_irq.h
+++ linux-2.6.31-master/arch/ia64/include/asm/hw_irq.h
@@ -67,9 +67,9 @@ extern int ia64_last_device_vector;
 #define IA64_NUM_DEVICE_VECTORS		(IA64_LAST_DEVICE_VECTOR - IA64_FIRST_DEVICE_VECTOR + 1)
 
 #define IA64_MCA_RENDEZ_VECTOR		0xe8	/* MCA rendez interrupt */
-#define IA64_PERFMON_VECTOR		0xee	/* performance monitor interrupt vector */
 #define IA64_TIMER_VECTOR		0xef	/* use highest-prio group 15 interrupt for timer */
 #define	IA64_MCA_WAKEUP_VECTOR		0xf0	/* MCA wakeup (must be >MCA_RENDEZ_VECTOR) */
+#define IA64_PERFMON_VECTOR		0xf1	/* performance monitor interrupt vector */
 #define IA64_IPI_LOCAL_TLB_FLUSH	0xfc	/* SMP flush local TLB */
 #define IA64_IPI_RESCHEDULE		0xfd	/* SMP reschedule */
 #define IA64_IPI_VECTOR			0xfe	/* inter-processor interrupt vector */
Index: linux-2.6.31-master/arch/ia64/include/asm/perfmon.h
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/include/asm/perfmon.h
+++ linux-2.6.31-master/arch/ia64/include/asm/perfmon.h
@@ -1,279 +1,59 @@
 /*
- * Copyright (C) 2001-2003 Hewlett-Packard Co
- *               Stephane Eranian <eranian@hpl.hp.com>
- */
-
-#ifndef _ASM_IA64_PERFMON_H
-#define _ASM_IA64_PERFMON_H
-
-/*
- * perfmon comamnds supported on all CPU models
- */
-#define PFM_WRITE_PMCS		0x01
-#define PFM_WRITE_PMDS		0x02
-#define PFM_READ_PMDS		0x03
-#define PFM_STOP		0x04
-#define PFM_START		0x05
-#define PFM_ENABLE		0x06 /* obsolete */
-#define PFM_DISABLE		0x07 /* obsolete */
-#define PFM_CREATE_CONTEXT	0x08
-#define PFM_DESTROY_CONTEXT	0x09 /* obsolete use close() */
-#define PFM_RESTART		0x0a
-#define PFM_PROTECT_CONTEXT	0x0b /* obsolete */
-#define PFM_GET_FEATURES	0x0c
-#define PFM_DEBUG		0x0d
-#define PFM_UNPROTECT_CONTEXT	0x0e /* obsolete */
-#define PFM_GET_PMC_RESET_VAL	0x0f
-#define PFM_LOAD_CONTEXT	0x10
-#define PFM_UNLOAD_CONTEXT	0x11
-
-/*
- * PMU model specific commands (may not be supported on all PMU models)
- */
-#define PFM_WRITE_IBRS		0x20
-#define PFM_WRITE_DBRS		0x21
-
-/*
- * context flags
- */
-#define PFM_FL_NOTIFY_BLOCK    	 0x01	/* block task on user level notifications */
-#define PFM_FL_SYSTEM_WIDE	 0x02	/* create a system wide context */
-#define PFM_FL_OVFL_NO_MSG	 0x80   /* do not post overflow/end messages for notification */
-
-/*
- * event set flags
- */
-#define PFM_SETFL_EXCL_IDLE      0x01   /* exclude idle task (syswide only) XXX: DO NOT USE YET */
-
-/*
- * PMC flags
- */
-#define PFM_REGFL_OVFL_NOTIFY	0x1	/* send notification on overflow */
-#define PFM_REGFL_RANDOM	0x2	/* randomize sampling interval   */
-
-/*
- * PMD/PMC/IBR/DBR return flags (ignored on input)
+ * Copyright (c) 2001-2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
  *
- * Those flags are used on output and must be checked in case EAGAIN is returned
- * by any of the calls using a pfarg_reg_t or pfarg_dbreg_t structure.
- */
-#define PFM_REG_RETFL_NOTAVAIL	(1UL<<31) /* set if register is implemented but not available */
-#define PFM_REG_RETFL_EINVAL	(1UL<<30) /* set if register entry is invalid */
-#define PFM_REG_RETFL_MASK	(PFM_REG_RETFL_NOTAVAIL|PFM_REG_RETFL_EINVAL)
-
-#define PFM_REG_HAS_ERROR(flag)	(((flag) & PFM_REG_RETFL_MASK) != 0)
-
-typedef unsigned char pfm_uuid_t[16];	/* custom sampling buffer identifier type */
-
-/*
- * Request structure used to define a context
- */
-typedef struct {
-	pfm_uuid_t     ctx_smpl_buf_id;	 /* which buffer format to use (if needed) */
-	unsigned long  ctx_flags;	 /* noblock/block */
-	unsigned short ctx_nextra_sets;	 /* number of extra event sets (you always get 1) */
-	unsigned short ctx_reserved1;	 /* for future use */
-	int	       ctx_fd;		 /* return arg: unique identification for context */
-	void	       *ctx_smpl_vaddr;	 /* return arg: virtual address of sampling buffer, is used */
-	unsigned long  ctx_reserved2[11];/* for future use */
-} pfarg_context_t;
-
-/*
- * Request structure used to write/read a PMC or PMD
- */
-typedef struct {
-	unsigned int	reg_num;	   /* which register */
-	unsigned short	reg_set;	   /* event set for this register */
-	unsigned short	reg_reserved1;	   /* for future use */
-
-	unsigned long	reg_value;	   /* initial pmc/pmd value */
-	unsigned long	reg_flags;	   /* input: pmc/pmd flags, return: reg error */
-
-	unsigned long	reg_long_reset;	   /* reset after buffer overflow notification */
-	unsigned long	reg_short_reset;   /* reset after counter overflow */
-
-	unsigned long	reg_reset_pmds[4]; /* which other counters to reset on overflow */
-	unsigned long	reg_random_seed;   /* seed value when randomization is used */
-	unsigned long	reg_random_mask;   /* bitmask used to limit random value */
-	unsigned long   reg_last_reset_val;/* return: PMD last reset value */
-
-	unsigned long	reg_smpl_pmds[4];  /* which pmds are accessed when PMC overflows */
-	unsigned long	reg_smpl_eventid;  /* opaque sampling event identifier */
-
-	unsigned long   reg_reserved2[3];   /* for future use */
-} pfarg_reg_t;
-
-typedef struct {
-	unsigned int	dbreg_num;		/* which debug register */
-	unsigned short	dbreg_set;		/* event set for this register */
-	unsigned short	dbreg_reserved1;	/* for future use */
-	unsigned long	dbreg_value;		/* value for debug register */
-	unsigned long	dbreg_flags;		/* return: dbreg error */
-	unsigned long	dbreg_reserved2[1];	/* for future use */
-} pfarg_dbreg_t;
-
-typedef struct {
-	unsigned int	ft_version;	/* perfmon: major [16-31], minor [0-15] */
-	unsigned int	ft_reserved;	/* reserved for future use */
-	unsigned long	reserved[4];	/* for future use */
-} pfarg_features_t;
-
-typedef struct {
-	pid_t		load_pid;	   /* process to load the context into */
-	unsigned short	load_set;	   /* first event set to load */
-	unsigned short	load_reserved1;	   /* for future use */
-	unsigned long	load_reserved2[3]; /* for future use */
-} pfarg_load_t;
-
-typedef struct {
-	int		msg_type;		/* generic message header */
-	int		msg_ctx_fd;		/* generic message header */
-	unsigned long	msg_ovfl_pmds[4];	/* which PMDs overflowed */
-	unsigned short  msg_active_set;		/* active set at the time of overflow */
-	unsigned short  msg_reserved1;		/* for future use */
-	unsigned int    msg_reserved2;		/* for future use */
-	unsigned long	msg_tstamp;		/* for perf tuning/debug */
-} pfm_ovfl_msg_t;
-
-typedef struct {
-	int		msg_type;		/* generic message header */
-	int		msg_ctx_fd;		/* generic message header */
-	unsigned long	msg_tstamp;		/* for perf tuning */
-} pfm_end_msg_t;
-
-typedef struct {
-	int		msg_type;		/* type of the message */
-	int		msg_ctx_fd;		/* unique identifier for the context */
-	unsigned long	msg_tstamp;		/* for perf tuning */
-} pfm_gen_msg_t;
-
-#define PFM_MSG_OVFL	1	/* an overflow happened */
-#define PFM_MSG_END	2	/* task to which context was attached ended */
-
-typedef union {
-	pfm_ovfl_msg_t	pfm_ovfl_msg;
-	pfm_end_msg_t	pfm_end_msg;
-	pfm_gen_msg_t	pfm_gen_msg;
-} pfm_msg_t;
-
-/*
- * Define the version numbers for both perfmon as a whole and the sampling buffer format.
+ * This file contains Itanium Processor Family specific definitions
+ * for the perfmon interface.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
  */
-#define PFM_VERSION_MAJ		 2U
-#define PFM_VERSION_MIN		 0U
-#define PFM_VERSION		 (((PFM_VERSION_MAJ&0xffff)<<16)|(PFM_VERSION_MIN & 0xffff))
-#define PFM_VERSION_MAJOR(x)	 (((x)>>16) & 0xffff)
-#define PFM_VERSION_MINOR(x)	 ((x) & 0xffff)
-
+#ifndef _ASM_IA64_PERFMON_H_
+#define _ASM_IA64_PERFMON_H_
 
 /*
- * miscellaneous architected definitions
+ * arch-specific user visible interface definitions
  */
-#define PMU_FIRST_COUNTER	4	/* first counting monitor (PMC/PMD) */
-#define PMU_MAX_PMCS		256	/* maximum architected number of PMC registers */
-#define PMU_MAX_PMDS		256	/* maximum architected number of PMD registers */
-
-#ifdef __KERNEL__
-
-extern long perfmonctl(int fd, int cmd, void *arg, int narg);
-
-typedef struct {
-	void (*handler)(int irq, void *arg, struct pt_regs *regs);
-} pfm_intr_handler_desc_t;
-
-extern void pfm_save_regs (struct task_struct *);
-extern void pfm_load_regs (struct task_struct *);
 
-extern void pfm_exit_thread(struct task_struct *);
-extern int  pfm_use_debug_registers(struct task_struct *);
-extern int  pfm_release_debug_registers(struct task_struct *);
-extern void pfm_syst_wide_update_task(struct task_struct *, unsigned long info, int is_ctxswin);
-extern void pfm_inherit(struct task_struct *task, struct pt_regs *regs);
-extern void pfm_init_percpu(void);
-extern void pfm_handle_work(void);
-extern int  pfm_install_alt_pmu_interrupt(pfm_intr_handler_desc_t *h);
-extern int  pfm_remove_alt_pmu_interrupt(pfm_intr_handler_desc_t *h);
+#define PFM_ARCH_MAX_PMCS	(256+64)
+#define PFM_ARCH_MAX_PMDS	(256+64)
 
-
-
-/*
- * Reset PMD register flags
- */
-#define PFM_PMD_SHORT_RESET	0
-#define PFM_PMD_LONG_RESET	1
-
-typedef union {
-	unsigned int val;
-	struct {
-		unsigned int notify_user:1;	/* notify user program of overflow */
-		unsigned int reset_ovfl_pmds:1;	/* reset overflowed PMDs */
-		unsigned int block_task:1;	/* block monitored task on kernel exit */
-		unsigned int mask_monitoring:1; /* mask monitors via PMCx.plm */
-		unsigned int reserved:28;	/* for future use */
-	} bits;
-} pfm_ovfl_ctrl_t;
-
-typedef struct {
-	unsigned char	ovfl_pmd;			/* index of overflowed PMD  */
-	unsigned char   ovfl_notify;			/* =1 if monitor requested overflow notification */
-	unsigned short  active_set;			/* event set active at the time of the overflow */
-	pfm_ovfl_ctrl_t ovfl_ctrl;			/* return: perfmon controls to set by handler */
-
-	unsigned long   pmd_last_reset;			/* last reset value of of the PMD */
-	unsigned long	smpl_pmds[4];			/* bitmask of other PMD of interest on overflow */
-	unsigned long   smpl_pmds_values[PMU_MAX_PMDS]; /* values for the other PMDs of interest */
-	unsigned long   pmd_value;			/* current 64-bit value of the PMD */
-	unsigned long	pmd_eventid;			/* eventid associated with PMD */
-} pfm_ovfl_arg_t;
-
-
-typedef struct {
-	char		*fmt_name;
-	pfm_uuid_t	fmt_uuid;
-	size_t		fmt_arg_size;
-	unsigned long	fmt_flags;
-
-	int		(*fmt_validate)(struct task_struct *task, unsigned int flags, int cpu, void *arg);
-	int		(*fmt_getsize)(struct task_struct *task, unsigned int flags, int cpu, void *arg, unsigned long *size);
-	int 		(*fmt_init)(struct task_struct *task, void *buf, unsigned int flags, int cpu, void *arg);
-	int		(*fmt_handler)(struct task_struct *task, void *buf, pfm_ovfl_arg_t *arg, struct pt_regs *regs, unsigned long stamp);
-	int		(*fmt_restart)(struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs);
-	int		(*fmt_restart_active)(struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs);
-	int		(*fmt_exit)(struct task_struct *task, void *buf, struct pt_regs *regs);
-
-	struct list_head fmt_list;
-} pfm_buffer_fmt_t;
-
-extern int pfm_register_buffer_fmt(pfm_buffer_fmt_t *fmt);
-extern int pfm_unregister_buffer_fmt(pfm_uuid_t uuid);
+#define PFM_ARCH_PMD_STK_ARG	8
+#define PFM_ARCH_PMC_STK_ARG	8
 
 /*
- * perfmon interface exported to modules
+ * Itanium specific context flags
+ *
+ * bits[00-15]: generic flags (see asm/perfmon.h)
+ * bits[16-31]: arch-specific flags
  */
-extern int pfm_mod_read_pmds(struct task_struct *, void *req, unsigned int nreq, struct pt_regs *regs);
-extern int pfm_mod_write_pmcs(struct task_struct *, void *req, unsigned int nreq, struct pt_regs *regs);
-extern int pfm_mod_write_ibrs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs);
-extern int pfm_mod_write_dbrs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs);
+#define PFM_ITA_FL_INSECURE 0x10000 /* clear psr.sp on non system, non self */
 
 /*
- * describe the content of the local_cpu_date->pfm_syst_info field
+ * Itanium specific public event set flags (set_flags)
+ *
+ * event set flags layout:
+ * bits[00-15] : generic flags
+ * bits[16-31] : arch-specific flags
  */
-#define PFM_CPUINFO_SYST_WIDE	0x1	/* if set a system wide session exists */
-#define PFM_CPUINFO_DCR_PP	0x2	/* if set the system wide session has started */
-#define PFM_CPUINFO_EXCL_IDLE	0x4	/* the system wide session excludes the idle task */
+#define PFM_ITA_SETFL_EXCL_INTR	0x10000	 /* exclude interrupt execution */
+#define PFM_ITA_SETFL_INTR_ONLY	0x20000	 /* include only interrupt execution */
+#define PFM_ITA_SETFL_IDLE_EXCL 0x40000  /* stop monitoring in idle loop */
 
 /*
- * sysctl control structure. visible to sampling formats
+ * compatibility for version v2.0 of the interface
  */
-typedef struct {
-	int	debug;		/* turn on/off debugging via syslog */
-	int	debug_ovfl;	/* turn on/off debug printk in overflow handler */
-	int	fastctxsw;	/* turn on/off fast (unsecure) ctxsw */
-	int	expert_mode;	/* turn on/off value checking */
-} pfm_sysctl_t;
-extern pfm_sysctl_t pfm_sysctl;
-
-
-#endif /* __KERNEL__ */
+#include <asm/perfmon_compat.h>
 
-#endif /* _ASM_IA64_PERFMON_H */
+#endif /* _ASM_IA64_PERFMON_H_ */
Index: linux-2.6.31-master/arch/ia64/include/asm/perfmon_compat.h
===================================================================
--- /dev/null
+++ linux-2.6.31-master/arch/ia64/include/asm/perfmon_compat.h
@@ -0,0 +1,167 @@
+/*
+ * Copyright (c) 2001-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This header file contains perfmon interface definition
+ * that are now obsolete and should be dropped in favor
+ * of their equivalent functions as explained below.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+
+#ifndef _ASM_IA64_PERFMON_COMPAT_H_
+#define _ASM_IA64_PERFMON_COMPAT_H_
+
+/*
+ * custom sampling buffer identifier type
+ */
+typedef __u8 pfm_uuid_t[16];
+
+/*
+ * obsolete perfmon commands. Supported only on IA-64 for
+ * backward compatiblity reasons with perfmon v2.0.
+ */
+#define PFM_WRITE_PMCS		0x01 /* use pfm_write_pmcs */
+#define PFM_WRITE_PMDS		0x02 /* use pfm_write_pmds */
+#define PFM_READ_PMDS		0x03 /* use pfm_read_pmds */
+#define PFM_STOP		0x04 /* use pfm_stop */
+#define PFM_START		0x05 /* use pfm_start */
+#define PFM_ENABLE		0x06 /* obsolete */
+#define PFM_DISABLE		0x07 /* obsolete */
+#define PFM_CREATE_CONTEXT	0x08 /* use pfm_create_context */
+#define PFM_DESTROY_CONTEXT	0x09 /* use close() */
+#define PFM_RESTART		0x0a /* use pfm_restart */
+#define PFM_PROTECT_CONTEXT	0x0b /* obsolete */
+#define PFM_GET_FEATURES	0x0c /* use /proc/sys/perfmon */
+#define PFM_DEBUG		0x0d /* /proc/sys/kernel/perfmon/debug */
+#define PFM_UNPROTECT_CONTEXT	0x0e /* obsolete */
+#define PFM_GET_PMC_RESET_VAL	0x0f /* use /proc/perfmon_map */
+#define PFM_LOAD_CONTEXT	0x10 /* use pfm_load_context */
+#define PFM_UNLOAD_CONTEXT	0x11 /* use pfm_unload_context */
+
+/*
+ * PMU model specific commands (may not be supported on all PMU models)
+ */
+#define PFM_WRITE_IBRS		0x20 /* obsolete: use PFM_WRITE_PMCS[256-263]*/
+#define PFM_WRITE_DBRS		0x21 /* obsolete: use PFM_WRITE_PMCS[264-271]*/
+
+/*
+ * argument to PFM_CREATE_CONTEXT
+ */
+struct pfarg_context {
+	pfm_uuid_t     ctx_smpl_buf_id;	 /* buffer format to use */
+	unsigned long  ctx_flags;	 /* noblock/block */
+	unsigned int   ctx_reserved1;	 /* for future use */
+	int	       ctx_fd;		 /* return: fildesc */
+	void	       *ctx_smpl_vaddr;	 /* return: vaddr of buffer */
+	unsigned long  ctx_reserved3[11];/* for future use */
+};
+
+/*
+ * argument structure for PFM_WRITE_PMCS/PFM_WRITE_PMDS/PFM_WRITE_PMDS
+ */
+struct pfarg_reg {
+	unsigned int	reg_num;	   /* which register */
+	unsigned short	reg_set;	   /* event set for this register */
+	unsigned short	reg_reserved1;	   /* for future use */
+
+	unsigned long	reg_value;	   /* initial pmc/pmd value */
+	unsigned long	reg_flags;	   /* input: flags, ret: error */
+
+	unsigned long	reg_long_reset;	   /* reset value after notification */
+	unsigned long	reg_short_reset;   /* reset after counter overflow */
+
+	unsigned long	reg_reset_pmds[4]; /* registers to reset on overflow */
+	unsigned long	reg_random_seed;   /* seed for randomization */
+	unsigned long	reg_random_mask;   /* random range limit */
+	unsigned long   reg_last_reset_val;/* return: PMD last reset value */
+
+	unsigned long	reg_smpl_pmds[4];  /* pmds to be saved on overflow */
+	unsigned long	reg_smpl_eventid;  /* opaque sampling event id */
+	unsigned long   reg_ovfl_switch_cnt;/* #overflows to switch */
+
+	unsigned long   reg_reserved2[2];   /* for future use */
+};
+
+/*
+ * argument to PFM_WRITE_IBRS/PFM_WRITE_DBRS
+ */
+struct pfarg_dbreg {
+	unsigned int	dbreg_num;		/* which debug register */
+	unsigned short	dbreg_set;		/* event set */
+	unsigned short	dbreg_reserved1;	/* for future use */
+	unsigned long	dbreg_value;		/* value for debug register */
+	unsigned long	dbreg_flags;		/* return: dbreg error */
+	unsigned long	dbreg_reserved2[1];	/* for future use */
+};
+
+/*
+ * argument to PFM_GET_FEATURES
+ */
+struct pfarg_features {
+	unsigned int	ft_version;	/* major [16-31], minor [0-15] */
+	unsigned int	ft_reserved;	/* reserved for future use */
+	unsigned long	reserved[4];	/* for future use */
+};
+
+typedef struct {
+	int		msg_type;		/* generic message header */
+	int		msg_ctx_fd;		/* generic message header */
+	unsigned long	msg_ovfl_pmds[4];	/* which PMDs overflowed */
+	unsigned short  msg_active_set;		/* active set on overflow */
+	unsigned short  msg_reserved1;		/* for future use */
+	unsigned int    msg_reserved2;		/* for future use */
+	unsigned long	msg_tstamp;		/* for perf tuning/debug */
+} pfm_ovfl_msg_t;
+
+typedef struct {
+	int		msg_type;		/* generic message header */
+	int		msg_ctx_fd;		/* generic message header */
+	unsigned long	msg_tstamp;		/* for perf tuning */
+} pfm_end_msg_t;
+
+typedef struct {
+	int		msg_type;		/* type of the message */
+	int		msg_ctx_fd;		/* context file descriptor */
+	unsigned long	msg_tstamp;		/* for perf tuning */
+} pfm_gen_msg_t;
+
+typedef union {
+	int type;
+	pfm_ovfl_msg_t	pfm_ovfl_msg;
+	pfm_end_msg_t	pfm_end_msg;
+	pfm_gen_msg_t	pfm_gen_msg;
+} pfm_msg_t;
+
+/*
+ * PMD/PMC return flags in case of error (ignored on input)
+ *
+ * reg_flags layout:
+ * bit 00-15 : generic flags
+ * bits[16-23] : arch-specific flags (see asm/perfmon.h)
+ * bit 24-31 : error codes
+ *
+ * Those flags are used on output and must be checked in case EINVAL is
+ * returned by a command accepting a vector of values and each has a flag
+ * field, such as pfarg_reg or pfarg_reg
+ */
+#define PFM_REG_RETFL_NOTAVAIL	(1<<31) /* not implemented or unaccessible */
+#define PFM_REG_RETFL_EINVAL	(1<<30) /* entry is invalid */
+#define PFM_REG_RETFL_MASK	(PFM_REG_RETFL_NOTAVAIL|\
+				 PFM_REG_RETFL_EINVAL)
+
+#define PFM_REG_HAS_ERROR(flag)	(((flag) & PFM_REG_RETFL_MASK) != 0)
+
+#endif /* _ASM_IA64_PERFMON_COMPAT_H_ */
Index: linux-2.6.31-master/arch/ia64/include/asm/perfmon_default_smpl.h
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/include/asm/perfmon_default_smpl.h
+++ linux-2.6.31-master/arch/ia64/include/asm/perfmon_default_smpl.h
@@ -1,83 +1,106 @@
 /*
- * Copyright (C) 2002-2003 Hewlett-Packard Co
- *               Stephane Eranian <eranian@hpl.hp.com>
+ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
  *
- * This file implements the default sampling buffer format
- * for Linux/ia64 perfmon subsystem.
+ * This file implements the old default sampling buffer format
+ * for the perfmon2 subsystem. For IA-64 only.
+ *
+ * It requires the use of the perfmon_compat.h header. It is recommended
+ * that applications be ported to the new format instead.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
  */
-#ifndef __PERFMON_DEFAULT_SMPL_H__
-#define __PERFMON_DEFAULT_SMPL_H__ 1
+#ifndef __ASM_IA64_PERFMON_DEFAULT_SMPL_H__
+#define __ASM_IA64_PERFMON_DEFAULT_SMPL_H__ 1
+
+#ifndef __ia64__
+#error "this file must be used for compatibility reasons only on IA-64"
+#endif
 
 #define PFM_DEFAULT_SMPL_UUID { \
-		0x4d, 0x72, 0xbe, 0xc0, 0x06, 0x64, 0x41, 0x43, 0x82, 0xb4, 0xd3, 0xfd, 0x27, 0x24, 0x3c, 0x97}
+		0x4d, 0x72, 0xbe, 0xc0, 0x06, 0x64, 0x41, 0x43, 0x82,\
+		0xb4, 0xd3, 0xfd, 0x27, 0x24, 0x3c, 0x97}
 
 /*
  * format specific parameters (passed at context creation)
  */
-typedef struct {
+struct pfm_default_smpl_arg {
 	unsigned long buf_size;		/* size of the buffer in bytes */
 	unsigned int  flags;		/* buffer specific flags */
 	unsigned int  res1;		/* for future use */
 	unsigned long reserved[2];	/* for future use */
-} pfm_default_smpl_arg_t;
+};
 
 /*
  * combined context+format specific structure. Can be passed
- * to PFM_CONTEXT_CREATE
+ * to PFM_CONTEXT_CREATE (not PFM_CONTEXT_CREATE2)
  */
-typedef struct {
-	pfarg_context_t		ctx_arg;
-	pfm_default_smpl_arg_t	buf_arg;
-} pfm_default_smpl_ctx_arg_t;
+struct pfm_default_smpl_ctx_arg {
+	struct pfarg_context		ctx_arg;
+	struct pfm_default_smpl_arg	buf_arg;
+};
 
 /*
  * This header is at the beginning of the sampling buffer returned to the user.
  * It is directly followed by the first record.
  */
-typedef struct {
-	unsigned long	hdr_count;		/* how many valid entries */
-	unsigned long	hdr_cur_offs;		/* current offset from top of buffer */
-	unsigned long	hdr_reserved2;		/* reserved for future use */
-
-	unsigned long	hdr_overflows;		/* how many times the buffer overflowed */
-	unsigned long   hdr_buf_size;		/* how many bytes in the buffer */
-
-	unsigned int	hdr_version;		/* contains perfmon version (smpl format diffs) */
-	unsigned int	hdr_reserved1;		/* for future use */
-	unsigned long	hdr_reserved[10];	/* for future use */
-} pfm_default_smpl_hdr_t;
+struct pfm_default_smpl_hdr {
+	u64	hdr_count;	/* how many valid entries */
+	u64	hdr_cur_offs;	/* current offset from top of buffer */
+	u64	dr_reserved2;	/* reserved for future use */
+
+	u64	hdr_overflows;	/* how many times the buffer overflowed */
+	u64	hdr_buf_size;	/* how many bytes in the buffer */
+
+	u32	hdr_version;	/* smpl format version*/
+	u32	hdr_reserved1;		/* for future use */
+	u64	hdr_reserved[10];	/* for future use */
+};
 
 /*
  * Entry header in the sampling buffer.  The header is directly followed
- * with the values of the PMD registers of interest saved in increasing 
- * index order: PMD4, PMD5, and so on. How many PMDs are present depends 
+ * with the values of the PMD registers of interest saved in increasing
+ * index order: PMD4, PMD5, and so on. How many PMDs are present depends
  * on how the session was programmed.
  *
  * In the case where multiple counters overflow at the same time, multiple
  * entries are written consecutively.
  *
- * last_reset_value member indicates the initial value of the overflowed PMD. 
+ * last_reset_value member indicates the initial value of the overflowed PMD.
  */
-typedef struct {
-        int             pid;                    /* thread id (for NPTL, this is gettid()) */
-        unsigned char   reserved1[3];           /* reserved for future use */
-        unsigned char   ovfl_pmd;               /* index of overflowed PMD */
-
-        unsigned long   last_reset_val;         /* initial value of overflowed PMD */
-        unsigned long   ip;                     /* where did the overflow interrupt happened  */
-        unsigned long   tstamp;                 /* ar.itc when entering perfmon intr. handler */
-
-        unsigned short  cpu;                    /* cpu on which the overfow occured */
-        unsigned short  set;                    /* event set active when overflow ocurred   */
-        int    		tgid;              	/* thread group id (for NPTL, this is getpid()) */
-} pfm_default_smpl_entry_t;
-
-#define PFM_DEFAULT_MAX_PMDS		64 /* how many pmds supported by data structures (sizeof(unsigned long) */
-#define PFM_DEFAULT_MAX_ENTRY_SIZE	(sizeof(pfm_default_smpl_entry_t)+(sizeof(unsigned long)*PFM_DEFAULT_MAX_PMDS))
-#define PFM_DEFAULT_SMPL_MIN_BUF_SIZE	(sizeof(pfm_default_smpl_hdr_t)+PFM_DEFAULT_MAX_ENTRY_SIZE)
+struct pfm_default_smpl_entry {
+	pid_t	pid;		/* thread id (for NPTL, this is gettid()) */
+	uint8_t	reserved1[3];	/* for future use */
+	uint8_t	ovfl_pmd;	/* overflow pmd for this sample */
+	u64	last_reset_val;	/* initial value of overflowed PMD */
+	unsigned long ip;	/* where did the overflow interrupt happened */
+	u64	tstamp; 	/* overflow timetamp */
+	u16	cpu;  		/* cpu on which the overfow occured */
+	u16	set;  		/* event set active when overflow ocurred   */
+	pid_t	tgid;		/* thread group id (for NPTL, this is getpid()) */
+};
+
+#define PFM_DEFAULT_MAX_PMDS		64 /* #pmds supported  */
+#define PFM_DEFAULT_MAX_ENTRY_SIZE	(sizeof(struct pfm_default_smpl_entry)+\
+					 (sizeof(u64)*PFM_DEFAULT_MAX_PMDS))
+#define PFM_DEFAULT_SMPL_MIN_BUF_SIZE	(sizeof(struct pfm_default_smpl_hdr)+\
+					 PFM_DEFAULT_MAX_ENTRY_SIZE)
 
 #define PFM_DEFAULT_SMPL_VERSION_MAJ	2U
-#define PFM_DEFAULT_SMPL_VERSION_MIN	0U
-#define PFM_DEFAULT_SMPL_VERSION	(((PFM_DEFAULT_SMPL_VERSION_MAJ&0xffff)<<16)|(PFM_DEFAULT_SMPL_VERSION_MIN & 0xffff))
+#define PFM_DEFAULT_SMPL_VERSION_MIN 1U
+#define PFM_DEFAULT_SMPL_VERSION (((PFM_DEFAULT_SMPL_VERSION_MAJ&0xffff)<<16)|\
+				    (PFM_DEFAULT_SMPL_VERSION_MIN & 0xffff))
 
-#endif /* __PERFMON_DEFAULT_SMPL_H__ */
+#endif /* __ASM_IA64_PERFMON_DEFAULT_SMPL_H__ */
Index: linux-2.6.31-master/arch/ia64/include/asm/perfmon_kern.h
===================================================================
--- /dev/null
+++ linux-2.6.31-master/arch/ia64/include/asm/perfmon_kern.h
@@ -0,0 +1,354 @@
+/*
+ * Copyright (c) 2001-2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file contains Itanium Processor Family specific definitions
+ * for the perfmon interface.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#ifndef _ASM_IA64_PERFMON_KERN_H_
+#define _ASM_IA64_PERFMON_KERN_H_
+
+#ifdef __KERNEL__
+
+#ifdef CONFIG_PERFMON
+#include <asm/unistd.h>
+#include <asm/hw_irq.h>
+
+/*
+ * describe the content of the pfm_syst_info field
+ * layout:
+ * bits[00-15] : generic flags
+ * bits[16-31] : arch-specific flags
+ */
+#define PFM_ITA_CPUINFO_IDLE_EXCL 0x10000 /* stop monitoring in idle loop */
+
+/*
+ * For some CPUs, the upper bits of a counter must be set in order for the
+ * overflow interrupt to happen. On overflow, the counter has wrapped around,
+ * and the upper bits are cleared. This function may be used to set them back.
+ */
+static inline void pfm_arch_ovfl_reset_pmd(struct pfm_context *ctx,
+					   unsigned int cnum)
+{}
+
+/*
+ * called from __pfm_interrupt_handler(). ctx is not NULL.
+ * ctx is locked. PMU interrupt is masked.
+ *
+ * must stop all monitoring to ensure handler has consistent view.
+ * must collect overflowed PMDs bitmask  into povfls_pmds and
+ * npend_ovfls. If no interrupt detected then npend_ovfls
+ * must be set to zero.
+ */
+static inline void pfm_arch_intr_freeze_pmu(struct pfm_context *ctx,
+					    struct pfm_event_set *set)
+{
+	u64 tmp;
+
+	/*
+	 * do not overwrite existing value, must
+	 * process those first (coming from context switch replay)
+	 */
+	if (set->npend_ovfls)
+		return;
+
+	ia64_srlz_d();
+
+	tmp =  ia64_get_pmc(0) & ~0xf;
+
+	set->povfl_pmds[0] = tmp;
+
+	set->npend_ovfls = ia64_popcnt(tmp);
+}
+
+static inline int pfm_arch_init_pmu_config(void)
+{
+	return 0;
+}
+
+static inline void pfm_arch_resend_irq(struct pfm_context *ctx)
+{
+	ia64_resend_irq(IA64_PERFMON_VECTOR);
+}
+
+static inline void pfm_arch_clear_pmd_ovfl_cond(struct pfm_context *ctx,
+					        struct pfm_event_set *set)
+{}
+
+static inline void pfm_arch_serialize(void)
+{
+	ia64_srlz_d();
+}
+
+static inline void pfm_arch_intr_unfreeze_pmu(struct pfm_context *ctx)
+{
+	PFM_DBG_ovfl("state=%d", ctx->state);
+	ia64_set_pmc(0, 0);
+	/* no serialization */
+}
+
+static inline void pfm_arch_write_pmc(struct pfm_context *ctx,
+				      unsigned int cnum, u64 value)
+{
+	if (cnum < 256) {
+		ia64_set_pmc(pfm_pmu_conf->pmc_desc[cnum].hw_addr, value);
+	} else if (cnum < 264) {
+		ia64_set_ibr(cnum-256, value);
+		ia64_dv_serialize_instruction();
+	} else {
+		ia64_set_dbr(cnum-264, value);
+		ia64_dv_serialize_instruction();
+	}
+}
+
+/*
+ * On IA-64, for per-thread context which have the ITA_FL_INSECURE
+ * flag, it is possible to start/stop monitoring directly from user evel
+ * without calling pfm_start()/pfm_stop. This allows very lightweight
+ * control yet the kernel sometimes needs to know if monitoring is actually
+ * on or off.
+ *
+ * Tracking of this information is normally done by pfm_start/pfm_stop
+ * in flags.started. Here we need to compensate by checking actual
+ * psr bit.
+ */
+static inline int pfm_arch_is_active(struct pfm_context *ctx)
+{
+	return ctx->flags.started
+	       || ia64_getreg(_IA64_REG_PSR) & (IA64_PSR_UP|IA64_PSR_PP);
+}
+
+static inline void pfm_arch_write_pmd(struct pfm_context *ctx,
+				      unsigned int cnum, u64 value)
+{
+	/*
+	 * for a counting PMD, overflow bit must be cleared
+	 */
+	if (pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_C64)
+		value &= pfm_pmu_conf->ovfl_mask;
+
+	/*
+	 * for counters, write to upper bits are ignored, no need to mask
+	 */
+	ia64_set_pmd(pfm_pmu_conf->pmd_desc[cnum].hw_addr, value);
+}
+
+static inline u64 pfm_arch_read_pmd(struct pfm_context *ctx, unsigned int cnum)
+{
+	return ia64_get_pmd(pfm_pmu_conf->pmd_desc[cnum].hw_addr);
+}
+
+static inline u64 pfm_arch_read_pmc(struct pfm_context *ctx, unsigned int cnum)
+{
+	return ia64_get_pmc(pfm_pmu_conf->pmc_desc[cnum].hw_addr);
+}
+
+static inline void pfm_arch_ctxswout_sys(struct task_struct *task,
+					 struct pfm_context *ctx)
+{
+	struct pt_regs *regs;
+
+	regs = task_pt_regs(task);
+	ia64_psr(regs)->pp = 0;
+}
+
+static inline void pfm_arch_ctxswin_sys(struct task_struct *task,
+					struct pfm_context *ctx)
+{
+	struct pt_regs *regs;
+
+	if (!(ctx->active_set->flags & PFM_ITA_SETFL_INTR_ONLY)) {
+		regs = task_pt_regs(task);
+		ia64_psr(regs)->pp = 1;
+	}
+}
+
+/*
+ * On IA-64, the PMDs are NOT saved by pfm_arch_freeze_pmu()
+ * when entering the PMU interrupt handler, thus, we need
+ * to save them in pfm_switch_sets_from_intr()
+ */
+static inline void pfm_arch_save_pmds_from_intr(struct pfm_context *ctx,
+					   struct pfm_event_set *set)
+{
+	pfm_save_pmds(ctx, set);
+}
+
+int pfm_arch_context_create(struct pfm_context *ctx, u32 ctx_flags);
+
+static inline void pfm_arch_context_free(struct pfm_context *ctx)
+{}
+
+int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx);
+void pfm_arch_ctxswin_thread(struct task_struct *task,
+			     struct pfm_context *ctx);
+
+void pfm_arch_unload_context(struct pfm_context *ctx);
+int pfm_arch_load_context(struct pfm_context *ctx);
+int pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags);
+
+void pfm_arch_mask_monitoring(struct pfm_context *ctx,
+			      struct pfm_event_set *set);
+void pfm_arch_unmask_monitoring(struct pfm_context *ctx,
+				struct pfm_event_set *set);
+
+void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
+void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set);
+
+void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx);
+void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx);
+
+int  pfm_arch_init(void);
+void pfm_arch_init_percpu(void);
+char *pfm_arch_get_pmu_module_name(void);
+
+int __pfm_use_dbregs(struct task_struct *task);
+int  __pfm_release_dbregs(struct task_struct *task);
+int pfm_ia64_mark_dbregs_used(struct pfm_context *ctx,
+			      struct pfm_event_set *set);
+
+void pfm_arch_show_session(struct seq_file *m);
+
+static inline int pfm_arch_reserve_regs(u64 *unavail_pmcs, u64 *unavail_pmds)
+{
+	return 0;
+}
+
+static inline void pfm_arch_release_regs(void)
+{}
+
+static inline int pfm_arch_acquire_pmu(void)
+{
+	return 0;
+}
+
+static inline void pfm_arch_release_pmu(void)
+{}
+
+/* not necessary on IA-64 */
+static inline void pfm_cacheflush(void *addr, unsigned int len)
+{}
+
+/*
+ * miscellaneous architected definitions
+ */
+#define PFM_ITA_FCNTR	4 /* first counting monitor (PMC/PMD) */
+
+/*
+ * private event set flags  (set_priv_flags)
+ */
+#define PFM_ITA_SETFL_USE_DBR	0x1000000 /* set uses debug registers */
+
+
+/*
+ * Itanium-specific data structures
+ */
+struct pfm_ia64_context_flags {
+	unsigned int use_dbr:1;	 /* use range restrictions (debug registers) */
+	unsigned int insecure:1; /* insecure monitoring for non-self session */
+	unsigned int reserved:30;/* for future use */
+};
+
+struct pfm_arch_context {
+	struct pfm_ia64_context_flags flags;	/* arch specific ctx flags */
+	u64			 ctx_saved_psr_up;/* storage for psr_up */
+#ifdef CONFIG_IA64_PERFMON_COMPAT
+	void			*ctx_smpl_vaddr; /* vaddr of user mapping */
+#endif
+};
+
+#ifdef CONFIG_IA64_PERFMON_COMPAT
+ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
+			     char __user *buf,
+			     int non_block,
+			     size_t size);
+int pfm_ia64_compat_init(void);
+int pfm_smpl_buf_alloc_compat(struct pfm_context *ctx, size_t rsize, int fd);
+#else
+static inline ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
+			     char __user *buf,
+			     int non_block,
+			     size_t size)
+{
+	return -EINVAL;
+}
+
+static inline int pfm_smpl_buf_alloc_compat(struct pfm_context *ctx,
+					    size_t rsize, struct file *filp)
+{
+	return -EINVAL;
+}
+#endif
+
+static inline void pfm_arch_arm_handle_work(struct task_struct *task)
+{
+	set_tsk_thread_flag(task, TIF_NOTIFY_RESUME);
+}
+
+static inline void pfm_arch_disarm_handle_work(struct task_struct *task)
+{
+	/*
+	 * since 2.6.28, we do not need this function anymore because
+	 * TIF_NOTIFY_RESUME, it automatically cleared by do_notify_resume_user()
+	 * so worst case we have a spurious call to this function
+	 */
+}
+
+static inline int pfm_arch_pmu_config_init(struct pfm_pmu_config *cfg)
+{
+	return 0;
+}
+
+extern struct pfm_ia64_pmu_info *pfm_ia64_pmu_info;
+
+#define PFM_ARCH_CTX_SIZE	(sizeof(struct pfm_arch_context))
+
+/*
+ * IA-64 does not need extra alignment requirements for the sampling buffer
+ */
+#define PFM_ARCH_SMPL_ALIGN_SIZE	0
+
+
+static inline void pfm_release_dbregs(struct task_struct *task)
+{
+	if (task->thread.flags & IA64_THREAD_DBG_VALID)
+		__pfm_release_dbregs(task);
+}
+
+#define pfm_use_dbregs(_t)     __pfm_use_dbregs(_t)
+
+static inline int pfm_arch_get_base_syscall(void)
+{
+	return __NR_pfm_create_context;
+}
+
+struct pfm_arch_pmu_info {
+	unsigned long mask_pmcs[PFM_PMC_BV]; /* modify on when masking */
+};
+
+DECLARE_PER_CPU(u32, pfm_syst_info);
+#else /* !CONFIG_PERFMON */
+/*
+ * perfmon ia64-specific hooks
+ */
+#define pfm_release_dbregs(_t) 		do { } while (0)
+#define pfm_use_dbregs(_t)     		(0)
+
+#endif /* CONFIG_PERFMON */
+
+#endif /* __KERNEL__ */
+#endif /* _ASM_IA64_PERFMON_KERN_H_ */
Index: linux-2.6.31-master/arch/ia64/include/asm/processor.h
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/include/asm/processor.h
+++ linux-2.6.31-master/arch/ia64/include/asm/processor.h
@@ -42,7 +42,6 @@
 
 #define IA64_THREAD_FPH_VALID	(__IA64_UL(1) << 0)	/* floating-point high state valid? */
 #define IA64_THREAD_DBG_VALID	(__IA64_UL(1) << 1)	/* debug registers valid? */
-#define IA64_THREAD_PM_VALID	(__IA64_UL(1) << 2)	/* performance registers valid? */
 #define IA64_THREAD_UAC_NOPRINT	(__IA64_UL(1) << 3)	/* don't log unaligned accesses */
 #define IA64_THREAD_UAC_SIGBUS	(__IA64_UL(1) << 4)	/* generate SIGBUS on unaligned acc. */
 #define IA64_THREAD_MIGRATION	(__IA64_UL(1) << 5)	/* require migration
@@ -321,14 +320,6 @@ struct thread_struct {
 #else
 # define INIT_THREAD_IA32
 #endif /* CONFIG_IA32_SUPPORT */
-#ifdef CONFIG_PERFMON
-	void *pfm_context;		     /* pointer to detailed PMU context */
-	unsigned long pfm_needs_checking;    /* when >0, pending perfmon work on kernel exit */
-# define INIT_THREAD_PM		.pfm_context =		NULL,     \
-				.pfm_needs_checking =	0UL,
-#else
-# define INIT_THREAD_PM
-#endif
 	unsigned long dbr[IA64_NUM_DBG_REGS];
 	unsigned long ibr[IA64_NUM_DBG_REGS];
 	struct ia64_fpreg fph[96];	/* saved/loaded on demand */
@@ -343,7 +334,6 @@ struct thread_struct {
 	.task_size =	DEFAULT_TASK_SIZE,			\
 	.last_fph_cpu =  -1,					\
 	INIT_THREAD_IA32					\
-	INIT_THREAD_PM						\
 	.dbr =		{0, },					\
 	.ibr =		{0, },					\
 	.fph =		{{{{0}}}, }				\
Index: linux-2.6.31-master/arch/ia64/include/asm/system.h
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/include/asm/system.h
+++ linux-2.6.31-master/arch/ia64/include/asm/system.h
@@ -217,6 +217,7 @@ struct task_struct;
 extern void ia64_save_extra (struct task_struct *task);
 extern void ia64_load_extra (struct task_struct *task);
 
+
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING
 extern void ia64_account_on_switch (struct task_struct *prev, struct task_struct *next);
 # define IA64_ACCOUNT_ON_SWITCH(p,n) ia64_account_on_switch(p,n)
@@ -224,16 +225,9 @@ extern void ia64_account_on_switch (stru
 # define IA64_ACCOUNT_ON_SWITCH(p,n)
 #endif
 
-#ifdef CONFIG_PERFMON
-  DECLARE_PER_CPU(unsigned long, pfm_syst_info);
-# define PERFMON_IS_SYSWIDE() (__get_cpu_var(pfm_syst_info) & 0x1)
-#else
-# define PERFMON_IS_SYSWIDE() (0)
-#endif
-
-#define IA64_HAS_EXTRA_STATE(t)							\
-	((t)->thread.flags & (IA64_THREAD_DBG_VALID|IA64_THREAD_PM_VALID)	\
-	 || IS_IA32_PROCESS(task_pt_regs(t)) || PERFMON_IS_SYSWIDE())
+#define IA64_HAS_EXTRA_STATE(t)						\
+	(((t)->thread.flags & IA64_THREAD_DBG_VALID)			\
+	 || IS_IA32_PROCESS(task_pt_regs(t)))
 
 #define __switch_to(prev,next,last) do {							 \
 	IA64_ACCOUNT_ON_SWITCH(prev, next);							 \
@@ -241,6 +235,10 @@ extern void ia64_account_on_switch (stru
 		ia64_save_extra(prev);								 \
 	if (IA64_HAS_EXTRA_STATE(next))								 \
 		ia64_load_extra(next);								 \
+	if (test_tsk_thread_flag(prev, TIF_PERFMON_CTXSW))					 \
+		pfm_ctxsw_out(prev, next);						 	 \
+	if (test_tsk_thread_flag(next, TIF_PERFMON_CTXSW))					 \
+		pfm_ctxsw_in(prev, next);						 	 \
 	ia64_psr(task_pt_regs(next))->dfh = !ia64_is_local_fpu_owner(next);			 \
 	(last) = ia64_switch_to((next));							 \
 } while (0)
Index: linux-2.6.31-master/arch/ia64/include/asm/thread_info.h
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/include/asm/thread_info.h
+++ linux-2.6.31-master/arch/ia64/include/asm/thread_info.h
@@ -107,6 +107,8 @@ struct thread_info {
 #define TIF_DB_DISABLED		19	/* debug trap disabled for fsyscall */
 #define TIF_FREEZE		20	/* is freezing for suspend */
 #define TIF_RESTORE_RSE		21	/* user RBS is newer than kernel RBS */
+#define TIF_PERFMON_CTXSW	22	/* perfmon needs ctxsw calls */
+#define TIF_PERFMON_WORK	23	/* work for pfm_handle_work() */
 
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
@@ -120,6 +122,8 @@ struct thread_info {
 #define _TIF_DB_DISABLED	(1 << TIF_DB_DISABLED)
 #define _TIF_FREEZE		(1 << TIF_FREEZE)
 #define _TIF_RESTORE_RSE	(1 << TIF_RESTORE_RSE)
+#define _TIF_PERFMON_CTXSW	(1 << TIF_PERFMON_CTXSW)
+#define _TIF_PERFMON_WORK	(1 << TIF_PERFMON_WORK)
 
 /* "work to do on user-return" bits */
 #define TIF_ALLWORK_MASK	(_TIF_SIGPENDING|_TIF_NOTIFY_RESUME|_TIF_SYSCALL_AUDIT|\
Index: linux-2.6.31-master/arch/ia64/include/asm/unistd.h
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/include/asm/unistd.h
+++ linux-2.6.31-master/arch/ia64/include/asm/unistd.h
@@ -311,11 +311,23 @@
 #define __NR_preadv			1319
 #define __NR_pwritev			1320
 #define __NR_rt_tgsigqueueinfo		1321
+#define __NR_pfm_create_context		1319
+#define __NR_pfm_write_pmcs		(__NR_pfm_create_context+1)
+#define __NR_pfm_write_pmds		(__NR_pfm_create_context+2)
+#define __NR_pfm_read_pmds		(__NR_pfm_create_context+3)
+#define __NR_pfm_load_context		(__NR_pfm_create_context+4)
+#define __NR_pfm_start			(__NR_pfm_create_context+5)
+#define __NR_pfm_stop			(__NR_pfm_create_context+6)
+#define __NR_pfm_restart		(__NR_pfm_create_context+7)
+#define __NR_pfm_create_evtsets		(__NR_pfm_create_context+8)
+#define __NR_pfm_getinfo_evtsets 	(__NR_pfm_create_context+9)
+#define __NR_pfm_delete_evtsets 	(__NR_pfm_create_context+10)
+#define __NR_pfm_unload_context		(__NR_pfm_create_context+11)
 
 #ifdef __KERNEL__
 
 
-#define NR_syscalls			298 /* length of syscall table */
+#define NR_syscalls			310 /* length of syscall table */
 
 /*
  * The following defines stop scripts/checksyscalls.sh from complaining about
Index: linux-2.6.31-master/arch/ia64/kernel/Makefile
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/Makefile
+++ linux-2.6.31-master/arch/ia64/kernel/Makefile
@@ -9,7 +9,7 @@ endif
 extra-y	:= head.o init_task.o vmlinux.lds
 
 obj-y := acpi.o entry.o efi.o efi_stub.o gate-data.o fsys.o ia64_ksyms.o irq.o irq_ia64.o	\
-	 irq_lsapic.o ivt.o machvec.o pal.o paravirt_patchlist.o patch.o process.o perfmon.o ptrace.o sal.o		\
+	 irq_lsapic.o ivt.o machvec.o pal.o paravirt_patchlist.o patch.o process.o ptrace.o sal.o		\
 	 salinfo.o setup.o signal.o sys_ia64.o time.o traps.o unaligned.o \
 	 unwind.o mca.o mca_asm.o topology.o dma-mapping.o
 
@@ -27,7 +27,6 @@ obj-$(CONFIG_IOSAPIC)		+= iosapic.o
 obj-$(CONFIG_MODULES)		+= module.o
 obj-$(CONFIG_SMP)		+= smp.o smpboot.o
 obj-$(CONFIG_NUMA)		+= numa.o
-obj-$(CONFIG_PERFMON)		+= perfmon_default_smpl.o
 obj-$(CONFIG_IA64_CYCLONE)	+= cyclone.o
 obj-$(CONFIG_CPU_FREQ)		+= cpufreq/
 obj-$(CONFIG_IA64_MCA_RECOVERY)	+= mca_recovery.o
Index: linux-2.6.31-master/arch/ia64/kernel/entry.S
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/entry.S
+++ linux-2.6.31-master/arch/ia64/kernel/entry.S
@@ -1806,6 +1806,18 @@ sys_call_table:
 	data8 sys_preadv
 	data8 sys_pwritev			// 1320
 	data8 sys_rt_tgsigqueueinfo
+	data8 sys_pfm_create_context
+	data8 sys_pfm_write_pmcs
+	data8 sys_pfm_write_pmds
+	data8 sys_pfm_read_pmds			// 1325
+	data8 sys_pfm_load_context
+	data8 sys_pfm_start
+	data8 sys_pfm_stop
+	data8 sys_pfm_restart
+	data8 sys_pfm_create_evtsets		// 1330
+	data8 sys_pfm_getinfo_evtsets
+	data8 sys_pfm_delete_evtsets
+	data8 sys_pfm_unload_context
 
 	.org sys_call_table + 8*NR_syscalls	// guard against failures to increase NR_syscalls
 #endif /* __IA64_ASM_PARAVIRTUALIZED_NATIVE */
Index: linux-2.6.31-master/arch/ia64/kernel/irq_ia64.c
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/irq_ia64.c
+++ linux-2.6.31-master/arch/ia64/kernel/irq_ia64.c
@@ -40,10 +40,6 @@
 #include <asm/system.h>
 #include <asm/tlbflush.h>
 
-#ifdef CONFIG_PERFMON
-# include <asm/perfmon.h>
-#endif
-
 #define IRQ_DEBUG	0
 
 #define IRQ_VECTOR_UNASSIGNED	(0)
@@ -666,9 +662,6 @@ init_IRQ (void)
 	}
 #endif
 #endif
-#ifdef CONFIG_PERFMON
-	pfm_init_percpu();
-#endif
 	platform_irq_init();
 }
 
Index: linux-2.6.31-master/arch/ia64/kernel/perfmon.c
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/perfmon.c
+++ /dev/null
@@ -1,6840 +0,0 @@
-/*
- * This file implements the perfmon-2 subsystem which is used
- * to program the IA-64 Performance Monitoring Unit (PMU).
- *
- * The initial version of perfmon.c was written by
- * Ganesh Venkitachalam, IBM Corp.
- *
- * Then it was modified for perfmon-1.x by Stephane Eranian and
- * David Mosberger, Hewlett Packard Co.
- *
- * Version Perfmon-2.x is a rewrite of perfmon-1.x
- * by Stephane Eranian, Hewlett Packard Co.
- *
- * Copyright (C) 1999-2005  Hewlett Packard Co
- *               Stephane Eranian <eranian@hpl.hp.com>
- *               David Mosberger-Tang <davidm@hpl.hp.com>
- *
- * More information about perfmon available at:
- * 	http://www.hpl.hp.com/research/linux/perfmon
- */
-
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <linux/sched.h>
-#include <linux/interrupt.h>
-#include <linux/proc_fs.h>
-#include <linux/seq_file.h>
-#include <linux/init.h>
-#include <linux/vmalloc.h>
-#include <linux/mm.h>
-#include <linux/sysctl.h>
-#include <linux/list.h>
-#include <linux/file.h>
-#include <linux/poll.h>
-#include <linux/vfs.h>
-#include <linux/smp.h>
-#include <linux/pagemap.h>
-#include <linux/mount.h>
-#include <linux/bitops.h>
-#include <linux/capability.h>
-#include <linux/rcupdate.h>
-#include <linux/completion.h>
-#include <linux/tracehook.h>
-
-#include <asm/errno.h>
-#include <asm/intrinsics.h>
-#include <asm/page.h>
-#include <asm/perfmon.h>
-#include <asm/processor.h>
-#include <asm/signal.h>
-#include <asm/system.h>
-#include <asm/uaccess.h>
-#include <asm/delay.h>
-
-#ifdef CONFIG_PERFMON
-/*
- * perfmon context state
- */
-#define PFM_CTX_UNLOADED	1	/* context is not loaded onto any task */
-#define PFM_CTX_LOADED		2	/* context is loaded onto a task */
-#define PFM_CTX_MASKED		3	/* context is loaded but monitoring is masked due to overflow */
-#define PFM_CTX_ZOMBIE		4	/* owner of the context is closing it */
-
-#define PFM_INVALID_ACTIVATION	(~0UL)
-
-#define PFM_NUM_PMC_REGS	64	/* PMC save area for ctxsw */
-#define PFM_NUM_PMD_REGS	64	/* PMD save area for ctxsw */
-
-/*
- * depth of message queue
- */
-#define PFM_MAX_MSGS		32
-#define PFM_CTXQ_EMPTY(g)	((g)->ctx_msgq_head == (g)->ctx_msgq_tail)
-
-/*
- * type of a PMU register (bitmask).
- * bitmask structure:
- * 	bit0   : register implemented
- * 	bit1   : end marker
- * 	bit2-3 : reserved
- * 	bit4   : pmc has pmc.pm
- * 	bit5   : pmc controls a counter (has pmc.oi), pmd is used as counter
- * 	bit6-7 : register type
- * 	bit8-31: reserved
- */
-#define PFM_REG_NOTIMPL		0x0 /* not implemented at all */
-#define PFM_REG_IMPL		0x1 /* register implemented */
-#define PFM_REG_END		0x2 /* end marker */
-#define PFM_REG_MONITOR		(0x1<<4|PFM_REG_IMPL) /* a PMC with a pmc.pm field only */
-#define PFM_REG_COUNTING	(0x2<<4|PFM_REG_MONITOR) /* a monitor + pmc.oi+ PMD used as a counter */
-#define PFM_REG_CONTROL		(0x4<<4|PFM_REG_IMPL) /* PMU control register */
-#define	PFM_REG_CONFIG		(0x8<<4|PFM_REG_IMPL) /* configuration register */
-#define PFM_REG_BUFFER	 	(0xc<<4|PFM_REG_IMPL) /* PMD used as buffer */
-
-#define PMC_IS_LAST(i)	(pmu_conf->pmc_desc[i].type & PFM_REG_END)
-#define PMD_IS_LAST(i)	(pmu_conf->pmd_desc[i].type & PFM_REG_END)
-
-#define PMC_OVFL_NOTIFY(ctx, i)	((ctx)->ctx_pmds[i].flags &  PFM_REGFL_OVFL_NOTIFY)
-
-/* i assumed unsigned */
-#define PMC_IS_IMPL(i)	  (i< PMU_MAX_PMCS && (pmu_conf->pmc_desc[i].type & PFM_REG_IMPL))
-#define PMD_IS_IMPL(i)	  (i< PMU_MAX_PMDS && (pmu_conf->pmd_desc[i].type & PFM_REG_IMPL))
-
-/* XXX: these assume that register i is implemented */
-#define PMD_IS_COUNTING(i) ((pmu_conf->pmd_desc[i].type & PFM_REG_COUNTING) == PFM_REG_COUNTING)
-#define PMC_IS_COUNTING(i) ((pmu_conf->pmc_desc[i].type & PFM_REG_COUNTING) == PFM_REG_COUNTING)
-#define PMC_IS_MONITOR(i)  ((pmu_conf->pmc_desc[i].type & PFM_REG_MONITOR)  == PFM_REG_MONITOR)
-#define PMC_IS_CONTROL(i)  ((pmu_conf->pmc_desc[i].type & PFM_REG_CONTROL)  == PFM_REG_CONTROL)
-
-#define PMC_DFL_VAL(i)     pmu_conf->pmc_desc[i].default_value
-#define PMC_RSVD_MASK(i)   pmu_conf->pmc_desc[i].reserved_mask
-#define PMD_PMD_DEP(i)	   pmu_conf->pmd_desc[i].dep_pmd[0]
-#define PMC_PMD_DEP(i)	   pmu_conf->pmc_desc[i].dep_pmd[0]
-
-#define PFM_NUM_IBRS	  IA64_NUM_DBG_REGS
-#define PFM_NUM_DBRS	  IA64_NUM_DBG_REGS
-
-#define CTX_OVFL_NOBLOCK(c)	((c)->ctx_fl_block == 0)
-#define CTX_HAS_SMPL(c)		((c)->ctx_fl_is_sampling)
-#define PFM_CTX_TASK(h)		(h)->ctx_task
-
-#define PMU_PMC_OI		5 /* position of pmc.oi bit */
-
-/* XXX: does not support more than 64 PMDs */
-#define CTX_USED_PMD(ctx, mask) (ctx)->ctx_used_pmds[0] |= (mask)
-#define CTX_IS_USED_PMD(ctx, c) (((ctx)->ctx_used_pmds[0] & (1UL << (c))) != 0UL)
-
-#define CTX_USED_MONITOR(ctx, mask) (ctx)->ctx_used_monitors[0] |= (mask)
-
-#define CTX_USED_IBR(ctx,n) 	(ctx)->ctx_used_ibrs[(n)>>6] |= 1UL<< ((n) % 64)
-#define CTX_USED_DBR(ctx,n) 	(ctx)->ctx_used_dbrs[(n)>>6] |= 1UL<< ((n) % 64)
-#define CTX_USES_DBREGS(ctx)	(((pfm_context_t *)(ctx))->ctx_fl_using_dbreg==1)
-#define PFM_CODE_RR	0	/* requesting code range restriction */
-#define PFM_DATA_RR	1	/* requestion data range restriction */
-
-#define PFM_CPUINFO_CLEAR(v)	pfm_get_cpu_var(pfm_syst_info) &= ~(v)
-#define PFM_CPUINFO_SET(v)	pfm_get_cpu_var(pfm_syst_info) |= (v)
-#define PFM_CPUINFO_GET()	pfm_get_cpu_var(pfm_syst_info)
-
-#define RDEP(x)	(1UL<<(x))
-
-/*
- * context protection macros
- * in SMP:
- * 	- we need to protect against CPU concurrency (spin_lock)
- * 	- we need to protect against PMU overflow interrupts (local_irq_disable)
- * in UP:
- * 	- we need to protect against PMU overflow interrupts (local_irq_disable)
- *
- * spin_lock_irqsave()/spin_unlock_irqrestore():
- * 	in SMP: local_irq_disable + spin_lock
- * 	in UP : local_irq_disable
- *
- * spin_lock()/spin_lock():
- * 	in UP : removed automatically
- * 	in SMP: protect against context accesses from other CPU. interrupts
- * 	        are not masked. This is useful for the PMU interrupt handler
- * 	        because we know we will not get PMU concurrency in that code.
- */
-#define PROTECT_CTX(c, f) \
-	do {  \
-		DPRINT(("spinlock_irq_save ctx %p by [%d]\n", c, task_pid_nr(current))); \
-		spin_lock_irqsave(&(c)->ctx_lock, f); \
-		DPRINT(("spinlocked ctx %p  by [%d]\n", c, task_pid_nr(current))); \
-	} while(0)
-
-#define UNPROTECT_CTX(c, f) \
-	do { \
-		DPRINT(("spinlock_irq_restore ctx %p by [%d]\n", c, task_pid_nr(current))); \
-		spin_unlock_irqrestore(&(c)->ctx_lock, f); \
-	} while(0)
-
-#define PROTECT_CTX_NOPRINT(c, f) \
-	do {  \
-		spin_lock_irqsave(&(c)->ctx_lock, f); \
-	} while(0)
-
-
-#define UNPROTECT_CTX_NOPRINT(c, f) \
-	do { \
-		spin_unlock_irqrestore(&(c)->ctx_lock, f); \
-	} while(0)
-
-
-#define PROTECT_CTX_NOIRQ(c) \
-	do {  \
-		spin_lock(&(c)->ctx_lock); \
-	} while(0)
-
-#define UNPROTECT_CTX_NOIRQ(c) \
-	do { \
-		spin_unlock(&(c)->ctx_lock); \
-	} while(0)
-
-
-#ifdef CONFIG_SMP
-
-#define GET_ACTIVATION()	pfm_get_cpu_var(pmu_activation_number)
-#define INC_ACTIVATION()	pfm_get_cpu_var(pmu_activation_number)++
-#define SET_ACTIVATION(c)	(c)->ctx_last_activation = GET_ACTIVATION()
-
-#else /* !CONFIG_SMP */
-#define SET_ACTIVATION(t) 	do {} while(0)
-#define GET_ACTIVATION(t) 	do {} while(0)
-#define INC_ACTIVATION(t) 	do {} while(0)
-#endif /* CONFIG_SMP */
-
-#define SET_PMU_OWNER(t, c)	do { pfm_get_cpu_var(pmu_owner) = (t); pfm_get_cpu_var(pmu_ctx) = (c); } while(0)
-#define GET_PMU_OWNER()		pfm_get_cpu_var(pmu_owner)
-#define GET_PMU_CTX()		pfm_get_cpu_var(pmu_ctx)
-
-#define LOCK_PFS(g)	    	spin_lock_irqsave(&pfm_sessions.pfs_lock, g)
-#define UNLOCK_PFS(g)	    	spin_unlock_irqrestore(&pfm_sessions.pfs_lock, g)
-
-#define PFM_REG_RETFLAG_SET(flags, val)	do { flags &= ~PFM_REG_RETFL_MASK; flags |= (val); } while(0)
-
-/*
- * cmp0 must be the value of pmc0
- */
-#define PMC0_HAS_OVFL(cmp0)  (cmp0 & ~0x1UL)
-
-#define PFMFS_MAGIC 0xa0b4d889
-
-/*
- * debugging
- */
-#define PFM_DEBUGGING 1
-#ifdef PFM_DEBUGGING
-#define DPRINT(a) \
-	do { \
-		if (unlikely(pfm_sysctl.debug >0)) { printk("%s.%d: CPU%d [%d] ", __func__, __LINE__, smp_processor_id(), task_pid_nr(current)); printk a; } \
-	} while (0)
-
-#define DPRINT_ovfl(a) \
-	do { \
-		if (unlikely(pfm_sysctl.debug > 0 && pfm_sysctl.debug_ovfl >0)) { printk("%s.%d: CPU%d [%d] ", __func__, __LINE__, smp_processor_id(), task_pid_nr(current)); printk a; } \
-	} while (0)
-#endif
-
-/*
- * 64-bit software counter structure
- *
- * the next_reset_type is applied to the next call to pfm_reset_regs()
- */
-typedef struct {
-	unsigned long	val;		/* virtual 64bit counter value */
-	unsigned long	lval;		/* last reset value */
-	unsigned long	long_reset;	/* reset value on sampling overflow */
-	unsigned long	short_reset;    /* reset value on overflow */
-	unsigned long	reset_pmds[4];  /* which other pmds to reset when this counter overflows */
-	unsigned long	smpl_pmds[4];   /* which pmds are accessed when counter overflow */
-	unsigned long	seed;		/* seed for random-number generator */
-	unsigned long	mask;		/* mask for random-number generator */
-	unsigned int 	flags;		/* notify/do not notify */
-	unsigned long	eventid;	/* overflow event identifier */
-} pfm_counter_t;
-
-/*
- * context flags
- */
-typedef struct {
-	unsigned int block:1;		/* when 1, task will blocked on user notifications */
-	unsigned int system:1;		/* do system wide monitoring */
-	unsigned int using_dbreg:1;	/* using range restrictions (debug registers) */
-	unsigned int is_sampling:1;	/* true if using a custom format */
-	unsigned int excl_idle:1;	/* exclude idle task in system wide session */
-	unsigned int going_zombie:1;	/* context is zombie (MASKED+blocking) */
-	unsigned int trap_reason:2;	/* reason for going into pfm_handle_work() */
-	unsigned int no_msg:1;		/* no message sent on overflow */
-	unsigned int can_restart:1;	/* allowed to issue a PFM_RESTART */
-	unsigned int reserved:22;
-} pfm_context_flags_t;
-
-#define PFM_TRAP_REASON_NONE		0x0	/* default value */
-#define PFM_TRAP_REASON_BLOCK		0x1	/* we need to block on overflow */
-#define PFM_TRAP_REASON_RESET		0x2	/* we need to reset PMDs */
-
-
-/*
- * perfmon context: encapsulates all the state of a monitoring session
- */
-
-typedef struct pfm_context {
-	spinlock_t		ctx_lock;		/* context protection */
-
-	pfm_context_flags_t	ctx_flags;		/* bitmask of flags  (block reason incl.) */
-	unsigned int		ctx_state;		/* state: active/inactive (no bitfield) */
-
-	struct task_struct 	*ctx_task;		/* task to which context is attached */
-
-	unsigned long		ctx_ovfl_regs[4];	/* which registers overflowed (notification) */
-
-	struct completion	ctx_restart_done;  	/* use for blocking notification mode */
-
-	unsigned long		ctx_used_pmds[4];	/* bitmask of PMD used            */
-	unsigned long		ctx_all_pmds[4];	/* bitmask of all accessible PMDs */
-	unsigned long		ctx_reload_pmds[4];	/* bitmask of force reload PMD on ctxsw in */
-
-	unsigned long		ctx_all_pmcs[4];	/* bitmask of all accessible PMCs */
-	unsigned long		ctx_reload_pmcs[4];	/* bitmask of force reload PMC on ctxsw in */
-	unsigned long		ctx_used_monitors[4];	/* bitmask of monitor PMC being used */
-
-	unsigned long		ctx_pmcs[PFM_NUM_PMC_REGS];	/*  saved copies of PMC values */
-
-	unsigned int		ctx_used_ibrs[1];		/* bitmask of used IBR (speedup ctxsw in) */
-	unsigned int		ctx_used_dbrs[1];		/* bitmask of used DBR (speedup ctxsw in) */
-	unsigned long		ctx_dbrs[IA64_NUM_DBG_REGS];	/* DBR values (cache) when not loaded */
-	unsigned long		ctx_ibrs[IA64_NUM_DBG_REGS];	/* IBR values (cache) when not loaded */
-
-	pfm_counter_t		ctx_pmds[PFM_NUM_PMD_REGS]; /* software state for PMDS */
-
-	unsigned long		th_pmcs[PFM_NUM_PMC_REGS];	/* PMC thread save state */
-	unsigned long		th_pmds[PFM_NUM_PMD_REGS];	/* PMD thread save state */
-
-	unsigned long		ctx_saved_psr_up;	/* only contains psr.up value */
-
-	unsigned long		ctx_last_activation;	/* context last activation number for last_cpu */
-	unsigned int		ctx_last_cpu;		/* CPU id of current or last CPU used (SMP only) */
-	unsigned int		ctx_cpu;		/* cpu to which perfmon is applied (system wide) */
-
-	int			ctx_fd;			/* file descriptor used my this context */
-	pfm_ovfl_arg_t		ctx_ovfl_arg;		/* argument to custom buffer format handler */
-
-	pfm_buffer_fmt_t	*ctx_buf_fmt;		/* buffer format callbacks */
-	void			*ctx_smpl_hdr;		/* points to sampling buffer header kernel vaddr */
-	unsigned long		ctx_smpl_size;		/* size of sampling buffer */
-	void			*ctx_smpl_vaddr;	/* user level virtual address of smpl buffer */
-
-	wait_queue_head_t 	ctx_msgq_wait;
-	pfm_msg_t		ctx_msgq[PFM_MAX_MSGS];
-	int			ctx_msgq_head;
-	int			ctx_msgq_tail;
-	struct fasync_struct	*ctx_async_queue;
-
-	wait_queue_head_t 	ctx_zombieq;		/* termination cleanup wait queue */
-} pfm_context_t;
-
-/*
- * magic number used to verify that structure is really
- * a perfmon context
- */
-#define PFM_IS_FILE(f)		((f)->f_op == &pfm_file_ops)
-
-#define PFM_GET_CTX(t)	 	((pfm_context_t *)(t)->thread.pfm_context)
-
-#ifdef CONFIG_SMP
-#define SET_LAST_CPU(ctx, v)	(ctx)->ctx_last_cpu = (v)
-#define GET_LAST_CPU(ctx)	(ctx)->ctx_last_cpu
-#else
-#define SET_LAST_CPU(ctx, v)	do {} while(0)
-#define GET_LAST_CPU(ctx)	do {} while(0)
-#endif
-
-
-#define ctx_fl_block		ctx_flags.block
-#define ctx_fl_system		ctx_flags.system
-#define ctx_fl_using_dbreg	ctx_flags.using_dbreg
-#define ctx_fl_is_sampling	ctx_flags.is_sampling
-#define ctx_fl_excl_idle	ctx_flags.excl_idle
-#define ctx_fl_going_zombie	ctx_flags.going_zombie
-#define ctx_fl_trap_reason	ctx_flags.trap_reason
-#define ctx_fl_no_msg		ctx_flags.no_msg
-#define ctx_fl_can_restart	ctx_flags.can_restart
-
-#define PFM_SET_WORK_PENDING(t, v)	do { (t)->thread.pfm_needs_checking = v; } while(0);
-#define PFM_GET_WORK_PENDING(t)		(t)->thread.pfm_needs_checking
-
-/*
- * global information about all sessions
- * mostly used to synchronize between system wide and per-process
- */
-typedef struct {
-	spinlock_t		pfs_lock;		   /* lock the structure */
-
-	unsigned int		pfs_task_sessions;	   /* number of per task sessions */
-	unsigned int		pfs_sys_sessions;	   /* number of per system wide sessions */
-	unsigned int		pfs_sys_use_dbregs;	   /* incremented when a system wide session uses debug regs */
-	unsigned int		pfs_ptrace_use_dbregs;	   /* incremented when a process uses debug regs */
-	struct task_struct	*pfs_sys_session[NR_CPUS]; /* point to task owning a system-wide session */
-} pfm_session_t;
-
-/*
- * information about a PMC or PMD.
- * dep_pmd[]: a bitmask of dependent PMD registers
- * dep_pmc[]: a bitmask of dependent PMC registers
- */
-typedef int (*pfm_reg_check_t)(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs);
-typedef struct {
-	unsigned int		type;
-	int			pm_pos;
-	unsigned long		default_value;	/* power-on default value */
-	unsigned long		reserved_mask;	/* bitmask of reserved bits */
-	pfm_reg_check_t		read_check;
-	pfm_reg_check_t		write_check;
-	unsigned long		dep_pmd[4];
-	unsigned long		dep_pmc[4];
-} pfm_reg_desc_t;
-
-/* assume cnum is a valid monitor */
-#define PMC_PM(cnum, val)	(((val) >> (pmu_conf->pmc_desc[cnum].pm_pos)) & 0x1)
-
-/*
- * This structure is initialized at boot time and contains
- * a description of the PMU main characteristics.
- *
- * If the probe function is defined, detection is based
- * on its return value: 
- * 	- 0 means recognized PMU
- * 	- anything else means not supported
- * When the probe function is not defined, then the pmu_family field
- * is used and it must match the host CPU family such that:
- * 	- cpu->family & config->pmu_family != 0
- */
-typedef struct {
-	unsigned long  ovfl_val;	/* overflow value for counters */
-
-	pfm_reg_desc_t *pmc_desc;	/* detailed PMC register dependencies descriptions */
-	pfm_reg_desc_t *pmd_desc;	/* detailed PMD register dependencies descriptions */
-
-	unsigned int   num_pmcs;	/* number of PMCS: computed at init time */
-	unsigned int   num_pmds;	/* number of PMDS: computed at init time */
-	unsigned long  impl_pmcs[4];	/* bitmask of implemented PMCS */
-	unsigned long  impl_pmds[4];	/* bitmask of implemented PMDS */
-
-	char	      *pmu_name;	/* PMU family name */
-	unsigned int  pmu_family;	/* cpuid family pattern used to identify pmu */
-	unsigned int  flags;		/* pmu specific flags */
-	unsigned int  num_ibrs;		/* number of IBRS: computed at init time */
-	unsigned int  num_dbrs;		/* number of DBRS: computed at init time */
-	unsigned int  num_counters;	/* PMC/PMD counting pairs : computed at init time */
-	int           (*probe)(void);   /* customized probe routine */
-	unsigned int  use_rr_dbregs:1;	/* set if debug registers used for range restriction */
-} pmu_config_t;
-/*
- * PMU specific flags
- */
-#define PFM_PMU_IRQ_RESEND	1	/* PMU needs explicit IRQ resend */
-
-/*
- * debug register related type definitions
- */
-typedef struct {
-	unsigned long ibr_mask:56;
-	unsigned long ibr_plm:4;
-	unsigned long ibr_ig:3;
-	unsigned long ibr_x:1;
-} ibr_mask_reg_t;
-
-typedef struct {
-	unsigned long dbr_mask:56;
-	unsigned long dbr_plm:4;
-	unsigned long dbr_ig:2;
-	unsigned long dbr_w:1;
-	unsigned long dbr_r:1;
-} dbr_mask_reg_t;
-
-typedef union {
-	unsigned long  val;
-	ibr_mask_reg_t ibr;
-	dbr_mask_reg_t dbr;
-} dbreg_t;
-
-
-/*
- * perfmon command descriptions
- */
-typedef struct {
-	int		(*cmd_func)(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs);
-	char		*cmd_name;
-	int		cmd_flags;
-	unsigned int	cmd_narg;
-	size_t		cmd_argsize;
-	int		(*cmd_getsize)(void *arg, size_t *sz);
-} pfm_cmd_desc_t;
-
-#define PFM_CMD_FD		0x01	/* command requires a file descriptor */
-#define PFM_CMD_ARG_READ	0x02	/* command must read argument(s) */
-#define PFM_CMD_ARG_RW		0x04	/* command must read/write argument(s) */
-#define PFM_CMD_STOP		0x08	/* command does not work on zombie context */
-
-
-#define PFM_CMD_NAME(cmd)	pfm_cmd_tab[(cmd)].cmd_name
-#define PFM_CMD_READ_ARG(cmd)	(pfm_cmd_tab[(cmd)].cmd_flags & PFM_CMD_ARG_READ)
-#define PFM_CMD_RW_ARG(cmd)	(pfm_cmd_tab[(cmd)].cmd_flags & PFM_CMD_ARG_RW)
-#define PFM_CMD_USE_FD(cmd)	(pfm_cmd_tab[(cmd)].cmd_flags & PFM_CMD_FD)
-#define PFM_CMD_STOPPED(cmd)	(pfm_cmd_tab[(cmd)].cmd_flags & PFM_CMD_STOP)
-
-#define PFM_CMD_ARG_MANY	-1 /* cannot be zero */
-
-typedef struct {
-	unsigned long pfm_spurious_ovfl_intr_count;	/* keep track of spurious ovfl interrupts */
-	unsigned long pfm_replay_ovfl_intr_count;	/* keep track of replayed ovfl interrupts */
-	unsigned long pfm_ovfl_intr_count; 		/* keep track of ovfl interrupts */
-	unsigned long pfm_ovfl_intr_cycles;		/* cycles spent processing ovfl interrupts */
-	unsigned long pfm_ovfl_intr_cycles_min;		/* min cycles spent processing ovfl interrupts */
-	unsigned long pfm_ovfl_intr_cycles_max;		/* max cycles spent processing ovfl interrupts */
-	unsigned long pfm_smpl_handler_calls;
-	unsigned long pfm_smpl_handler_cycles;
-	char pad[SMP_CACHE_BYTES] ____cacheline_aligned;
-} pfm_stats_t;
-
-/*
- * perfmon internal variables
- */
-static pfm_stats_t		pfm_stats[NR_CPUS];
-static pfm_session_t		pfm_sessions;	/* global sessions information */
-
-static DEFINE_SPINLOCK(pfm_alt_install_check);
-static pfm_intr_handler_desc_t  *pfm_alt_intr_handler;
-
-static struct proc_dir_entry 	*perfmon_dir;
-static pfm_uuid_t		pfm_null_uuid = {0,};
-
-static spinlock_t		pfm_buffer_fmt_lock;
-static LIST_HEAD(pfm_buffer_fmt_list);
-
-static pmu_config_t		*pmu_conf;
-
-/* sysctl() controls */
-pfm_sysctl_t pfm_sysctl;
-EXPORT_SYMBOL(pfm_sysctl);
-
-static ctl_table pfm_ctl_table[]={
-	{
-		.ctl_name	= CTL_UNNUMBERED,
-		.procname	= "debug",
-		.data		= &pfm_sysctl.debug,
-		.maxlen		= sizeof(int),
-		.mode		= 0666,
-		.proc_handler	= &proc_dointvec,
-	},
-	{
-		.ctl_name	= CTL_UNNUMBERED,
-		.procname	= "debug_ovfl",
-		.data		= &pfm_sysctl.debug_ovfl,
-		.maxlen		= sizeof(int),
-		.mode		= 0666,
-		.proc_handler	= &proc_dointvec,
-	},
-	{
-		.ctl_name	= CTL_UNNUMBERED,
-		.procname	= "fastctxsw",
-		.data		= &pfm_sysctl.fastctxsw,
-		.maxlen		= sizeof(int),
-		.mode		= 0600,
-		.proc_handler	=  &proc_dointvec,
-	},
-	{
-		.ctl_name	= CTL_UNNUMBERED,
-		.procname	= "expert_mode",
-		.data		= &pfm_sysctl.expert_mode,
-		.maxlen		= sizeof(int),
-		.mode		= 0600,
-		.proc_handler	= &proc_dointvec,
-	},
-	{}
-};
-static ctl_table pfm_sysctl_dir[] = {
-	{
-		.ctl_name	= CTL_UNNUMBERED,
-		.procname	= "perfmon",
-		.mode		= 0555,
-		.child		= pfm_ctl_table,
-	},
- 	{}
-};
-static ctl_table pfm_sysctl_root[] = {
-	{
-		.ctl_name	= CTL_KERN,
-		.procname	= "kernel",
-		.mode		= 0555,
-		.child		= pfm_sysctl_dir,
-	},
- 	{}
-};
-static struct ctl_table_header *pfm_sysctl_header;
-
-static int pfm_context_unload(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs);
-
-#define pfm_get_cpu_var(v)		__ia64_per_cpu_var(v)
-#define pfm_get_cpu_data(a,b)		per_cpu(a, b)
-
-static inline void
-pfm_put_task(struct task_struct *task)
-{
-	if (task != current) put_task_struct(task);
-}
-
-static inline void
-pfm_reserve_page(unsigned long a)
-{
-	SetPageReserved(vmalloc_to_page((void *)a));
-}
-static inline void
-pfm_unreserve_page(unsigned long a)
-{
-	ClearPageReserved(vmalloc_to_page((void*)a));
-}
-
-static inline unsigned long
-pfm_protect_ctx_ctxsw(pfm_context_t *x)
-{
-	spin_lock(&(x)->ctx_lock);
-	return 0UL;
-}
-
-static inline void
-pfm_unprotect_ctx_ctxsw(pfm_context_t *x, unsigned long f)
-{
-	spin_unlock(&(x)->ctx_lock);
-}
-
-static inline unsigned int
-pfm_do_munmap(struct mm_struct *mm, unsigned long addr, size_t len, int acct)
-{
-	return do_munmap(mm, addr, len);
-}
-
-static inline unsigned long 
-pfm_get_unmapped_area(struct file *file, unsigned long addr, unsigned long len, unsigned long pgoff, unsigned long flags, unsigned long exec)
-{
-	return get_unmapped_area(file, addr, len, pgoff, flags);
-}
-
-
-static int
-pfmfs_get_sb(struct file_system_type *fs_type, int flags, const char *dev_name, void *data,
-	     struct vfsmount *mnt)
-{
-	return get_sb_pseudo(fs_type, "pfm:", NULL, PFMFS_MAGIC, mnt);
-}
-
-static struct file_system_type pfm_fs_type = {
-	.name     = "pfmfs",
-	.get_sb   = pfmfs_get_sb,
-	.kill_sb  = kill_anon_super,
-};
-
-DEFINE_PER_CPU(unsigned long, pfm_syst_info);
-DEFINE_PER_CPU(struct task_struct *, pmu_owner);
-DEFINE_PER_CPU(pfm_context_t  *, pmu_ctx);
-DEFINE_PER_CPU(unsigned long, pmu_activation_number);
-EXPORT_PER_CPU_SYMBOL_GPL(pfm_syst_info);
-
-
-/* forward declaration */
-static const struct file_operations pfm_file_ops;
-
-/*
- * forward declarations
- */
-#ifndef CONFIG_SMP
-static void pfm_lazy_save_regs (struct task_struct *ta);
-#endif
-
-void dump_pmu_state(const char *);
-static int pfm_write_ibr_dbr(int mode, pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs);
-
-#include "perfmon_itanium.h"
-#include "perfmon_mckinley.h"
-#include "perfmon_montecito.h"
-#include "perfmon_generic.h"
-
-static pmu_config_t *pmu_confs[]={
-	&pmu_conf_mont,
-	&pmu_conf_mck,
-	&pmu_conf_ita,
-	&pmu_conf_gen, /* must be last */
-	NULL
-};
-
-
-static int pfm_end_notify_user(pfm_context_t *ctx);
-
-static inline void
-pfm_clear_psr_pp(void)
-{
-	ia64_rsm(IA64_PSR_PP);
-	ia64_srlz_i();
-}
-
-static inline void
-pfm_set_psr_pp(void)
-{
-	ia64_ssm(IA64_PSR_PP);
-	ia64_srlz_i();
-}
-
-static inline void
-pfm_clear_psr_up(void)
-{
-	ia64_rsm(IA64_PSR_UP);
-	ia64_srlz_i();
-}
-
-static inline void
-pfm_set_psr_up(void)
-{
-	ia64_ssm(IA64_PSR_UP);
-	ia64_srlz_i();
-}
-
-static inline unsigned long
-pfm_get_psr(void)
-{
-	unsigned long tmp;
-	tmp = ia64_getreg(_IA64_REG_PSR);
-	ia64_srlz_i();
-	return tmp;
-}
-
-static inline void
-pfm_set_psr_l(unsigned long val)
-{
-	ia64_setreg(_IA64_REG_PSR_L, val);
-	ia64_srlz_i();
-}
-
-static inline void
-pfm_freeze_pmu(void)
-{
-	ia64_set_pmc(0,1UL);
-	ia64_srlz_d();
-}
-
-static inline void
-pfm_unfreeze_pmu(void)
-{
-	ia64_set_pmc(0,0UL);
-	ia64_srlz_d();
-}
-
-static inline void
-pfm_restore_ibrs(unsigned long *ibrs, unsigned int nibrs)
-{
-	int i;
-
-	for (i=0; i < nibrs; i++) {
-		ia64_set_ibr(i, ibrs[i]);
-		ia64_dv_serialize_instruction();
-	}
-	ia64_srlz_i();
-}
-
-static inline void
-pfm_restore_dbrs(unsigned long *dbrs, unsigned int ndbrs)
-{
-	int i;
-
-	for (i=0; i < ndbrs; i++) {
-		ia64_set_dbr(i, dbrs[i]);
-		ia64_dv_serialize_data();
-	}
-	ia64_srlz_d();
-}
-
-/*
- * PMD[i] must be a counter. no check is made
- */
-static inline unsigned long
-pfm_read_soft_counter(pfm_context_t *ctx, int i)
-{
-	return ctx->ctx_pmds[i].val + (ia64_get_pmd(i) & pmu_conf->ovfl_val);
-}
-
-/*
- * PMD[i] must be a counter. no check is made
- */
-static inline void
-pfm_write_soft_counter(pfm_context_t *ctx, int i, unsigned long val)
-{
-	unsigned long ovfl_val = pmu_conf->ovfl_val;
-
-	ctx->ctx_pmds[i].val = val  & ~ovfl_val;
-	/*
-	 * writing to unimplemented part is ignore, so we do not need to
-	 * mask off top part
-	 */
-	ia64_set_pmd(i, val & ovfl_val);
-}
-
-static pfm_msg_t *
-pfm_get_new_msg(pfm_context_t *ctx)
-{
-	int idx, next;
-
-	next = (ctx->ctx_msgq_tail+1) % PFM_MAX_MSGS;
-
-	DPRINT(("ctx_fd=%p head=%d tail=%d\n", ctx, ctx->ctx_msgq_head, ctx->ctx_msgq_tail));
-	if (next == ctx->ctx_msgq_head) return NULL;
-
- 	idx = 	ctx->ctx_msgq_tail;
-	ctx->ctx_msgq_tail = next;
-
-	DPRINT(("ctx=%p head=%d tail=%d msg=%d\n", ctx, ctx->ctx_msgq_head, ctx->ctx_msgq_tail, idx));
-
-	return ctx->ctx_msgq+idx;
-}
-
-static pfm_msg_t *
-pfm_get_next_msg(pfm_context_t *ctx)
-{
-	pfm_msg_t *msg;
-
-	DPRINT(("ctx=%p head=%d tail=%d\n", ctx, ctx->ctx_msgq_head, ctx->ctx_msgq_tail));
-
-	if (PFM_CTXQ_EMPTY(ctx)) return NULL;
-
-	/*
-	 * get oldest message
-	 */
-	msg = ctx->ctx_msgq+ctx->ctx_msgq_head;
-
-	/*
-	 * and move forward
-	 */
-	ctx->ctx_msgq_head = (ctx->ctx_msgq_head+1) % PFM_MAX_MSGS;
-
-	DPRINT(("ctx=%p head=%d tail=%d type=%d\n", ctx, ctx->ctx_msgq_head, ctx->ctx_msgq_tail, msg->pfm_gen_msg.msg_type));
-
-	return msg;
-}
-
-static void
-pfm_reset_msgq(pfm_context_t *ctx)
-{
-	ctx->ctx_msgq_head = ctx->ctx_msgq_tail = 0;
-	DPRINT(("ctx=%p msgq reset\n", ctx));
-}
-
-static void *
-pfm_rvmalloc(unsigned long size)
-{
-	void *mem;
-	unsigned long addr;
-
-	size = PAGE_ALIGN(size);
-	mem  = vmalloc(size);
-	if (mem) {
-		//printk("perfmon: CPU%d pfm_rvmalloc(%ld)=%p\n", smp_processor_id(), size, mem);
-		memset(mem, 0, size);
-		addr = (unsigned long)mem;
-		while (size > 0) {
-			pfm_reserve_page(addr);
-			addr+=PAGE_SIZE;
-			size-=PAGE_SIZE;
-		}
-	}
-	return mem;
-}
-
-static void
-pfm_rvfree(void *mem, unsigned long size)
-{
-	unsigned long addr;
-
-	if (mem) {
-		DPRINT(("freeing physical buffer @%p size=%lu\n", mem, size));
-		addr = (unsigned long) mem;
-		while ((long) size > 0) {
-			pfm_unreserve_page(addr);
-			addr+=PAGE_SIZE;
-			size-=PAGE_SIZE;
-		}
-		vfree(mem);
-	}
-	return;
-}
-
-static pfm_context_t *
-pfm_context_alloc(int ctx_flags)
-{
-	pfm_context_t *ctx;
-
-	/* 
-	 * allocate context descriptor 
-	 * must be able to free with interrupts disabled
-	 */
-	ctx = kzalloc(sizeof(pfm_context_t), GFP_KERNEL);
-	if (ctx) {
-		DPRINT(("alloc ctx @%p\n", ctx));
-
-		/*
-		 * init context protection lock
-		 */
-		spin_lock_init(&ctx->ctx_lock);
-
-		/*
-		 * context is unloaded
-		 */
-		ctx->ctx_state = PFM_CTX_UNLOADED;
-
-		/*
-		 * initialization of context's flags
-		 */
-		ctx->ctx_fl_block       = (ctx_flags & PFM_FL_NOTIFY_BLOCK) ? 1 : 0;
-		ctx->ctx_fl_system      = (ctx_flags & PFM_FL_SYSTEM_WIDE) ? 1: 0;
-		ctx->ctx_fl_no_msg      = (ctx_flags & PFM_FL_OVFL_NO_MSG) ? 1: 0;
-		/*
-		 * will move to set properties
-		 * ctx->ctx_fl_excl_idle   = (ctx_flags & PFM_FL_EXCL_IDLE) ? 1: 0;
-		 */
-
-		/*
-		 * init restart semaphore to locked
-		 */
-		init_completion(&ctx->ctx_restart_done);
-
-		/*
-		 * activation is used in SMP only
-		 */
-		ctx->ctx_last_activation = PFM_INVALID_ACTIVATION;
-		SET_LAST_CPU(ctx, -1);
-
-		/*
-		 * initialize notification message queue
-		 */
-		ctx->ctx_msgq_head = ctx->ctx_msgq_tail = 0;
-		init_waitqueue_head(&ctx->ctx_msgq_wait);
-		init_waitqueue_head(&ctx->ctx_zombieq);
-
-	}
-	return ctx;
-}
-
-static void
-pfm_context_free(pfm_context_t *ctx)
-{
-	if (ctx) {
-		DPRINT(("free ctx @%p\n", ctx));
-		kfree(ctx);
-	}
-}
-
-static void
-pfm_mask_monitoring(struct task_struct *task)
-{
-	pfm_context_t *ctx = PFM_GET_CTX(task);
-	unsigned long mask, val, ovfl_mask;
-	int i;
-
-	DPRINT_ovfl(("masking monitoring for [%d]\n", task_pid_nr(task)));
-
-	ovfl_mask = pmu_conf->ovfl_val;
-	/*
-	 * monitoring can only be masked as a result of a valid
-	 * counter overflow. In UP, it means that the PMU still
-	 * has an owner. Note that the owner can be different
-	 * from the current task. However the PMU state belongs
-	 * to the owner.
-	 * In SMP, a valid overflow only happens when task is
-	 * current. Therefore if we come here, we know that
-	 * the PMU state belongs to the current task, therefore
-	 * we can access the live registers.
-	 *
-	 * So in both cases, the live register contains the owner's
-	 * state. We can ONLY touch the PMU registers and NOT the PSR.
-	 *
-	 * As a consequence to this call, the ctx->th_pmds[] array
-	 * contains stale information which must be ignored
-	 * when context is reloaded AND monitoring is active (see
-	 * pfm_restart).
-	 */
-	mask = ctx->ctx_used_pmds[0];
-	for (i = 0; mask; i++, mask>>=1) {
-		/* skip non used pmds */
-		if ((mask & 0x1) == 0) continue;
-		val = ia64_get_pmd(i);
-
-		if (PMD_IS_COUNTING(i)) {
-			/*
-		 	 * we rebuild the full 64 bit value of the counter
-		 	 */
-			ctx->ctx_pmds[i].val += (val & ovfl_mask);
-		} else {
-			ctx->ctx_pmds[i].val = val;
-		}
-		DPRINT_ovfl(("pmd[%d]=0x%lx hw_pmd=0x%lx\n",
-			i,
-			ctx->ctx_pmds[i].val,
-			val & ovfl_mask));
-	}
-	/*
-	 * mask monitoring by setting the privilege level to 0
-	 * we cannot use psr.pp/psr.up for this, it is controlled by
-	 * the user
-	 *
-	 * if task is current, modify actual registers, otherwise modify
-	 * thread save state, i.e., what will be restored in pfm_load_regs()
-	 */
-	mask = ctx->ctx_used_monitors[0] >> PMU_FIRST_COUNTER;
-	for(i= PMU_FIRST_COUNTER; mask; i++, mask>>=1) {
-		if ((mask & 0x1) == 0UL) continue;
-		ia64_set_pmc(i, ctx->th_pmcs[i] & ~0xfUL);
-		ctx->th_pmcs[i] &= ~0xfUL;
-		DPRINT_ovfl(("pmc[%d]=0x%lx\n", i, ctx->th_pmcs[i]));
-	}
-	/*
-	 * make all of this visible
-	 */
-	ia64_srlz_d();
-}
-
-/*
- * must always be done with task == current
- *
- * context must be in MASKED state when calling
- */
-static void
-pfm_restore_monitoring(struct task_struct *task)
-{
-	pfm_context_t *ctx = PFM_GET_CTX(task);
-	unsigned long mask, ovfl_mask;
-	unsigned long psr, val;
-	int i, is_system;
-
-	is_system = ctx->ctx_fl_system;
-	ovfl_mask = pmu_conf->ovfl_val;
-
-	if (task != current) {
-		printk(KERN_ERR "perfmon.%d: invalid task[%d] current[%d]\n", __LINE__, task_pid_nr(task), task_pid_nr(current));
-		return;
-	}
-	if (ctx->ctx_state != PFM_CTX_MASKED) {
-		printk(KERN_ERR "perfmon.%d: task[%d] current[%d] invalid state=%d\n", __LINE__,
-			task_pid_nr(task), task_pid_nr(current), ctx->ctx_state);
-		return;
-	}
-	psr = pfm_get_psr();
-	/*
-	 * monitoring is masked via the PMC.
-	 * As we restore their value, we do not want each counter to
-	 * restart right away. We stop monitoring using the PSR,
-	 * restore the PMC (and PMD) and then re-establish the psr
-	 * as it was. Note that there can be no pending overflow at
-	 * this point, because monitoring was MASKED.
-	 *
-	 * system-wide session are pinned and self-monitoring
-	 */
-	if (is_system && (PFM_CPUINFO_GET() & PFM_CPUINFO_DCR_PP)) {
-		/* disable dcr pp */
-		ia64_setreg(_IA64_REG_CR_DCR, ia64_getreg(_IA64_REG_CR_DCR) & ~IA64_DCR_PP);
-		pfm_clear_psr_pp();
-	} else {
-		pfm_clear_psr_up();
-	}
-	/*
-	 * first, we restore the PMD
-	 */
-	mask = ctx->ctx_used_pmds[0];
-	for (i = 0; mask; i++, mask>>=1) {
-		/* skip non used pmds */
-		if ((mask & 0x1) == 0) continue;
-
-		if (PMD_IS_COUNTING(i)) {
-			/*
-			 * we split the 64bit value according to
-			 * counter width
-			 */
-			val = ctx->ctx_pmds[i].val & ovfl_mask;
-			ctx->ctx_pmds[i].val &= ~ovfl_mask;
-		} else {
-			val = ctx->ctx_pmds[i].val;
-		}
-		ia64_set_pmd(i, val);
-
-		DPRINT(("pmd[%d]=0x%lx hw_pmd=0x%lx\n",
-			i,
-			ctx->ctx_pmds[i].val,
-			val));
-	}
-	/*
-	 * restore the PMCs
-	 */
-	mask = ctx->ctx_used_monitors[0] >> PMU_FIRST_COUNTER;
-	for(i= PMU_FIRST_COUNTER; mask; i++, mask>>=1) {
-		if ((mask & 0x1) == 0UL) continue;
-		ctx->th_pmcs[i] = ctx->ctx_pmcs[i];
-		ia64_set_pmc(i, ctx->th_pmcs[i]);
-		DPRINT(("[%d] pmc[%d]=0x%lx\n",
-					task_pid_nr(task), i, ctx->th_pmcs[i]));
-	}
-	ia64_srlz_d();
-
-	/*
-	 * must restore DBR/IBR because could be modified while masked
-	 * XXX: need to optimize 
-	 */
-	if (ctx->ctx_fl_using_dbreg) {
-		pfm_restore_ibrs(ctx->ctx_ibrs, pmu_conf->num_ibrs);
-		pfm_restore_dbrs(ctx->ctx_dbrs, pmu_conf->num_dbrs);
-	}
-
-	/*
-	 * now restore PSR
-	 */
-	if (is_system && (PFM_CPUINFO_GET() & PFM_CPUINFO_DCR_PP)) {
-		/* enable dcr pp */
-		ia64_setreg(_IA64_REG_CR_DCR, ia64_getreg(_IA64_REG_CR_DCR) | IA64_DCR_PP);
-		ia64_srlz_i();
-	}
-	pfm_set_psr_l(psr);
-}
-
-static inline void
-pfm_save_pmds(unsigned long *pmds, unsigned long mask)
-{
-	int i;
-
-	ia64_srlz_d();
-
-	for (i=0; mask; i++, mask>>=1) {
-		if (mask & 0x1) pmds[i] = ia64_get_pmd(i);
-	}
-}
-
-/*
- * reload from thread state (used for ctxw only)
- */
-static inline void
-pfm_restore_pmds(unsigned long *pmds, unsigned long mask)
-{
-	int i;
-	unsigned long val, ovfl_val = pmu_conf->ovfl_val;
-
-	for (i=0; mask; i++, mask>>=1) {
-		if ((mask & 0x1) == 0) continue;
-		val = PMD_IS_COUNTING(i) ? pmds[i] & ovfl_val : pmds[i];
-		ia64_set_pmd(i, val);
-	}
-	ia64_srlz_d();
-}
-
-/*
- * propagate PMD from context to thread-state
- */
-static inline void
-pfm_copy_pmds(struct task_struct *task, pfm_context_t *ctx)
-{
-	unsigned long ovfl_val = pmu_conf->ovfl_val;
-	unsigned long mask = ctx->ctx_all_pmds[0];
-	unsigned long val;
-	int i;
-
-	DPRINT(("mask=0x%lx\n", mask));
-
-	for (i=0; mask; i++, mask>>=1) {
-
-		val = ctx->ctx_pmds[i].val;
-
-		/*
-		 * We break up the 64 bit value into 2 pieces
-		 * the lower bits go to the machine state in the
-		 * thread (will be reloaded on ctxsw in).
-		 * The upper part stays in the soft-counter.
-		 */
-		if (PMD_IS_COUNTING(i)) {
-			ctx->ctx_pmds[i].val = val & ~ovfl_val;
-			 val &= ovfl_val;
-		}
-		ctx->th_pmds[i] = val;
-
-		DPRINT(("pmd[%d]=0x%lx soft_val=0x%lx\n",
-			i,
-			ctx->th_pmds[i],
-			ctx->ctx_pmds[i].val));
-	}
-}
-
-/*
- * propagate PMC from context to thread-state
- */
-static inline void
-pfm_copy_pmcs(struct task_struct *task, pfm_context_t *ctx)
-{
-	unsigned long mask = ctx->ctx_all_pmcs[0];
-	int i;
-
-	DPRINT(("mask=0x%lx\n", mask));
-
-	for (i=0; mask; i++, mask>>=1) {
-		/* masking 0 with ovfl_val yields 0 */
-		ctx->th_pmcs[i] = ctx->ctx_pmcs[i];
-		DPRINT(("pmc[%d]=0x%lx\n", i, ctx->th_pmcs[i]));
-	}
-}
-
-
-
-static inline void
-pfm_restore_pmcs(unsigned long *pmcs, unsigned long mask)
-{
-	int i;
-
-	for (i=0; mask; i++, mask>>=1) {
-		if ((mask & 0x1) == 0) continue;
-		ia64_set_pmc(i, pmcs[i]);
-	}
-	ia64_srlz_d();
-}
-
-static inline int
-pfm_uuid_cmp(pfm_uuid_t a, pfm_uuid_t b)
-{
-	return memcmp(a, b, sizeof(pfm_uuid_t));
-}
-
-static inline int
-pfm_buf_fmt_exit(pfm_buffer_fmt_t *fmt, struct task_struct *task, void *buf, struct pt_regs *regs)
-{
-	int ret = 0;
-	if (fmt->fmt_exit) ret = (*fmt->fmt_exit)(task, buf, regs);
-	return ret;
-}
-
-static inline int
-pfm_buf_fmt_getsize(pfm_buffer_fmt_t *fmt, struct task_struct *task, unsigned int flags, int cpu, void *arg, unsigned long *size)
-{
-	int ret = 0;
-	if (fmt->fmt_getsize) ret = (*fmt->fmt_getsize)(task, flags, cpu, arg, size);
-	return ret;
-}
-
-
-static inline int
-pfm_buf_fmt_validate(pfm_buffer_fmt_t *fmt, struct task_struct *task, unsigned int flags,
-		     int cpu, void *arg)
-{
-	int ret = 0;
-	if (fmt->fmt_validate) ret = (*fmt->fmt_validate)(task, flags, cpu, arg);
-	return ret;
-}
-
-static inline int
-pfm_buf_fmt_init(pfm_buffer_fmt_t *fmt, struct task_struct *task, void *buf, unsigned int flags,
-		     int cpu, void *arg)
-{
-	int ret = 0;
-	if (fmt->fmt_init) ret = (*fmt->fmt_init)(task, buf, flags, cpu, arg);
-	return ret;
-}
-
-static inline int
-pfm_buf_fmt_restart(pfm_buffer_fmt_t *fmt, struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs)
-{
-	int ret = 0;
-	if (fmt->fmt_restart) ret = (*fmt->fmt_restart)(task, ctrl, buf, regs);
-	return ret;
-}
-
-static inline int
-pfm_buf_fmt_restart_active(pfm_buffer_fmt_t *fmt, struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs)
-{
-	int ret = 0;
-	if (fmt->fmt_restart_active) ret = (*fmt->fmt_restart_active)(task, ctrl, buf, regs);
-	return ret;
-}
-
-static pfm_buffer_fmt_t *
-__pfm_find_buffer_fmt(pfm_uuid_t uuid)
-{
-	struct list_head * pos;
-	pfm_buffer_fmt_t * entry;
-
-	list_for_each(pos, &pfm_buffer_fmt_list) {
-		entry = list_entry(pos, pfm_buffer_fmt_t, fmt_list);
-		if (pfm_uuid_cmp(uuid, entry->fmt_uuid) == 0)
-			return entry;
-	}
-	return NULL;
-}
- 
-/*
- * find a buffer format based on its uuid
- */
-static pfm_buffer_fmt_t *
-pfm_find_buffer_fmt(pfm_uuid_t uuid)
-{
-	pfm_buffer_fmt_t * fmt;
-	spin_lock(&pfm_buffer_fmt_lock);
-	fmt = __pfm_find_buffer_fmt(uuid);
-	spin_unlock(&pfm_buffer_fmt_lock);
-	return fmt;
-}
- 
-int
-pfm_register_buffer_fmt(pfm_buffer_fmt_t *fmt)
-{
-	int ret = 0;
-
-	/* some sanity checks */
-	if (fmt == NULL || fmt->fmt_name == NULL) return -EINVAL;
-
-	/* we need at least a handler */
-	if (fmt->fmt_handler == NULL) return -EINVAL;
-
-	/*
-	 * XXX: need check validity of fmt_arg_size
-	 */
-
-	spin_lock(&pfm_buffer_fmt_lock);
-
-	if (__pfm_find_buffer_fmt(fmt->fmt_uuid)) {
-		printk(KERN_ERR "perfmon: duplicate sampling format: %s\n", fmt->fmt_name);
-		ret = -EBUSY;
-		goto out;
-	} 
-	list_add(&fmt->fmt_list, &pfm_buffer_fmt_list);
-	printk(KERN_INFO "perfmon: added sampling format %s\n", fmt->fmt_name);
-
-out:
-	spin_unlock(&pfm_buffer_fmt_lock);
- 	return ret;
-}
-EXPORT_SYMBOL(pfm_register_buffer_fmt);
-
-int
-pfm_unregister_buffer_fmt(pfm_uuid_t uuid)
-{
-	pfm_buffer_fmt_t *fmt;
-	int ret = 0;
-
-	spin_lock(&pfm_buffer_fmt_lock);
-
-	fmt = __pfm_find_buffer_fmt(uuid);
-	if (!fmt) {
-		printk(KERN_ERR "perfmon: cannot unregister format, not found\n");
-		ret = -EINVAL;
-		goto out;
-	}
-	list_del_init(&fmt->fmt_list);
-	printk(KERN_INFO "perfmon: removed sampling format: %s\n", fmt->fmt_name);
-
-out:
-	spin_unlock(&pfm_buffer_fmt_lock);
-	return ret;
-
-}
-EXPORT_SYMBOL(pfm_unregister_buffer_fmt);
-
-extern void update_pal_halt_status(int);
-
-static int
-pfm_reserve_session(struct task_struct *task, int is_syswide, unsigned int cpu)
-{
-	unsigned long flags;
-	/*
-	 * validity checks on cpu_mask have been done upstream
-	 */
-	LOCK_PFS(flags);
-
-	DPRINT(("in sys_sessions=%u task_sessions=%u dbregs=%u syswide=%d cpu=%u\n",
-		pfm_sessions.pfs_sys_sessions,
-		pfm_sessions.pfs_task_sessions,
-		pfm_sessions.pfs_sys_use_dbregs,
-		is_syswide,
-		cpu));
-
-	if (is_syswide) {
-		/*
-		 * cannot mix system wide and per-task sessions
-		 */
-		if (pfm_sessions.pfs_task_sessions > 0UL) {
-			DPRINT(("system wide not possible, %u conflicting task_sessions\n",
-			  	pfm_sessions.pfs_task_sessions));
-			goto abort;
-		}
-
-		if (pfm_sessions.pfs_sys_session[cpu]) goto error_conflict;
-
-		DPRINT(("reserving system wide session on CPU%u currently on CPU%u\n", cpu, smp_processor_id()));
-
-		pfm_sessions.pfs_sys_session[cpu] = task;
-
-		pfm_sessions.pfs_sys_sessions++ ;
-
-	} else {
-		if (pfm_sessions.pfs_sys_sessions) goto abort;
-		pfm_sessions.pfs_task_sessions++;
-	}
-
-	DPRINT(("out sys_sessions=%u task_sessions=%u dbregs=%u syswide=%d cpu=%u\n",
-		pfm_sessions.pfs_sys_sessions,
-		pfm_sessions.pfs_task_sessions,
-		pfm_sessions.pfs_sys_use_dbregs,
-		is_syswide,
-		cpu));
-
-	/*
-	 * disable default_idle() to go to PAL_HALT
-	 */
-	update_pal_halt_status(0);
-
-	UNLOCK_PFS(flags);
-
-	return 0;
-
-error_conflict:
-	DPRINT(("system wide not possible, conflicting session [%d] on CPU%d\n",
-  		task_pid_nr(pfm_sessions.pfs_sys_session[cpu]),
-		cpu));
-abort:
-	UNLOCK_PFS(flags);
-
-	return -EBUSY;
-
-}
-
-static int
-pfm_unreserve_session(pfm_context_t *ctx, int is_syswide, unsigned int cpu)
-{
-	unsigned long flags;
-	/*
-	 * validity checks on cpu_mask have been done upstream
-	 */
-	LOCK_PFS(flags);
-
-	DPRINT(("in sys_sessions=%u task_sessions=%u dbregs=%u syswide=%d cpu=%u\n",
-		pfm_sessions.pfs_sys_sessions,
-		pfm_sessions.pfs_task_sessions,
-		pfm_sessions.pfs_sys_use_dbregs,
-		is_syswide,
-		cpu));
-
-
-	if (is_syswide) {
-		pfm_sessions.pfs_sys_session[cpu] = NULL;
-		/*
-		 * would not work with perfmon+more than one bit in cpu_mask
-		 */
-		if (ctx && ctx->ctx_fl_using_dbreg) {
-			if (pfm_sessions.pfs_sys_use_dbregs == 0) {
-				printk(KERN_ERR "perfmon: invalid release for ctx %p sys_use_dbregs=0\n", ctx);
-			} else {
-				pfm_sessions.pfs_sys_use_dbregs--;
-			}
-		}
-		pfm_sessions.pfs_sys_sessions--;
-	} else {
-		pfm_sessions.pfs_task_sessions--;
-	}
-	DPRINT(("out sys_sessions=%u task_sessions=%u dbregs=%u syswide=%d cpu=%u\n",
-		pfm_sessions.pfs_sys_sessions,
-		pfm_sessions.pfs_task_sessions,
-		pfm_sessions.pfs_sys_use_dbregs,
-		is_syswide,
-		cpu));
-
-	/*
-	 * if possible, enable default_idle() to go into PAL_HALT
-	 */
-	if (pfm_sessions.pfs_task_sessions == 0 && pfm_sessions.pfs_sys_sessions == 0)
-		update_pal_halt_status(1);
-
-	UNLOCK_PFS(flags);
-
-	return 0;
-}
-
-/*
- * removes virtual mapping of the sampling buffer.
- * IMPORTANT: cannot be called with interrupts disable, e.g. inside
- * a PROTECT_CTX() section.
- */
-static int
-pfm_remove_smpl_mapping(struct task_struct *task, void *vaddr, unsigned long size)
-{
-	int r;
-
-	/* sanity checks */
-	if (task->mm == NULL || size == 0UL || vaddr == NULL) {
-		printk(KERN_ERR "perfmon: pfm_remove_smpl_mapping [%d] invalid context mm=%p\n", task_pid_nr(task), task->mm);
-		return -EINVAL;
-	}
-
-	DPRINT(("smpl_vaddr=%p size=%lu\n", vaddr, size));
-
-	/*
-	 * does the actual unmapping
-	 */
-	down_write(&task->mm->mmap_sem);
-
-	DPRINT(("down_write done smpl_vaddr=%p size=%lu\n", vaddr, size));
-
-	r = pfm_do_munmap(task->mm, (unsigned long)vaddr, size, 0);
-
-	up_write(&task->mm->mmap_sem);
-	if (r !=0) {
-		printk(KERN_ERR "perfmon: [%d] unable to unmap sampling buffer @%p size=%lu\n", task_pid_nr(task), vaddr, size);
-	}
-
-	DPRINT(("do_unmap(%p, %lu)=%d\n", vaddr, size, r));
-
-	return 0;
-}
-
-/*
- * free actual physical storage used by sampling buffer
- */
-#if 0
-static int
-pfm_free_smpl_buffer(pfm_context_t *ctx)
-{
-	pfm_buffer_fmt_t *fmt;
-
-	if (ctx->ctx_smpl_hdr == NULL) goto invalid_free;
-
-	/*
-	 * we won't use the buffer format anymore
-	 */
-	fmt = ctx->ctx_buf_fmt;
-
-	DPRINT(("sampling buffer @%p size %lu vaddr=%p\n",
-		ctx->ctx_smpl_hdr,
-		ctx->ctx_smpl_size,
-		ctx->ctx_smpl_vaddr));
-
-	pfm_buf_fmt_exit(fmt, current, NULL, NULL);
-
-	/*
-	 * free the buffer
-	 */
-	pfm_rvfree(ctx->ctx_smpl_hdr, ctx->ctx_smpl_size);
-
-	ctx->ctx_smpl_hdr  = NULL;
-	ctx->ctx_smpl_size = 0UL;
-
-	return 0;
-
-invalid_free:
-	printk(KERN_ERR "perfmon: pfm_free_smpl_buffer [%d] no buffer\n", task_pid_nr(current));
-	return -EINVAL;
-}
-#endif
-
-static inline void
-pfm_exit_smpl_buffer(pfm_buffer_fmt_t *fmt)
-{
-	if (fmt == NULL) return;
-
-	pfm_buf_fmt_exit(fmt, current, NULL, NULL);
-
-}
-
-/*
- * pfmfs should _never_ be mounted by userland - too much of security hassle,
- * no real gain from having the whole whorehouse mounted. So we don't need
- * any operations on the root directory. However, we need a non-trivial
- * d_name - pfm: will go nicely and kill the special-casing in procfs.
- */
-static struct vfsmount *pfmfs_mnt;
-
-static int __init
-init_pfm_fs(void)
-{
-	int err = register_filesystem(&pfm_fs_type);
-	if (!err) {
-		pfmfs_mnt = kern_mount(&pfm_fs_type);
-		err = PTR_ERR(pfmfs_mnt);
-		if (IS_ERR(pfmfs_mnt))
-			unregister_filesystem(&pfm_fs_type);
-		else
-			err = 0;
-	}
-	return err;
-}
-
-static ssize_t
-pfm_read(struct file *filp, char __user *buf, size_t size, loff_t *ppos)
-{
-	pfm_context_t *ctx;
-	pfm_msg_t *msg;
-	ssize_t ret;
-	unsigned long flags;
-  	DECLARE_WAITQUEUE(wait, current);
-	if (PFM_IS_FILE(filp) == 0) {
-		printk(KERN_ERR "perfmon: pfm_poll: bad magic [%d]\n", task_pid_nr(current));
-		return -EINVAL;
-	}
-
-	ctx = (pfm_context_t *)filp->private_data;
-	if (ctx == NULL) {
-		printk(KERN_ERR "perfmon: pfm_read: NULL ctx [%d]\n", task_pid_nr(current));
-		return -EINVAL;
-	}
-
-	/*
-	 * check even when there is no message
-	 */
-	if (size < sizeof(pfm_msg_t)) {
-		DPRINT(("message is too small ctx=%p (>=%ld)\n", ctx, sizeof(pfm_msg_t)));
-		return -EINVAL;
-	}
-
-	PROTECT_CTX(ctx, flags);
-
-  	/*
-	 * put ourselves on the wait queue
-	 */
-  	add_wait_queue(&ctx->ctx_msgq_wait, &wait);
-
-
-  	for(;;) {
-		/*
-		 * check wait queue
-		 */
-
-  		set_current_state(TASK_INTERRUPTIBLE);
-
-		DPRINT(("head=%d tail=%d\n", ctx->ctx_msgq_head, ctx->ctx_msgq_tail));
-
-		ret = 0;
-		if(PFM_CTXQ_EMPTY(ctx) == 0) break;
-
-		UNPROTECT_CTX(ctx, flags);
-
-		/*
-		 * check non-blocking read
-		 */
-      		ret = -EAGAIN;
-		if(filp->f_flags & O_NONBLOCK) break;
-
-		/*
-		 * check pending signals
-		 */
-		if(signal_pending(current)) {
-			ret = -EINTR;
-			break;
-		}
-      		/*
-		 * no message, so wait
-		 */
-      		schedule();
-
-		PROTECT_CTX(ctx, flags);
-	}
-	DPRINT(("[%d] back to running ret=%ld\n", task_pid_nr(current), ret));
-  	set_current_state(TASK_RUNNING);
-	remove_wait_queue(&ctx->ctx_msgq_wait, &wait);
-
-	if (ret < 0) goto abort;
-
-	ret = -EINVAL;
-	msg = pfm_get_next_msg(ctx);
-	if (msg == NULL) {
-		printk(KERN_ERR "perfmon: pfm_read no msg for ctx=%p [%d]\n", ctx, task_pid_nr(current));
-		goto abort_locked;
-	}
-
-	DPRINT(("fd=%d type=%d\n", msg->pfm_gen_msg.msg_ctx_fd, msg->pfm_gen_msg.msg_type));
-
-	ret = -EFAULT;
-  	if(copy_to_user(buf, msg, sizeof(pfm_msg_t)) == 0) ret = sizeof(pfm_msg_t);
-
-abort_locked:
-	UNPROTECT_CTX(ctx, flags);
-abort:
-	return ret;
-}
-
-static ssize_t
-pfm_write(struct file *file, const char __user *ubuf,
-			  size_t size, loff_t *ppos)
-{
-	DPRINT(("pfm_write called\n"));
-	return -EINVAL;
-}
-
-static unsigned int
-pfm_poll(struct file *filp, poll_table * wait)
-{
-	pfm_context_t *ctx;
-	unsigned long flags;
-	unsigned int mask = 0;
-
-	if (PFM_IS_FILE(filp) == 0) {
-		printk(KERN_ERR "perfmon: pfm_poll: bad magic [%d]\n", task_pid_nr(current));
-		return 0;
-	}
-
-	ctx = (pfm_context_t *)filp->private_data;
-	if (ctx == NULL) {
-		printk(KERN_ERR "perfmon: pfm_poll: NULL ctx [%d]\n", task_pid_nr(current));
-		return 0;
-	}
-
-
-	DPRINT(("pfm_poll ctx_fd=%d before poll_wait\n", ctx->ctx_fd));
-
-	poll_wait(filp, &ctx->ctx_msgq_wait, wait);
-
-	PROTECT_CTX(ctx, flags);
-
-	if (PFM_CTXQ_EMPTY(ctx) == 0)
-		mask =  POLLIN | POLLRDNORM;
-
-	UNPROTECT_CTX(ctx, flags);
-
-	DPRINT(("pfm_poll ctx_fd=%d mask=0x%x\n", ctx->ctx_fd, mask));
-
-	return mask;
-}
-
-static int
-pfm_ioctl(struct inode *inode, struct file *file, unsigned int cmd, unsigned long arg)
-{
-	DPRINT(("pfm_ioctl called\n"));
-	return -EINVAL;
-}
-
-/*
- * interrupt cannot be masked when coming here
- */
-static inline int
-pfm_do_fasync(int fd, struct file *filp, pfm_context_t *ctx, int on)
-{
-	int ret;
-
-	ret = fasync_helper (fd, filp, on, &ctx->ctx_async_queue);
-
-	DPRINT(("pfm_fasync called by [%d] on ctx_fd=%d on=%d async_queue=%p ret=%d\n",
-		task_pid_nr(current),
-		fd,
-		on,
-		ctx->ctx_async_queue, ret));
-
-	return ret;
-}
-
-static int
-pfm_fasync(int fd, struct file *filp, int on)
-{
-	pfm_context_t *ctx;
-	int ret;
-
-	if (PFM_IS_FILE(filp) == 0) {
-		printk(KERN_ERR "perfmon: pfm_fasync bad magic [%d]\n", task_pid_nr(current));
-		return -EBADF;
-	}
-
-	ctx = (pfm_context_t *)filp->private_data;
-	if (ctx == NULL) {
-		printk(KERN_ERR "perfmon: pfm_fasync NULL ctx [%d]\n", task_pid_nr(current));
-		return -EBADF;
-	}
-	/*
-	 * we cannot mask interrupts during this call because this may
-	 * may go to sleep if memory is not readily avalaible.
-	 *
-	 * We are protected from the conetxt disappearing by the get_fd()/put_fd()
-	 * done in caller. Serialization of this function is ensured by caller.
-	 */
-	ret = pfm_do_fasync(fd, filp, ctx, on);
-
-
-	DPRINT(("pfm_fasync called on ctx_fd=%d on=%d async_queue=%p ret=%d\n",
-		fd,
-		on,
-		ctx->ctx_async_queue, ret));
-
-	return ret;
-}
-
-#ifdef CONFIG_SMP
-/*
- * this function is exclusively called from pfm_close().
- * The context is not protected at that time, nor are interrupts
- * on the remote CPU. That's necessary to avoid deadlocks.
- */
-static void
-pfm_syswide_force_stop(void *info)
-{
-	pfm_context_t   *ctx = (pfm_context_t *)info;
-	struct pt_regs *regs = task_pt_regs(current);
-	struct task_struct *owner;
-	unsigned long flags;
-	int ret;
-
-	if (ctx->ctx_cpu != smp_processor_id()) {
-		printk(KERN_ERR "perfmon: pfm_syswide_force_stop for CPU%d  but on CPU%d\n",
-			ctx->ctx_cpu,
-			smp_processor_id());
-		return;
-	}
-	owner = GET_PMU_OWNER();
-	if (owner != ctx->ctx_task) {
-		printk(KERN_ERR "perfmon: pfm_syswide_force_stop CPU%d unexpected owner [%d] instead of [%d]\n",
-			smp_processor_id(),
-			task_pid_nr(owner), task_pid_nr(ctx->ctx_task));
-		return;
-	}
-	if (GET_PMU_CTX() != ctx) {
-		printk(KERN_ERR "perfmon: pfm_syswide_force_stop CPU%d unexpected ctx %p instead of %p\n",
-			smp_processor_id(),
-			GET_PMU_CTX(), ctx);
-		return;
-	}
-
-	DPRINT(("on CPU%d forcing system wide stop for [%d]\n", smp_processor_id(), task_pid_nr(ctx->ctx_task)));
-	/*
-	 * the context is already protected in pfm_close(), we simply
-	 * need to mask interrupts to avoid a PMU interrupt race on
-	 * this CPU
-	 */
-	local_irq_save(flags);
-
-	ret = pfm_context_unload(ctx, NULL, 0, regs);
-	if (ret) {
-		DPRINT(("context_unload returned %d\n", ret));
-	}
-
-	/*
-	 * unmask interrupts, PMU interrupts are now spurious here
-	 */
-	local_irq_restore(flags);
-}
-
-static void
-pfm_syswide_cleanup_other_cpu(pfm_context_t *ctx)
-{
-	int ret;
-
-	DPRINT(("calling CPU%d for cleanup\n", ctx->ctx_cpu));
-	ret = smp_call_function_single(ctx->ctx_cpu, pfm_syswide_force_stop, ctx, 1);
-	DPRINT(("called CPU%d for cleanup ret=%d\n", ctx->ctx_cpu, ret));
-}
-#endif /* CONFIG_SMP */
-
-/*
- * called for each close(). Partially free resources.
- * When caller is self-monitoring, the context is unloaded.
- */
-static int
-pfm_flush(struct file *filp, fl_owner_t id)
-{
-	pfm_context_t *ctx;
-	struct task_struct *task;
-	struct pt_regs *regs;
-	unsigned long flags;
-	unsigned long smpl_buf_size = 0UL;
-	void *smpl_buf_vaddr = NULL;
-	int state, is_system;
-
-	if (PFM_IS_FILE(filp) == 0) {
-		DPRINT(("bad magic for\n"));
-		return -EBADF;
-	}
-
-	ctx = (pfm_context_t *)filp->private_data;
-	if (ctx == NULL) {
-		printk(KERN_ERR "perfmon: pfm_flush: NULL ctx [%d]\n", task_pid_nr(current));
-		return -EBADF;
-	}
-
-	/*
-	 * remove our file from the async queue, if we use this mode.
-	 * This can be done without the context being protected. We come
-	 * here when the context has become unreachable by other tasks.
-	 *
-	 * We may still have active monitoring at this point and we may
-	 * end up in pfm_overflow_handler(). However, fasync_helper()
-	 * operates with interrupts disabled and it cleans up the
-	 * queue. If the PMU handler is called prior to entering
-	 * fasync_helper() then it will send a signal. If it is
-	 * invoked after, it will find an empty queue and no
-	 * signal will be sent. In both case, we are safe
-	 */
-	PROTECT_CTX(ctx, flags);
-
-	state     = ctx->ctx_state;
-	is_system = ctx->ctx_fl_system;
-
-	task = PFM_CTX_TASK(ctx);
-	regs = task_pt_regs(task);
-
-	DPRINT(("ctx_state=%d is_current=%d\n",
-		state,
-		task == current ? 1 : 0));
-
-	/*
-	 * if state == UNLOADED, then task is NULL
-	 */
-
-	/*
-	 * we must stop and unload because we are losing access to the context.
-	 */
-	if (task == current) {
-#ifdef CONFIG_SMP
-		/*
-		 * the task IS the owner but it migrated to another CPU: that's bad
-		 * but we must handle this cleanly. Unfortunately, the kernel does
-		 * not provide a mechanism to block migration (while the context is loaded).
-		 *
-		 * We need to release the resource on the ORIGINAL cpu.
-		 */
-		if (is_system && ctx->ctx_cpu != smp_processor_id()) {
-
-			DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
-			/*
-			 * keep context protected but unmask interrupt for IPI
-			 */
-			local_irq_restore(flags);
-
-			pfm_syswide_cleanup_other_cpu(ctx);
-
-			/*
-			 * restore interrupt masking
-			 */
-			local_irq_save(flags);
-
-			/*
-			 * context is unloaded at this point
-			 */
-		} else
-#endif /* CONFIG_SMP */
-		{
-
-			DPRINT(("forcing unload\n"));
-			/*
-		 	* stop and unload, returning with state UNLOADED
-		 	* and session unreserved.
-		 	*/
-			pfm_context_unload(ctx, NULL, 0, regs);
-
-			DPRINT(("ctx_state=%d\n", ctx->ctx_state));
-		}
-	}
-
-	/*
-	 * remove virtual mapping, if any, for the calling task.
-	 * cannot reset ctx field until last user is calling close().
-	 *
-	 * ctx_smpl_vaddr must never be cleared because it is needed
-	 * by every task with access to the context
-	 *
-	 * When called from do_exit(), the mm context is gone already, therefore
-	 * mm is NULL, i.e., the VMA is already gone  and we do not have to
-	 * do anything here
-	 */
-	if (ctx->ctx_smpl_vaddr && current->mm) {
-		smpl_buf_vaddr = ctx->ctx_smpl_vaddr;
-		smpl_buf_size  = ctx->ctx_smpl_size;
-	}
-
-	UNPROTECT_CTX(ctx, flags);
-
-	/*
-	 * if there was a mapping, then we systematically remove it
-	 * at this point. Cannot be done inside critical section
-	 * because some VM function reenables interrupts.
-	 *
-	 */
-	if (smpl_buf_vaddr) pfm_remove_smpl_mapping(current, smpl_buf_vaddr, smpl_buf_size);
-
-	return 0;
-}
-/*
- * called either on explicit close() or from exit_files(). 
- * Only the LAST user of the file gets to this point, i.e., it is
- * called only ONCE.
- *
- * IMPORTANT: we get called ONLY when the refcnt on the file gets to zero 
- * (fput()),i.e, last task to access the file. Nobody else can access the 
- * file at this point.
- *
- * When called from exit_files(), the VMA has been freed because exit_mm()
- * is executed before exit_files().
- *
- * When called from exit_files(), the current task is not yet ZOMBIE but we
- * flush the PMU state to the context. 
- */
-static int
-pfm_close(struct inode *inode, struct file *filp)
-{
-	pfm_context_t *ctx;
-	struct task_struct *task;
-	struct pt_regs *regs;
-  	DECLARE_WAITQUEUE(wait, current);
-	unsigned long flags;
-	unsigned long smpl_buf_size = 0UL;
-	void *smpl_buf_addr = NULL;
-	int free_possible = 1;
-	int state, is_system;
-
-	DPRINT(("pfm_close called private=%p\n", filp->private_data));
-
-	if (PFM_IS_FILE(filp) == 0) {
-		DPRINT(("bad magic\n"));
-		return -EBADF;
-	}
-	
-	ctx = (pfm_context_t *)filp->private_data;
-	if (ctx == NULL) {
-		printk(KERN_ERR "perfmon: pfm_close: NULL ctx [%d]\n", task_pid_nr(current));
-		return -EBADF;
-	}
-
-	PROTECT_CTX(ctx, flags);
-
-	state     = ctx->ctx_state;
-	is_system = ctx->ctx_fl_system;
-
-	task = PFM_CTX_TASK(ctx);
-	regs = task_pt_regs(task);
-
-	DPRINT(("ctx_state=%d is_current=%d\n", 
-		state,
-		task == current ? 1 : 0));
-
-	/*
-	 * if task == current, then pfm_flush() unloaded the context
-	 */
-	if (state == PFM_CTX_UNLOADED) goto doit;
-
-	/*
-	 * context is loaded/masked and task != current, we need to
-	 * either force an unload or go zombie
-	 */
-
-	/*
-	 * The task is currently blocked or will block after an overflow.
-	 * we must force it to wakeup to get out of the
-	 * MASKED state and transition to the unloaded state by itself.
-	 *
-	 * This situation is only possible for per-task mode
-	 */
-	if (state == PFM_CTX_MASKED && CTX_OVFL_NOBLOCK(ctx) == 0) {
-
-		/*
-		 * set a "partial" zombie state to be checked
-		 * upon return from down() in pfm_handle_work().
-		 *
-		 * We cannot use the ZOMBIE state, because it is checked
-		 * by pfm_load_regs() which is called upon wakeup from down().
-		 * In such case, it would free the context and then we would
-		 * return to pfm_handle_work() which would access the
-		 * stale context. Instead, we set a flag invisible to pfm_load_regs()
-		 * but visible to pfm_handle_work().
-		 *
-		 * For some window of time, we have a zombie context with
-		 * ctx_state = MASKED  and not ZOMBIE
-		 */
-		ctx->ctx_fl_going_zombie = 1;
-
-		/*
-		 * force task to wake up from MASKED state
-		 */
-		complete(&ctx->ctx_restart_done);
-
-		DPRINT(("waking up ctx_state=%d\n", state));
-
-		/*
-		 * put ourself to sleep waiting for the other
-		 * task to report completion
-		 *
-		 * the context is protected by mutex, therefore there
-		 * is no risk of being notified of completion before
-		 * begin actually on the waitq.
-		 */
-  		set_current_state(TASK_INTERRUPTIBLE);
-  		add_wait_queue(&ctx->ctx_zombieq, &wait);
-
-		UNPROTECT_CTX(ctx, flags);
-
-		/*
-		 * XXX: check for signals :
-		 * 	- ok for explicit close
-		 * 	- not ok when coming from exit_files()
-		 */
-      		schedule();
-
-
-		PROTECT_CTX(ctx, flags);
-
-
-		remove_wait_queue(&ctx->ctx_zombieq, &wait);
-  		set_current_state(TASK_RUNNING);
-
-		/*
-		 * context is unloaded at this point
-		 */
-		DPRINT(("after zombie wakeup ctx_state=%d for\n", state));
-	}
-	else if (task != current) {
-#ifdef CONFIG_SMP
-		/*
-	 	 * switch context to zombie state
-	 	 */
-		ctx->ctx_state = PFM_CTX_ZOMBIE;
-
-		DPRINT(("zombie ctx for [%d]\n", task_pid_nr(task)));
-		/*
-		 * cannot free the context on the spot. deferred until
-		 * the task notices the ZOMBIE state
-		 */
-		free_possible = 0;
-#else
-		pfm_context_unload(ctx, NULL, 0, regs);
-#endif
-	}
-
-doit:
-	/* reload state, may have changed during  opening of critical section */
-	state = ctx->ctx_state;
-
-	/*
-	 * the context is still attached to a task (possibly current)
-	 * we cannot destroy it right now
-	 */
-
-	/*
-	 * we must free the sampling buffer right here because
-	 * we cannot rely on it being cleaned up later by the
-	 * monitored task. It is not possible to free vmalloc'ed
-	 * memory in pfm_load_regs(). Instead, we remove the buffer
-	 * now. should there be subsequent PMU overflow originally
-	 * meant for sampling, the will be converted to spurious
-	 * and that's fine because the monitoring tools is gone anyway.
-	 */
-	if (ctx->ctx_smpl_hdr) {
-		smpl_buf_addr = ctx->ctx_smpl_hdr;
-		smpl_buf_size = ctx->ctx_smpl_size;
-		/* no more sampling */
-		ctx->ctx_smpl_hdr = NULL;
-		ctx->ctx_fl_is_sampling = 0;
-	}
-
-	DPRINT(("ctx_state=%d free_possible=%d addr=%p size=%lu\n",
-		state,
-		free_possible,
-		smpl_buf_addr,
-		smpl_buf_size));
-
-	if (smpl_buf_addr) pfm_exit_smpl_buffer(ctx->ctx_buf_fmt);
-
-	/*
-	 * UNLOADED that the session has already been unreserved.
-	 */
-	if (state == PFM_CTX_ZOMBIE) {
-		pfm_unreserve_session(ctx, ctx->ctx_fl_system , ctx->ctx_cpu);
-	}
-
-	/*
-	 * disconnect file descriptor from context must be done
-	 * before we unlock.
-	 */
-	filp->private_data = NULL;
-
-	/*
-	 * if we free on the spot, the context is now completely unreachable
-	 * from the callers side. The monitored task side is also cut, so we
-	 * can freely cut.
-	 *
-	 * If we have a deferred free, only the caller side is disconnected.
-	 */
-	UNPROTECT_CTX(ctx, flags);
-
-	/*
-	 * All memory free operations (especially for vmalloc'ed memory)
-	 * MUST be done with interrupts ENABLED.
-	 */
-	if (smpl_buf_addr)  pfm_rvfree(smpl_buf_addr, smpl_buf_size);
-
-	/*
-	 * return the memory used by the context
-	 */
-	if (free_possible) pfm_context_free(ctx);
-
-	return 0;
-}
-
-static int
-pfm_no_open(struct inode *irrelevant, struct file *dontcare)
-{
-	DPRINT(("pfm_no_open called\n"));
-	return -ENXIO;
-}
-
-
-
-static const struct file_operations pfm_file_ops = {
-	.llseek   = no_llseek,
-	.read     = pfm_read,
-	.write    = pfm_write,
-	.poll     = pfm_poll,
-	.ioctl    = pfm_ioctl,
-	.open     = pfm_no_open,	/* special open code to disallow open via /proc */
-	.fasync   = pfm_fasync,
-	.release  = pfm_close,
-	.flush	  = pfm_flush
-};
-
-static int
-pfmfs_delete_dentry(struct dentry *dentry)
-{
-	return 1;
-}
-
-static const struct dentry_operations pfmfs_dentry_operations = {
-	.d_delete = pfmfs_delete_dentry,
-};
-
-
-static struct file *
-pfm_alloc_file(pfm_context_t *ctx)
-{
-	struct file *file;
-	struct inode *inode;
-	struct dentry *dentry;
-	char name[32];
-	struct qstr this;
-
-	/*
-	 * allocate a new inode
-	 */
-	inode = new_inode(pfmfs_mnt->mnt_sb);
-	if (!inode)
-		return ERR_PTR(-ENOMEM);
-
-	DPRINT(("new inode ino=%ld @%p\n", inode->i_ino, inode));
-
-	inode->i_mode = S_IFCHR|S_IRUGO;
-	inode->i_uid  = current_fsuid();
-	inode->i_gid  = current_fsgid();
-
-	sprintf(name, "[%lu]", inode->i_ino);
-	this.name = name;
-	this.len  = strlen(name);
-	this.hash = inode->i_ino;
-
-	/*
-	 * allocate a new dcache entry
-	 */
-	dentry = d_alloc(pfmfs_mnt->mnt_sb->s_root, &this);
-	if (!dentry) {
-		iput(inode);
-		return ERR_PTR(-ENOMEM);
-	}
-
-	dentry->d_op = &pfmfs_dentry_operations;
-	d_add(dentry, inode);
-
-	file = alloc_file(pfmfs_mnt, dentry, FMODE_READ, &pfm_file_ops);
-	if (!file) {
-		dput(dentry);
-		return ERR_PTR(-ENFILE);
-	}
-
-	file->f_flags = O_RDONLY;
-	file->private_data = ctx;
-
-	return file;
-}
-
-static int
-pfm_remap_buffer(struct vm_area_struct *vma, unsigned long buf, unsigned long addr, unsigned long size)
-{
-	DPRINT(("CPU%d buf=0x%lx addr=0x%lx size=%ld\n", smp_processor_id(), buf, addr, size));
-
-	while (size > 0) {
-		unsigned long pfn = ia64_tpa(buf) >> PAGE_SHIFT;
-
-
-		if (remap_pfn_range(vma, addr, pfn, PAGE_SIZE, PAGE_READONLY))
-			return -ENOMEM;
-
-		addr  += PAGE_SIZE;
-		buf   += PAGE_SIZE;
-		size  -= PAGE_SIZE;
-	}
-	return 0;
-}
-
-/*
- * allocate a sampling buffer and remaps it into the user address space of the task
- */
-static int
-pfm_smpl_buffer_alloc(struct task_struct *task, struct file *filp, pfm_context_t *ctx, unsigned long rsize, void **user_vaddr)
-{
-	struct mm_struct *mm = task->mm;
-	struct vm_area_struct *vma = NULL;
-	unsigned long size;
-	void *smpl_buf;
-
-
-	/*
-	 * the fixed header + requested size and align to page boundary
-	 */
-	size = PAGE_ALIGN(rsize);
-
-	DPRINT(("sampling buffer rsize=%lu size=%lu bytes\n", rsize, size));
-
-	/*
-	 * check requested size to avoid Denial-of-service attacks
-	 * XXX: may have to refine this test
-	 * Check against address space limit.
-	 *
-	 * if ((mm->total_vm << PAGE_SHIFT) + len> task->rlim[RLIMIT_AS].rlim_cur)
-	 * 	return -ENOMEM;
-	 */
-	if (size > task->signal->rlim[RLIMIT_MEMLOCK].rlim_cur)
-		return -ENOMEM;
-
-	/*
-	 * We do the easy to undo allocations first.
- 	 *
-	 * pfm_rvmalloc(), clears the buffer, so there is no leak
-	 */
-	smpl_buf = pfm_rvmalloc(size);
-	if (smpl_buf == NULL) {
-		DPRINT(("Can't allocate sampling buffer\n"));
-		return -ENOMEM;
-	}
-
-	DPRINT(("smpl_buf @%p\n", smpl_buf));
-
-	/* allocate vma */
-	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
-	if (!vma) {
-		DPRINT(("Cannot allocate vma\n"));
-		goto error_kmem;
-	}
-
-	/*
-	 * partially initialize the vma for the sampling buffer
-	 */
-	vma->vm_mm	     = mm;
-	vma->vm_file	     = filp;
-	vma->vm_flags	     = VM_READ| VM_MAYREAD |VM_RESERVED;
-	vma->vm_page_prot    = PAGE_READONLY; /* XXX may need to change */
-
-	/*
-	 * Now we have everything we need and we can initialize
-	 * and connect all the data structures
-	 */
-
-	ctx->ctx_smpl_hdr   = smpl_buf;
-	ctx->ctx_smpl_size  = size; /* aligned size */
-
-	/*
-	 * Let's do the difficult operations next.
-	 *
-	 * now we atomically find some area in the address space and
-	 * remap the buffer in it.
-	 */
-	down_write(&task->mm->mmap_sem);
-
-	/* find some free area in address space, must have mmap sem held */
-	vma->vm_start = pfm_get_unmapped_area(NULL, 0, size, 0, MAP_PRIVATE|MAP_ANONYMOUS, 0);
-	if (vma->vm_start == 0UL) {
-		DPRINT(("Cannot find unmapped area for size %ld\n", size));
-		up_write(&task->mm->mmap_sem);
-		goto error;
-	}
-	vma->vm_end = vma->vm_start + size;
-	vma->vm_pgoff = vma->vm_start >> PAGE_SHIFT;
-
-	DPRINT(("aligned size=%ld, hdr=%p mapped @0x%lx\n", size, ctx->ctx_smpl_hdr, vma->vm_start));
-
-	/* can only be applied to current task, need to have the mm semaphore held when called */
-	if (pfm_remap_buffer(vma, (unsigned long)smpl_buf, vma->vm_start, size)) {
-		DPRINT(("Can't remap buffer\n"));
-		up_write(&task->mm->mmap_sem);
-		goto error;
-	}
-
-	get_file(filp);
-
-	/*
-	 * now insert the vma in the vm list for the process, must be
-	 * done with mmap lock held
-	 */
-	insert_vm_struct(mm, vma);
-
-	mm->total_vm  += size >> PAGE_SHIFT;
-	vm_stat_account(vma->vm_mm, vma->vm_flags, vma->vm_file,
-							vma_pages(vma));
-	up_write(&task->mm->mmap_sem);
-
-	/*
-	 * keep track of user level virtual address
-	 */
-	ctx->ctx_smpl_vaddr = (void *)vma->vm_start;
-	*(unsigned long *)user_vaddr = vma->vm_start;
-
-	return 0;
-
-error:
-	kmem_cache_free(vm_area_cachep, vma);
-error_kmem:
-	pfm_rvfree(smpl_buf, size);
-
-	return -ENOMEM;
-}
-
-/*
- * XXX: do something better here
- */
-static int
-pfm_bad_permissions(struct task_struct *task)
-{
-	const struct cred *tcred;
-	uid_t uid = current_uid();
-	gid_t gid = current_gid();
-	int ret;
-
-	rcu_read_lock();
-	tcred = __task_cred(task);
-
-	/* inspired by ptrace_attach() */
-	DPRINT(("cur: uid=%d gid=%d task: euid=%d suid=%d uid=%d egid=%d sgid=%d\n",
-		uid,
-		gid,
-		tcred->euid,
-		tcred->suid,
-		tcred->uid,
-		tcred->egid,
-		tcred->sgid));
-
-	ret = ((uid != tcred->euid)
-	       || (uid != tcred->suid)
-	       || (uid != tcred->uid)
-	       || (gid != tcred->egid)
-	       || (gid != tcred->sgid)
-	       || (gid != tcred->gid)) && !capable(CAP_SYS_PTRACE);
-
-	rcu_read_unlock();
-	return ret;
-}
-
-static int
-pfarg_is_sane(struct task_struct *task, pfarg_context_t *pfx)
-{
-	int ctx_flags;
-
-	/* valid signal */
-
-	ctx_flags = pfx->ctx_flags;
-
-	if (ctx_flags & PFM_FL_SYSTEM_WIDE) {
-
-		/*
-		 * cannot block in this mode
-		 */
-		if (ctx_flags & PFM_FL_NOTIFY_BLOCK) {
-			DPRINT(("cannot use blocking mode when in system wide monitoring\n"));
-			return -EINVAL;
-		}
-	} else {
-	}
-	/* probably more to add here */
-
-	return 0;
-}
-
-static int
-pfm_setup_buffer_fmt(struct task_struct *task, struct file *filp, pfm_context_t *ctx, unsigned int ctx_flags,
-		     unsigned int cpu, pfarg_context_t *arg)
-{
-	pfm_buffer_fmt_t *fmt = NULL;
-	unsigned long size = 0UL;
-	void *uaddr = NULL;
-	void *fmt_arg = NULL;
-	int ret = 0;
-#define PFM_CTXARG_BUF_ARG(a)	(pfm_buffer_fmt_t *)(a+1)
-
-	/* invoke and lock buffer format, if found */
-	fmt = pfm_find_buffer_fmt(arg->ctx_smpl_buf_id);
-	if (fmt == NULL) {
-		DPRINT(("[%d] cannot find buffer format\n", task_pid_nr(task)));
-		return -EINVAL;
-	}
-
-	/*
-	 * buffer argument MUST be contiguous to pfarg_context_t
-	 */
-	if (fmt->fmt_arg_size) fmt_arg = PFM_CTXARG_BUF_ARG(arg);
-
-	ret = pfm_buf_fmt_validate(fmt, task, ctx_flags, cpu, fmt_arg);
-
-	DPRINT(("[%d] after validate(0x%x,%d,%p)=%d\n", task_pid_nr(task), ctx_flags, cpu, fmt_arg, ret));
-
-	if (ret) goto error;
-
-	/* link buffer format and context */
-	ctx->ctx_buf_fmt = fmt;
-	ctx->ctx_fl_is_sampling = 1; /* assume record() is defined */
-
-	/*
-	 * check if buffer format wants to use perfmon buffer allocation/mapping service
-	 */
-	ret = pfm_buf_fmt_getsize(fmt, task, ctx_flags, cpu, fmt_arg, &size);
-	if (ret) goto error;
-
-	if (size) {
-		/*
-		 * buffer is always remapped into the caller's address space
-		 */
-		ret = pfm_smpl_buffer_alloc(current, filp, ctx, size, &uaddr);
-		if (ret) goto error;
-
-		/* keep track of user address of buffer */
-		arg->ctx_smpl_vaddr = uaddr;
-	}
-	ret = pfm_buf_fmt_init(fmt, task, ctx->ctx_smpl_hdr, ctx_flags, cpu, fmt_arg);
-
-error:
-	return ret;
-}
-
-static void
-pfm_reset_pmu_state(pfm_context_t *ctx)
-{
-	int i;
-
-	/*
-	 * install reset values for PMC.
-	 */
-	for (i=1; PMC_IS_LAST(i) == 0; i++) {
-		if (PMC_IS_IMPL(i) == 0) continue;
-		ctx->ctx_pmcs[i] = PMC_DFL_VAL(i);
-		DPRINT(("pmc[%d]=0x%lx\n", i, ctx->ctx_pmcs[i]));
-	}
-	/*
-	 * PMD registers are set to 0UL when the context in memset()
-	 */
-
-	/*
-	 * On context switched restore, we must restore ALL pmc and ALL pmd even
-	 * when they are not actively used by the task. In UP, the incoming process
-	 * may otherwise pick up left over PMC, PMD state from the previous process.
-	 * As opposed to PMD, stale PMC can cause harm to the incoming
-	 * process because they may change what is being measured.
-	 * Therefore, we must systematically reinstall the entire
-	 * PMC state. In SMP, the same thing is possible on the
-	 * same CPU but also on between 2 CPUs.
-	 *
-	 * The problem with PMD is information leaking especially
-	 * to user level when psr.sp=0
-	 *
-	 * There is unfortunately no easy way to avoid this problem
-	 * on either UP or SMP. This definitively slows down the
-	 * pfm_load_regs() function.
-	 */
-
-	 /*
-	  * bitmask of all PMCs accessible to this context
-	  *
-	  * PMC0 is treated differently.
-	  */
-	ctx->ctx_all_pmcs[0] = pmu_conf->impl_pmcs[0] & ~0x1;
-
-	/*
-	 * bitmask of all PMDs that are accessible to this context
-	 */
-	ctx->ctx_all_pmds[0] = pmu_conf->impl_pmds[0];
-
-	DPRINT(("<%d> all_pmcs=0x%lx all_pmds=0x%lx\n", ctx->ctx_fd, ctx->ctx_all_pmcs[0],ctx->ctx_all_pmds[0]));
-
-	/*
-	 * useful in case of re-enable after disable
-	 */
-	ctx->ctx_used_ibrs[0] = 0UL;
-	ctx->ctx_used_dbrs[0] = 0UL;
-}
-
-static int
-pfm_ctx_getsize(void *arg, size_t *sz)
-{
-	pfarg_context_t *req = (pfarg_context_t *)arg;
-	pfm_buffer_fmt_t *fmt;
-
-	*sz = 0;
-
-	if (!pfm_uuid_cmp(req->ctx_smpl_buf_id, pfm_null_uuid)) return 0;
-
-	fmt = pfm_find_buffer_fmt(req->ctx_smpl_buf_id);
-	if (fmt == NULL) {
-		DPRINT(("cannot find buffer format\n"));
-		return -EINVAL;
-	}
-	/* get just enough to copy in user parameters */
-	*sz = fmt->fmt_arg_size;
-	DPRINT(("arg_size=%lu\n", *sz));
-
-	return 0;
-}
-
-
-
-/*
- * cannot attach if :
- * 	- kernel task
- * 	- task not owned by caller
- * 	- task incompatible with context mode
- */
-static int
-pfm_task_incompatible(pfm_context_t *ctx, struct task_struct *task)
-{
-	/*
-	 * no kernel task or task not owner by caller
-	 */
-	if (task->mm == NULL) {
-		DPRINT(("task [%d] has not memory context (kernel thread)\n", task_pid_nr(task)));
-		return -EPERM;
-	}
-	if (pfm_bad_permissions(task)) {
-		DPRINT(("no permission to attach to  [%d]\n", task_pid_nr(task)));
-		return -EPERM;
-	}
-	/*
-	 * cannot block in self-monitoring mode
-	 */
-	if (CTX_OVFL_NOBLOCK(ctx) == 0 && task == current) {
-		DPRINT(("cannot load a blocking context on self for [%d]\n", task_pid_nr(task)));
-		return -EINVAL;
-	}
-
-	if (task->exit_state == EXIT_ZOMBIE) {
-		DPRINT(("cannot attach to  zombie task [%d]\n", task_pid_nr(task)));
-		return -EBUSY;
-	}
-
-	/*
-	 * always ok for self
-	 */
-	if (task == current) return 0;
-
-	if (!task_is_stopped_or_traced(task)) {
-		DPRINT(("cannot attach to non-stopped task [%d] state=%ld\n", task_pid_nr(task), task->state));
-		return -EBUSY;
-	}
-	/*
-	 * make sure the task is off any CPU
-	 */
-	wait_task_inactive(task, 0);
-
-	/* more to come... */
-
-	return 0;
-}
-
-static int
-pfm_get_task(pfm_context_t *ctx, pid_t pid, struct task_struct **task)
-{
-	struct task_struct *p = current;
-	int ret;
-
-	/* XXX: need to add more checks here */
-	if (pid < 2) return -EPERM;
-
-	if (pid != task_pid_vnr(current)) {
-
-		read_lock(&tasklist_lock);
-
-		p = find_task_by_vpid(pid);
-
-		/* make sure task cannot go away while we operate on it */
-		if (p) get_task_struct(p);
-
-		read_unlock(&tasklist_lock);
-
-		if (p == NULL) return -ESRCH;
-	}
-
-	ret = pfm_task_incompatible(ctx, p);
-	if (ret == 0) {
-		*task = p;
-	} else if (p != current) {
-		pfm_put_task(p);
-	}
-	return ret;
-}
-
-
-
-static int
-pfm_context_create(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	pfarg_context_t *req = (pfarg_context_t *)arg;
-	struct file *filp;
-	struct path path;
-	int ctx_flags;
-	int fd;
-	int ret;
-
-	/* let's check the arguments first */
-	ret = pfarg_is_sane(current, req);
-	if (ret < 0)
-		return ret;
-
-	ctx_flags = req->ctx_flags;
-
-	ret = -ENOMEM;
-
-	fd = get_unused_fd();
-	if (fd < 0)
-		return fd;
-
-	ctx = pfm_context_alloc(ctx_flags);
-	if (!ctx)
-		goto error;
-
-	filp = pfm_alloc_file(ctx);
-	if (IS_ERR(filp)) {
-		ret = PTR_ERR(filp);
-		goto error_file;
-	}
-
-	req->ctx_fd = ctx->ctx_fd = fd;
-
-	/*
-	 * does the user want to sample?
-	 */
-	if (pfm_uuid_cmp(req->ctx_smpl_buf_id, pfm_null_uuid)) {
-		ret = pfm_setup_buffer_fmt(current, filp, ctx, ctx_flags, 0, req);
-		if (ret)
-			goto buffer_error;
-	}
-
-	DPRINT(("ctx=%p flags=0x%x system=%d notify_block=%d excl_idle=%d no_msg=%d ctx_fd=%d \n",
-		ctx,
-		ctx_flags,
-		ctx->ctx_fl_system,
-		ctx->ctx_fl_block,
-		ctx->ctx_fl_excl_idle,
-		ctx->ctx_fl_no_msg,
-		ctx->ctx_fd));
-
-	/*
-	 * initialize soft PMU state
-	 */
-	pfm_reset_pmu_state(ctx);
-
-	fd_install(fd, filp);
-
-	return 0;
-
-buffer_error:
-	path = filp->f_path;
-	put_filp(filp);
-	path_put(&path);
-
-	if (ctx->ctx_buf_fmt) {
-		pfm_buf_fmt_exit(ctx->ctx_buf_fmt, current, NULL, regs);
-	}
-error_file:
-	pfm_context_free(ctx);
-
-error:
-	put_unused_fd(fd);
-	return ret;
-}
-
-static inline unsigned long
-pfm_new_counter_value (pfm_counter_t *reg, int is_long_reset)
-{
-	unsigned long val = is_long_reset ? reg->long_reset : reg->short_reset;
-	unsigned long new_seed, old_seed = reg->seed, mask = reg->mask;
-	extern unsigned long carta_random32 (unsigned long seed);
-
-	if (reg->flags & PFM_REGFL_RANDOM) {
-		new_seed = carta_random32(old_seed);
-		val -= (old_seed & mask);	/* counter values are negative numbers! */
-		if ((mask >> 32) != 0)
-			/* construct a full 64-bit random value: */
-			new_seed |= carta_random32(old_seed >> 32) << 32;
-		reg->seed = new_seed;
-	}
-	reg->lval = val;
-	return val;
-}
-
-static void
-pfm_reset_regs_masked(pfm_context_t *ctx, unsigned long *ovfl_regs, int is_long_reset)
-{
-	unsigned long mask = ovfl_regs[0];
-	unsigned long reset_others = 0UL;
-	unsigned long val;
-	int i;
-
-	/*
-	 * now restore reset value on sampling overflowed counters
-	 */
-	mask >>= PMU_FIRST_COUNTER;
-	for(i = PMU_FIRST_COUNTER; mask; i++, mask >>= 1) {
-
-		if ((mask & 0x1UL) == 0UL) continue;
-
-		ctx->ctx_pmds[i].val = val = pfm_new_counter_value(ctx->ctx_pmds+ i, is_long_reset);
-		reset_others        |= ctx->ctx_pmds[i].reset_pmds[0];
-
-		DPRINT_ovfl((" %s reset ctx_pmds[%d]=%lx\n", is_long_reset ? "long" : "short", i, val));
-	}
-
-	/*
-	 * Now take care of resetting the other registers
-	 */
-	for(i = 0; reset_others; i++, reset_others >>= 1) {
-
-		if ((reset_others & 0x1) == 0) continue;
-
-		ctx->ctx_pmds[i].val = val = pfm_new_counter_value(ctx->ctx_pmds + i, is_long_reset);
-
-		DPRINT_ovfl(("%s reset_others pmd[%d]=%lx\n",
-			  is_long_reset ? "long" : "short", i, val));
-	}
-}
-
-static void
-pfm_reset_regs(pfm_context_t *ctx, unsigned long *ovfl_regs, int is_long_reset)
-{
-	unsigned long mask = ovfl_regs[0];
-	unsigned long reset_others = 0UL;
-	unsigned long val;
-	int i;
-
-	DPRINT_ovfl(("ovfl_regs=0x%lx is_long_reset=%d\n", ovfl_regs[0], is_long_reset));
-
-	if (ctx->ctx_state == PFM_CTX_MASKED) {
-		pfm_reset_regs_masked(ctx, ovfl_regs, is_long_reset);
-		return;
-	}
-
-	/*
-	 * now restore reset value on sampling overflowed counters
-	 */
-	mask >>= PMU_FIRST_COUNTER;
-	for(i = PMU_FIRST_COUNTER; mask; i++, mask >>= 1) {
-
-		if ((mask & 0x1UL) == 0UL) continue;
-
-		val           = pfm_new_counter_value(ctx->ctx_pmds+ i, is_long_reset);
-		reset_others |= ctx->ctx_pmds[i].reset_pmds[0];
-
-		DPRINT_ovfl((" %s reset ctx_pmds[%d]=%lx\n", is_long_reset ? "long" : "short", i, val));
-
-		pfm_write_soft_counter(ctx, i, val);
-	}
-
-	/*
-	 * Now take care of resetting the other registers
-	 */
-	for(i = 0; reset_others; i++, reset_others >>= 1) {
-
-		if ((reset_others & 0x1) == 0) continue;
-
-		val = pfm_new_counter_value(ctx->ctx_pmds + i, is_long_reset);
-
-		if (PMD_IS_COUNTING(i)) {
-			pfm_write_soft_counter(ctx, i, val);
-		} else {
-			ia64_set_pmd(i, val);
-		}
-		DPRINT_ovfl(("%s reset_others pmd[%d]=%lx\n",
-			  is_long_reset ? "long" : "short", i, val));
-	}
-	ia64_srlz_d();
-}
-
-static int
-pfm_write_pmcs(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	struct task_struct *task;
-	pfarg_reg_t *req = (pfarg_reg_t *)arg;
-	unsigned long value, pmc_pm;
-	unsigned long smpl_pmds, reset_pmds, impl_pmds;
-	unsigned int cnum, reg_flags, flags, pmc_type;
-	int i, can_access_pmu = 0, is_loaded, is_system, expert_mode;
-	int is_monitor, is_counting, state;
-	int ret = -EINVAL;
-	pfm_reg_check_t	wr_func;
-#define PFM_CHECK_PMC_PM(x, y, z) ((x)->ctx_fl_system ^ PMC_PM(y, z))
-
-	state     = ctx->ctx_state;
-	is_loaded = state == PFM_CTX_LOADED ? 1 : 0;
-	is_system = ctx->ctx_fl_system;
-	task      = ctx->ctx_task;
-	impl_pmds = pmu_conf->impl_pmds[0];
-
-	if (state == PFM_CTX_ZOMBIE) return -EINVAL;
-
-	if (is_loaded) {
-		/*
-		 * In system wide and when the context is loaded, access can only happen
-		 * when the caller is running on the CPU being monitored by the session.
-		 * It does not have to be the owner (ctx_task) of the context per se.
-		 */
-		if (is_system && ctx->ctx_cpu != smp_processor_id()) {
-			DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
-			return -EBUSY;
-		}
-		can_access_pmu = GET_PMU_OWNER() == task || is_system ? 1 : 0;
-	}
-	expert_mode = pfm_sysctl.expert_mode; 
-
-	for (i = 0; i < count; i++, req++) {
-
-		cnum       = req->reg_num;
-		reg_flags  = req->reg_flags;
-		value      = req->reg_value;
-		smpl_pmds  = req->reg_smpl_pmds[0];
-		reset_pmds = req->reg_reset_pmds[0];
-		flags      = 0;
-
-
-		if (cnum >= PMU_MAX_PMCS) {
-			DPRINT(("pmc%u is invalid\n", cnum));
-			goto error;
-		}
-
-		pmc_type   = pmu_conf->pmc_desc[cnum].type;
-		pmc_pm     = (value >> pmu_conf->pmc_desc[cnum].pm_pos) & 0x1;
-		is_counting = (pmc_type & PFM_REG_COUNTING) == PFM_REG_COUNTING ? 1 : 0;
-		is_monitor  = (pmc_type & PFM_REG_MONITOR) == PFM_REG_MONITOR ? 1 : 0;
-
-		/*
-		 * we reject all non implemented PMC as well
-		 * as attempts to modify PMC[0-3] which are used
-		 * as status registers by the PMU
-		 */
-		if ((pmc_type & PFM_REG_IMPL) == 0 || (pmc_type & PFM_REG_CONTROL) == PFM_REG_CONTROL) {
-			DPRINT(("pmc%u is unimplemented or no-access pmc_type=%x\n", cnum, pmc_type));
-			goto error;
-		}
-		wr_func = pmu_conf->pmc_desc[cnum].write_check;
-		/*
-		 * If the PMC is a monitor, then if the value is not the default:
-		 * 	- system-wide session: PMCx.pm=1 (privileged monitor)
-		 * 	- per-task           : PMCx.pm=0 (user monitor)
-		 */
-		if (is_monitor && value != PMC_DFL_VAL(cnum) && is_system ^ pmc_pm) {
-			DPRINT(("pmc%u pmc_pm=%lu is_system=%d\n",
-				cnum,
-				pmc_pm,
-				is_system));
-			goto error;
-		}
-
-		if (is_counting) {
-			/*
-		 	 * enforce generation of overflow interrupt. Necessary on all
-		 	 * CPUs.
-		 	 */
-			value |= 1 << PMU_PMC_OI;
-
-			if (reg_flags & PFM_REGFL_OVFL_NOTIFY) {
-				flags |= PFM_REGFL_OVFL_NOTIFY;
-			}
-
-			if (reg_flags & PFM_REGFL_RANDOM) flags |= PFM_REGFL_RANDOM;
-
-			/* verify validity of smpl_pmds */
-			if ((smpl_pmds & impl_pmds) != smpl_pmds) {
-				DPRINT(("invalid smpl_pmds 0x%lx for pmc%u\n", smpl_pmds, cnum));
-				goto error;
-			}
-
-			/* verify validity of reset_pmds */
-			if ((reset_pmds & impl_pmds) != reset_pmds) {
-				DPRINT(("invalid reset_pmds 0x%lx for pmc%u\n", reset_pmds, cnum));
-				goto error;
-			}
-		} else {
-			if (reg_flags & (PFM_REGFL_OVFL_NOTIFY|PFM_REGFL_RANDOM)) {
-				DPRINT(("cannot set ovfl_notify or random on pmc%u\n", cnum));
-				goto error;
-			}
-			/* eventid on non-counting monitors are ignored */
-		}
-
-		/*
-		 * execute write checker, if any
-		 */
-		if (likely(expert_mode == 0 && wr_func)) {
-			ret = (*wr_func)(task, ctx, cnum, &value, regs);
-			if (ret) goto error;
-			ret = -EINVAL;
-		}
-
-		/*
-		 * no error on this register
-		 */
-		PFM_REG_RETFLAG_SET(req->reg_flags, 0);
-
-		/*
-		 * Now we commit the changes to the software state
-		 */
-
-		/*
-		 * update overflow information
-		 */
-		if (is_counting) {
-			/*
-		 	 * full flag update each time a register is programmed
-		 	 */
-			ctx->ctx_pmds[cnum].flags = flags;
-
-			ctx->ctx_pmds[cnum].reset_pmds[0] = reset_pmds;
-			ctx->ctx_pmds[cnum].smpl_pmds[0]  = smpl_pmds;
-			ctx->ctx_pmds[cnum].eventid       = req->reg_smpl_eventid;
-
-			/*
-			 * Mark all PMDS to be accessed as used.
-			 *
-			 * We do not keep track of PMC because we have to
-			 * systematically restore ALL of them.
-			 *
-			 * We do not update the used_monitors mask, because
-			 * if we have not programmed them, then will be in
-			 * a quiescent state, therefore we will not need to
-			 * mask/restore then when context is MASKED.
-			 */
-			CTX_USED_PMD(ctx, reset_pmds);
-			CTX_USED_PMD(ctx, smpl_pmds);
-			/*
-		 	 * make sure we do not try to reset on
-		 	 * restart because we have established new values
-		 	 */
-			if (state == PFM_CTX_MASKED) ctx->ctx_ovfl_regs[0] &= ~1UL << cnum;
-		}
-		/*
-		 * Needed in case the user does not initialize the equivalent
-		 * PMD. Clearing is done indirectly via pfm_reset_pmu_state() so there is no
-		 * possible leak here.
-		 */
-		CTX_USED_PMD(ctx, pmu_conf->pmc_desc[cnum].dep_pmd[0]);
-
-		/*
-		 * keep track of the monitor PMC that we are using.
-		 * we save the value of the pmc in ctx_pmcs[] and if
-		 * the monitoring is not stopped for the context we also
-		 * place it in the saved state area so that it will be
-		 * picked up later by the context switch code.
-		 *
-		 * The value in ctx_pmcs[] can only be changed in pfm_write_pmcs().
-		 *
-		 * The value in th_pmcs[] may be modified on overflow, i.e.,  when
-		 * monitoring needs to be stopped.
-		 */
-		if (is_monitor) CTX_USED_MONITOR(ctx, 1UL << cnum);
-
-		/*
-		 * update context state
-		 */
-		ctx->ctx_pmcs[cnum] = value;
-
-		if (is_loaded) {
-			/*
-			 * write thread state
-			 */
-			if (is_system == 0) ctx->th_pmcs[cnum] = value;
-
-			/*
-			 * write hardware register if we can
-			 */
-			if (can_access_pmu) {
-				ia64_set_pmc(cnum, value);
-			}
-#ifdef CONFIG_SMP
-			else {
-				/*
-				 * per-task SMP only here
-				 *
-			 	 * we are guaranteed that the task is not running on the other CPU,
-			 	 * we indicate that this PMD will need to be reloaded if the task
-			 	 * is rescheduled on the CPU it ran last on.
-			 	 */
-				ctx->ctx_reload_pmcs[0] |= 1UL << cnum;
-			}
-#endif
-		}
-
-		DPRINT(("pmc[%u]=0x%lx ld=%d apmu=%d flags=0x%x all_pmcs=0x%lx used_pmds=0x%lx eventid=%ld smpl_pmds=0x%lx reset_pmds=0x%lx reloads_pmcs=0x%lx used_monitors=0x%lx ovfl_regs=0x%lx\n",
-			  cnum,
-			  value,
-			  is_loaded,
-			  can_access_pmu,
-			  flags,
-			  ctx->ctx_all_pmcs[0],
-			  ctx->ctx_used_pmds[0],
-			  ctx->ctx_pmds[cnum].eventid,
-			  smpl_pmds,
-			  reset_pmds,
-			  ctx->ctx_reload_pmcs[0],
-			  ctx->ctx_used_monitors[0],
-			  ctx->ctx_ovfl_regs[0]));
-	}
-
-	/*
-	 * make sure the changes are visible
-	 */
-	if (can_access_pmu) ia64_srlz_d();
-
-	return 0;
-error:
-	PFM_REG_RETFLAG_SET(req->reg_flags, PFM_REG_RETFL_EINVAL);
-	return ret;
-}
-
-static int
-pfm_write_pmds(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	struct task_struct *task;
-	pfarg_reg_t *req = (pfarg_reg_t *)arg;
-	unsigned long value, hw_value, ovfl_mask;
-	unsigned int cnum;
-	int i, can_access_pmu = 0, state;
-	int is_counting, is_loaded, is_system, expert_mode;
-	int ret = -EINVAL;
-	pfm_reg_check_t wr_func;
-
-
-	state     = ctx->ctx_state;
-	is_loaded = state == PFM_CTX_LOADED ? 1 : 0;
-	is_system = ctx->ctx_fl_system;
-	ovfl_mask = pmu_conf->ovfl_val;
-	task      = ctx->ctx_task;
-
-	if (unlikely(state == PFM_CTX_ZOMBIE)) return -EINVAL;
-
-	/*
-	 * on both UP and SMP, we can only write to the PMC when the task is
-	 * the owner of the local PMU.
-	 */
-	if (likely(is_loaded)) {
-		/*
-		 * In system wide and when the context is loaded, access can only happen
-		 * when the caller is running on the CPU being monitored by the session.
-		 * It does not have to be the owner (ctx_task) of the context per se.
-		 */
-		if (unlikely(is_system && ctx->ctx_cpu != smp_processor_id())) {
-			DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
-			return -EBUSY;
-		}
-		can_access_pmu = GET_PMU_OWNER() == task || is_system ? 1 : 0;
-	}
-	expert_mode = pfm_sysctl.expert_mode; 
-
-	for (i = 0; i < count; i++, req++) {
-
-		cnum  = req->reg_num;
-		value = req->reg_value;
-
-		if (!PMD_IS_IMPL(cnum)) {
-			DPRINT(("pmd[%u] is unimplemented or invalid\n", cnum));
-			goto abort_mission;
-		}
-		is_counting = PMD_IS_COUNTING(cnum);
-		wr_func     = pmu_conf->pmd_desc[cnum].write_check;
-
-		/*
-		 * execute write checker, if any
-		 */
-		if (unlikely(expert_mode == 0 && wr_func)) {
-			unsigned long v = value;
-
-			ret = (*wr_func)(task, ctx, cnum, &v, regs);
-			if (ret) goto abort_mission;
-
-			value = v;
-			ret   = -EINVAL;
-		}
-
-		/*
-		 * no error on this register
-		 */
-		PFM_REG_RETFLAG_SET(req->reg_flags, 0);
-
-		/*
-		 * now commit changes to software state
-		 */
-		hw_value = value;
-
-		/*
-		 * update virtualized (64bits) counter
-		 */
-		if (is_counting) {
-			/*
-			 * write context state
-			 */
-			ctx->ctx_pmds[cnum].lval = value;
-
-			/*
-			 * when context is load we use the split value
-			 */
-			if (is_loaded) {
-				hw_value = value &  ovfl_mask;
-				value    = value & ~ovfl_mask;
-			}
-		}
-		/*
-		 * update reset values (not just for counters)
-		 */
-		ctx->ctx_pmds[cnum].long_reset  = req->reg_long_reset;
-		ctx->ctx_pmds[cnum].short_reset = req->reg_short_reset;
-
-		/*
-		 * update randomization parameters (not just for counters)
-		 */
-		ctx->ctx_pmds[cnum].seed = req->reg_random_seed;
-		ctx->ctx_pmds[cnum].mask = req->reg_random_mask;
-
-		/*
-		 * update context value
-		 */
-		ctx->ctx_pmds[cnum].val  = value;
-
-		/*
-		 * Keep track of what we use
-		 *
-		 * We do not keep track of PMC because we have to
-		 * systematically restore ALL of them.
-		 */
-		CTX_USED_PMD(ctx, PMD_PMD_DEP(cnum));
-
-		/*
-		 * mark this PMD register used as well
-		 */
-		CTX_USED_PMD(ctx, RDEP(cnum));
-
-		/*
-		 * make sure we do not try to reset on
-		 * restart because we have established new values
-		 */
-		if (is_counting && state == PFM_CTX_MASKED) {
-			ctx->ctx_ovfl_regs[0] &= ~1UL << cnum;
-		}
-
-		if (is_loaded) {
-			/*
-		 	 * write thread state
-		 	 */
-			if (is_system == 0) ctx->th_pmds[cnum] = hw_value;
-
-			/*
-			 * write hardware register if we can
-			 */
-			if (can_access_pmu) {
-				ia64_set_pmd(cnum, hw_value);
-			} else {
-#ifdef CONFIG_SMP
-				/*
-			 	 * we are guaranteed that the task is not running on the other CPU,
-			 	 * we indicate that this PMD will need to be reloaded if the task
-			 	 * is rescheduled on the CPU it ran last on.
-			 	 */
-				ctx->ctx_reload_pmds[0] |= 1UL << cnum;
-#endif
-			}
-		}
-
-		DPRINT(("pmd[%u]=0x%lx ld=%d apmu=%d, hw_value=0x%lx ctx_pmd=0x%lx  short_reset=0x%lx "
-			  "long_reset=0x%lx notify=%c seed=0x%lx mask=0x%lx used_pmds=0x%lx reset_pmds=0x%lx reload_pmds=0x%lx all_pmds=0x%lx ovfl_regs=0x%lx\n",
-			cnum,
-			value,
-			is_loaded,
-			can_access_pmu,
-			hw_value,
-			ctx->ctx_pmds[cnum].val,
-			ctx->ctx_pmds[cnum].short_reset,
-			ctx->ctx_pmds[cnum].long_reset,
-			PMC_OVFL_NOTIFY(ctx, cnum) ? 'Y':'N',
-			ctx->ctx_pmds[cnum].seed,
-			ctx->ctx_pmds[cnum].mask,
-			ctx->ctx_used_pmds[0],
-			ctx->ctx_pmds[cnum].reset_pmds[0],
-			ctx->ctx_reload_pmds[0],
-			ctx->ctx_all_pmds[0],
-			ctx->ctx_ovfl_regs[0]));
-	}
-
-	/*
-	 * make changes visible
-	 */
-	if (can_access_pmu) ia64_srlz_d();
-
-	return 0;
-
-abort_mission:
-	/*
-	 * for now, we have only one possibility for error
-	 */
-	PFM_REG_RETFLAG_SET(req->reg_flags, PFM_REG_RETFL_EINVAL);
-	return ret;
-}
-
-/*
- * By the way of PROTECT_CONTEXT(), interrupts are masked while we are in this function.
- * Therefore we know, we do not have to worry about the PMU overflow interrupt. If an
- * interrupt is delivered during the call, it will be kept pending until we leave, making
- * it appears as if it had been generated at the UNPROTECT_CONTEXT(). At least we are
- * guaranteed to return consistent data to the user, it may simply be old. It is not
- * trivial to treat the overflow while inside the call because you may end up in
- * some module sampling buffer code causing deadlocks.
- */
-static int
-pfm_read_pmds(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	struct task_struct *task;
-	unsigned long val = 0UL, lval, ovfl_mask, sval;
-	pfarg_reg_t *req = (pfarg_reg_t *)arg;
-	unsigned int cnum, reg_flags = 0;
-	int i, can_access_pmu = 0, state;
-	int is_loaded, is_system, is_counting, expert_mode;
-	int ret = -EINVAL;
-	pfm_reg_check_t rd_func;
-
-	/*
-	 * access is possible when loaded only for
-	 * self-monitoring tasks or in UP mode
-	 */
-
-	state     = ctx->ctx_state;
-	is_loaded = state == PFM_CTX_LOADED ? 1 : 0;
-	is_system = ctx->ctx_fl_system;
-	ovfl_mask = pmu_conf->ovfl_val;
-	task      = ctx->ctx_task;
-
-	if (state == PFM_CTX_ZOMBIE) return -EINVAL;
-
-	if (likely(is_loaded)) {
-		/*
-		 * In system wide and when the context is loaded, access can only happen
-		 * when the caller is running on the CPU being monitored by the session.
-		 * It does not have to be the owner (ctx_task) of the context per se.
-		 */
-		if (unlikely(is_system && ctx->ctx_cpu != smp_processor_id())) {
-			DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
-			return -EBUSY;
-		}
-		/*
-		 * this can be true when not self-monitoring only in UP
-		 */
-		can_access_pmu = GET_PMU_OWNER() == task || is_system ? 1 : 0;
-
-		if (can_access_pmu) ia64_srlz_d();
-	}
-	expert_mode = pfm_sysctl.expert_mode; 
-
-	DPRINT(("ld=%d apmu=%d ctx_state=%d\n",
-		is_loaded,
-		can_access_pmu,
-		state));
-
-	/*
-	 * on both UP and SMP, we can only read the PMD from the hardware register when
-	 * the task is the owner of the local PMU.
-	 */
-
-	for (i = 0; i < count; i++, req++) {
-
-		cnum        = req->reg_num;
-		reg_flags   = req->reg_flags;
-
-		if (unlikely(!PMD_IS_IMPL(cnum))) goto error;
-		/*
-		 * we can only read the register that we use. That includes
-		 * the one we explicitly initialize AND the one we want included
-		 * in the sampling buffer (smpl_regs).
-		 *
-		 * Having this restriction allows optimization in the ctxsw routine
-		 * without compromising security (leaks)
-		 */
-		if (unlikely(!CTX_IS_USED_PMD(ctx, cnum))) goto error;
-
-		sval        = ctx->ctx_pmds[cnum].val;
-		lval        = ctx->ctx_pmds[cnum].lval;
-		is_counting = PMD_IS_COUNTING(cnum);
-
-		/*
-		 * If the task is not the current one, then we check if the
-		 * PMU state is still in the local live register due to lazy ctxsw.
-		 * If true, then we read directly from the registers.
-		 */
-		if (can_access_pmu){
-			val = ia64_get_pmd(cnum);
-		} else {
-			/*
-			 * context has been saved
-			 * if context is zombie, then task does not exist anymore.
-			 * In this case, we use the full value saved in the context (pfm_flush_regs()).
-			 */
-			val = is_loaded ? ctx->th_pmds[cnum] : 0UL;
-		}
-		rd_func = pmu_conf->pmd_desc[cnum].read_check;
-
-		if (is_counting) {
-			/*
-			 * XXX: need to check for overflow when loaded
-			 */
-			val &= ovfl_mask;
-			val += sval;
-		}
-
-		/*
-		 * execute read checker, if any
-		 */
-		if (unlikely(expert_mode == 0 && rd_func)) {
-			unsigned long v = val;
-			ret = (*rd_func)(ctx->ctx_task, ctx, cnum, &v, regs);
-			if (ret) goto error;
-			val = v;
-			ret = -EINVAL;
-		}
-
-		PFM_REG_RETFLAG_SET(reg_flags, 0);
-
-		DPRINT(("pmd[%u]=0x%lx\n", cnum, val));
-
-		/*
-		 * update register return value, abort all if problem during copy.
-		 * we only modify the reg_flags field. no check mode is fine because
-		 * access has been verified upfront in sys_perfmonctl().
-		 */
-		req->reg_value            = val;
-		req->reg_flags            = reg_flags;
-		req->reg_last_reset_val   = lval;
-	}
-
-	return 0;
-
-error:
-	PFM_REG_RETFLAG_SET(req->reg_flags, PFM_REG_RETFL_EINVAL);
-	return ret;
-}
-
-int
-pfm_mod_write_pmcs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs)
-{
-	pfm_context_t *ctx;
-
-	if (req == NULL) return -EINVAL;
-
- 	ctx = GET_PMU_CTX();
-
-	if (ctx == NULL) return -EINVAL;
-
-	/*
-	 * for now limit to current task, which is enough when calling
-	 * from overflow handler
-	 */
-	if (task != current && ctx->ctx_fl_system == 0) return -EBUSY;
-
-	return pfm_write_pmcs(ctx, req, nreq, regs);
-}
-EXPORT_SYMBOL(pfm_mod_write_pmcs);
-
-int
-pfm_mod_read_pmds(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs)
-{
-	pfm_context_t *ctx;
-
-	if (req == NULL) return -EINVAL;
-
- 	ctx = GET_PMU_CTX();
-
-	if (ctx == NULL) return -EINVAL;
-
-	/*
-	 * for now limit to current task, which is enough when calling
-	 * from overflow handler
-	 */
-	if (task != current && ctx->ctx_fl_system == 0) return -EBUSY;
-
-	return pfm_read_pmds(ctx, req, nreq, regs);
-}
-EXPORT_SYMBOL(pfm_mod_read_pmds);
-
-/*
- * Only call this function when a process it trying to
- * write the debug registers (reading is always allowed)
- */
-int
-pfm_use_debug_registers(struct task_struct *task)
-{
-	pfm_context_t *ctx = task->thread.pfm_context;
-	unsigned long flags;
-	int ret = 0;
-
-	if (pmu_conf->use_rr_dbregs == 0) return 0;
-
-	DPRINT(("called for [%d]\n", task_pid_nr(task)));
-
-	/*
-	 * do it only once
-	 */
-	if (task->thread.flags & IA64_THREAD_DBG_VALID) return 0;
-
-	/*
-	 * Even on SMP, we do not need to use an atomic here because
-	 * the only way in is via ptrace() and this is possible only when the
-	 * process is stopped. Even in the case where the ctxsw out is not totally
-	 * completed by the time we come here, there is no way the 'stopped' process
-	 * could be in the middle of fiddling with the pfm_write_ibr_dbr() routine.
-	 * So this is always safe.
-	 */
-	if (ctx && ctx->ctx_fl_using_dbreg == 1) return -1;
-
-	LOCK_PFS(flags);
-
-	/*
-	 * We cannot allow setting breakpoints when system wide monitoring
-	 * sessions are using the debug registers.
-	 */
-	if (pfm_sessions.pfs_sys_use_dbregs> 0)
-		ret = -1;
-	else
-		pfm_sessions.pfs_ptrace_use_dbregs++;
-
-	DPRINT(("ptrace_use_dbregs=%u  sys_use_dbregs=%u by [%d] ret = %d\n",
-		  pfm_sessions.pfs_ptrace_use_dbregs,
-		  pfm_sessions.pfs_sys_use_dbregs,
-		  task_pid_nr(task), ret));
-
-	UNLOCK_PFS(flags);
-
-	return ret;
-}
-
-/*
- * This function is called for every task that exits with the
- * IA64_THREAD_DBG_VALID set. This indicates a task which was
- * able to use the debug registers for debugging purposes via
- * ptrace(). Therefore we know it was not using them for
- * perfmormance monitoring, so we only decrement the number
- * of "ptraced" debug register users to keep the count up to date
- */
-int
-pfm_release_debug_registers(struct task_struct *task)
-{
-	unsigned long flags;
-	int ret;
-
-	if (pmu_conf->use_rr_dbregs == 0) return 0;
-
-	LOCK_PFS(flags);
-	if (pfm_sessions.pfs_ptrace_use_dbregs == 0) {
-		printk(KERN_ERR "perfmon: invalid release for [%d] ptrace_use_dbregs=0\n", task_pid_nr(task));
-		ret = -1;
-	}  else {
-		pfm_sessions.pfs_ptrace_use_dbregs--;
-		ret = 0;
-	}
-	UNLOCK_PFS(flags);
-
-	return ret;
-}
-
-static int
-pfm_restart(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	struct task_struct *task;
-	pfm_buffer_fmt_t *fmt;
-	pfm_ovfl_ctrl_t rst_ctrl;
-	int state, is_system;
-	int ret = 0;
-
-	state     = ctx->ctx_state;
-	fmt       = ctx->ctx_buf_fmt;
-	is_system = ctx->ctx_fl_system;
-	task      = PFM_CTX_TASK(ctx);
-
-	switch(state) {
-		case PFM_CTX_MASKED:
-			break;
-		case PFM_CTX_LOADED: 
-			if (CTX_HAS_SMPL(ctx) && fmt->fmt_restart_active) break;
-			/* fall through */
-		case PFM_CTX_UNLOADED:
-		case PFM_CTX_ZOMBIE:
-			DPRINT(("invalid state=%d\n", state));
-			return -EBUSY;
-		default:
-			DPRINT(("state=%d, cannot operate (no active_restart handler)\n", state));
-			return -EINVAL;
-	}
-
-	/*
- 	 * In system wide and when the context is loaded, access can only happen
- 	 * when the caller is running on the CPU being monitored by the session.
- 	 * It does not have to be the owner (ctx_task) of the context per se.
- 	 */
-	if (is_system && ctx->ctx_cpu != smp_processor_id()) {
-		DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
-		return -EBUSY;
-	}
-
-	/* sanity check */
-	if (unlikely(task == NULL)) {
-		printk(KERN_ERR "perfmon: [%d] pfm_restart no task\n", task_pid_nr(current));
-		return -EINVAL;
-	}
-
-	if (task == current || is_system) {
-
-		fmt = ctx->ctx_buf_fmt;
-
-		DPRINT(("restarting self %d ovfl=0x%lx\n",
-			task_pid_nr(task),
-			ctx->ctx_ovfl_regs[0]));
-
-		if (CTX_HAS_SMPL(ctx)) {
-
-			prefetch(ctx->ctx_smpl_hdr);
-
-			rst_ctrl.bits.mask_monitoring = 0;
-			rst_ctrl.bits.reset_ovfl_pmds = 0;
-
-			if (state == PFM_CTX_LOADED)
-				ret = pfm_buf_fmt_restart_active(fmt, task, &rst_ctrl, ctx->ctx_smpl_hdr, regs);
-			else
-				ret = pfm_buf_fmt_restart(fmt, task, &rst_ctrl, ctx->ctx_smpl_hdr, regs);
-		} else {
-			rst_ctrl.bits.mask_monitoring = 0;
-			rst_ctrl.bits.reset_ovfl_pmds = 1;
-		}
-
-		if (ret == 0) {
-			if (rst_ctrl.bits.reset_ovfl_pmds)
-				pfm_reset_regs(ctx, ctx->ctx_ovfl_regs, PFM_PMD_LONG_RESET);
-
-			if (rst_ctrl.bits.mask_monitoring == 0) {
-				DPRINT(("resuming monitoring for [%d]\n", task_pid_nr(task)));
-
-				if (state == PFM_CTX_MASKED) pfm_restore_monitoring(task);
-			} else {
-				DPRINT(("keeping monitoring stopped for [%d]\n", task_pid_nr(task)));
-
-				// cannot use pfm_stop_monitoring(task, regs);
-			}
-		}
-		/*
-		 * clear overflowed PMD mask to remove any stale information
-		 */
-		ctx->ctx_ovfl_regs[0] = 0UL;
-
-		/*
-		 * back to LOADED state
-		 */
-		ctx->ctx_state = PFM_CTX_LOADED;
-
-		/*
-		 * XXX: not really useful for self monitoring
-		 */
-		ctx->ctx_fl_can_restart = 0;
-
-		return 0;
-	}
-
-	/* 
-	 * restart another task
-	 */
-
-	/*
-	 * When PFM_CTX_MASKED, we cannot issue a restart before the previous 
-	 * one is seen by the task.
-	 */
-	if (state == PFM_CTX_MASKED) {
-		if (ctx->ctx_fl_can_restart == 0) return -EINVAL;
-		/*
-		 * will prevent subsequent restart before this one is
-		 * seen by other task
-		 */
-		ctx->ctx_fl_can_restart = 0;
-	}
-
-	/*
-	 * if blocking, then post the semaphore is PFM_CTX_MASKED, i.e.
-	 * the task is blocked or on its way to block. That's the normal
-	 * restart path. If the monitoring is not masked, then the task
-	 * can be actively monitoring and we cannot directly intervene.
-	 * Therefore we use the trap mechanism to catch the task and
-	 * force it to reset the buffer/reset PMDs.
-	 *
-	 * if non-blocking, then we ensure that the task will go into
-	 * pfm_handle_work() before returning to user mode.
-	 *
-	 * We cannot explicitly reset another task, it MUST always
-	 * be done by the task itself. This works for system wide because
-	 * the tool that is controlling the session is logically doing 
-	 * "self-monitoring".
-	 */
-	if (CTX_OVFL_NOBLOCK(ctx) == 0 && state == PFM_CTX_MASKED) {
-		DPRINT(("unblocking [%d] \n", task_pid_nr(task)));
-		complete(&ctx->ctx_restart_done);
-	} else {
-		DPRINT(("[%d] armed exit trap\n", task_pid_nr(task)));
-
-		ctx->ctx_fl_trap_reason = PFM_TRAP_REASON_RESET;
-
-		PFM_SET_WORK_PENDING(task, 1);
-
-		set_notify_resume(task);
-
-		/*
-		 * XXX: send reschedule if task runs on another CPU
-		 */
-	}
-	return 0;
-}
-
-static int
-pfm_debug(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	unsigned int m = *(unsigned int *)arg;
-
-	pfm_sysctl.debug = m == 0 ? 0 : 1;
-
-	printk(KERN_INFO "perfmon debugging %s (timing reset)\n", pfm_sysctl.debug ? "on" : "off");
-
-	if (m == 0) {
-		memset(pfm_stats, 0, sizeof(pfm_stats));
-		for(m=0; m < NR_CPUS; m++) pfm_stats[m].pfm_ovfl_intr_cycles_min = ~0UL;
-	}
-	return 0;
-}
-
-/*
- * arg can be NULL and count can be zero for this function
- */
-static int
-pfm_write_ibr_dbr(int mode, pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	struct thread_struct *thread = NULL;
-	struct task_struct *task;
-	pfarg_dbreg_t *req = (pfarg_dbreg_t *)arg;
-	unsigned long flags;
-	dbreg_t dbreg;
-	unsigned int rnum;
-	int first_time;
-	int ret = 0, state;
-	int i, can_access_pmu = 0;
-	int is_system, is_loaded;
-
-	if (pmu_conf->use_rr_dbregs == 0) return -EINVAL;
-
-	state     = ctx->ctx_state;
-	is_loaded = state == PFM_CTX_LOADED ? 1 : 0;
-	is_system = ctx->ctx_fl_system;
-	task      = ctx->ctx_task;
-
-	if (state == PFM_CTX_ZOMBIE) return -EINVAL;
-
-	/*
-	 * on both UP and SMP, we can only write to the PMC when the task is
-	 * the owner of the local PMU.
-	 */
-	if (is_loaded) {
-		thread = &task->thread;
-		/*
-		 * In system wide and when the context is loaded, access can only happen
-		 * when the caller is running on the CPU being monitored by the session.
-		 * It does not have to be the owner (ctx_task) of the context per se.
-		 */
-		if (unlikely(is_system && ctx->ctx_cpu != smp_processor_id())) {
-			DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
-			return -EBUSY;
-		}
-		can_access_pmu = GET_PMU_OWNER() == task || is_system ? 1 : 0;
-	}
-
-	/*
-	 * we do not need to check for ipsr.db because we do clear ibr.x, dbr.r, and dbr.w
-	 * ensuring that no real breakpoint can be installed via this call.
-	 *
-	 * IMPORTANT: regs can be NULL in this function
-	 */
-
-	first_time = ctx->ctx_fl_using_dbreg == 0;
-
-	/*
-	 * don't bother if we are loaded and task is being debugged
-	 */
-	if (is_loaded && (thread->flags & IA64_THREAD_DBG_VALID) != 0) {
-		DPRINT(("debug registers already in use for [%d]\n", task_pid_nr(task)));
-		return -EBUSY;
-	}
-
-	/*
-	 * check for debug registers in system wide mode
-	 *
-	 * If though a check is done in pfm_context_load(),
-	 * we must repeat it here, in case the registers are
-	 * written after the context is loaded
-	 */
-	if (is_loaded) {
-		LOCK_PFS(flags);
-
-		if (first_time && is_system) {
-			if (pfm_sessions.pfs_ptrace_use_dbregs)
-				ret = -EBUSY;
-			else
-				pfm_sessions.pfs_sys_use_dbregs++;
-		}
-		UNLOCK_PFS(flags);
-	}
-
-	if (ret != 0) return ret;
-
-	/*
-	 * mark ourself as user of the debug registers for
-	 * perfmon purposes.
-	 */
-	ctx->ctx_fl_using_dbreg = 1;
-
-	/*
- 	 * clear hardware registers to make sure we don't
- 	 * pick up stale state.
-	 *
-	 * for a system wide session, we do not use
-	 * thread.dbr, thread.ibr because this process
-	 * never leaves the current CPU and the state
-	 * is shared by all processes running on it
- 	 */
-	if (first_time && can_access_pmu) {
-		DPRINT(("[%d] clearing ibrs, dbrs\n", task_pid_nr(task)));
-		for (i=0; i < pmu_conf->num_ibrs; i++) {
-			ia64_set_ibr(i, 0UL);
-			ia64_dv_serialize_instruction();
-		}
-		ia64_srlz_i();
-		for (i=0; i < pmu_conf->num_dbrs; i++) {
-			ia64_set_dbr(i, 0UL);
-			ia64_dv_serialize_data();
-		}
-		ia64_srlz_d();
-	}
-
-	/*
-	 * Now install the values into the registers
-	 */
-	for (i = 0; i < count; i++, req++) {
-
-		rnum      = req->dbreg_num;
-		dbreg.val = req->dbreg_value;
-
-		ret = -EINVAL;
-
-		if ((mode == PFM_CODE_RR && rnum >= PFM_NUM_IBRS) || ((mode == PFM_DATA_RR) && rnum >= PFM_NUM_DBRS)) {
-			DPRINT(("invalid register %u val=0x%lx mode=%d i=%d count=%d\n",
-				  rnum, dbreg.val, mode, i, count));
-
-			goto abort_mission;
-		}
-
-		/*
-		 * make sure we do not install enabled breakpoint
-		 */
-		if (rnum & 0x1) {
-			if (mode == PFM_CODE_RR)
-				dbreg.ibr.ibr_x = 0;
-			else
-				dbreg.dbr.dbr_r = dbreg.dbr.dbr_w = 0;
-		}
-
-		PFM_REG_RETFLAG_SET(req->dbreg_flags, 0);
-
-		/*
-		 * Debug registers, just like PMC, can only be modified
-		 * by a kernel call. Moreover, perfmon() access to those
-		 * registers are centralized in this routine. The hardware
-		 * does not modify the value of these registers, therefore,
-		 * if we save them as they are written, we can avoid having
-		 * to save them on context switch out. This is made possible
-		 * by the fact that when perfmon uses debug registers, ptrace()
-		 * won't be able to modify them concurrently.
-		 */
-		if (mode == PFM_CODE_RR) {
-			CTX_USED_IBR(ctx, rnum);
-
-			if (can_access_pmu) {
-				ia64_set_ibr(rnum, dbreg.val);
-				ia64_dv_serialize_instruction();
-			}
-
-			ctx->ctx_ibrs[rnum] = dbreg.val;
-
-			DPRINT(("write ibr%u=0x%lx used_ibrs=0x%x ld=%d apmu=%d\n",
-				rnum, dbreg.val, ctx->ctx_used_ibrs[0], is_loaded, can_access_pmu));
-		} else {
-			CTX_USED_DBR(ctx, rnum);
-
-			if (can_access_pmu) {
-				ia64_set_dbr(rnum, dbreg.val);
-				ia64_dv_serialize_data();
-			}
-			ctx->ctx_dbrs[rnum] = dbreg.val;
-
-			DPRINT(("write dbr%u=0x%lx used_dbrs=0x%x ld=%d apmu=%d\n",
-				rnum, dbreg.val, ctx->ctx_used_dbrs[0], is_loaded, can_access_pmu));
-		}
-	}
-
-	return 0;
-
-abort_mission:
-	/*
-	 * in case it was our first attempt, we undo the global modifications
-	 */
-	if (first_time) {
-		LOCK_PFS(flags);
-		if (ctx->ctx_fl_system) {
-			pfm_sessions.pfs_sys_use_dbregs--;
-		}
-		UNLOCK_PFS(flags);
-		ctx->ctx_fl_using_dbreg = 0;
-	}
-	/*
-	 * install error return flag
-	 */
-	PFM_REG_RETFLAG_SET(req->dbreg_flags, PFM_REG_RETFL_EINVAL);
-
-	return ret;
-}
-
-static int
-pfm_write_ibrs(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	return pfm_write_ibr_dbr(PFM_CODE_RR, ctx, arg, count, regs);
-}
-
-static int
-pfm_write_dbrs(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	return pfm_write_ibr_dbr(PFM_DATA_RR, ctx, arg, count, regs);
-}
-
-int
-pfm_mod_write_ibrs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs)
-{
-	pfm_context_t *ctx;
-
-	if (req == NULL) return -EINVAL;
-
- 	ctx = GET_PMU_CTX();
-
-	if (ctx == NULL) return -EINVAL;
-
-	/*
-	 * for now limit to current task, which is enough when calling
-	 * from overflow handler
-	 */
-	if (task != current && ctx->ctx_fl_system == 0) return -EBUSY;
-
-	return pfm_write_ibrs(ctx, req, nreq, regs);
-}
-EXPORT_SYMBOL(pfm_mod_write_ibrs);
-
-int
-pfm_mod_write_dbrs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs)
-{
-	pfm_context_t *ctx;
-
-	if (req == NULL) return -EINVAL;
-
- 	ctx = GET_PMU_CTX();
-
-	if (ctx == NULL) return -EINVAL;
-
-	/*
-	 * for now limit to current task, which is enough when calling
-	 * from overflow handler
-	 */
-	if (task != current && ctx->ctx_fl_system == 0) return -EBUSY;
-
-	return pfm_write_dbrs(ctx, req, nreq, regs);
-}
-EXPORT_SYMBOL(pfm_mod_write_dbrs);
-
-
-static int
-pfm_get_features(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	pfarg_features_t *req = (pfarg_features_t *)arg;
-
-	req->ft_version = PFM_VERSION;
-	return 0;
-}
-
-static int
-pfm_stop(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	struct pt_regs *tregs;
-	struct task_struct *task = PFM_CTX_TASK(ctx);
-	int state, is_system;
-
-	state     = ctx->ctx_state;
-	is_system = ctx->ctx_fl_system;
-
-	/*
-	 * context must be attached to issue the stop command (includes LOADED,MASKED,ZOMBIE)
-	 */
-	if (state == PFM_CTX_UNLOADED) return -EINVAL;
-
-	/*
- 	 * In system wide and when the context is loaded, access can only happen
- 	 * when the caller is running on the CPU being monitored by the session.
- 	 * It does not have to be the owner (ctx_task) of the context per se.
- 	 */
-	if (is_system && ctx->ctx_cpu != smp_processor_id()) {
-		DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
-		return -EBUSY;
-	}
-	DPRINT(("task [%d] ctx_state=%d is_system=%d\n",
-		task_pid_nr(PFM_CTX_TASK(ctx)),
-		state,
-		is_system));
-	/*
-	 * in system mode, we need to update the PMU directly
-	 * and the user level state of the caller, which may not
-	 * necessarily be the creator of the context.
-	 */
-	if (is_system) {
-		/*
-		 * Update local PMU first
-		 *
-		 * disable dcr pp
-		 */
-		ia64_setreg(_IA64_REG_CR_DCR, ia64_getreg(_IA64_REG_CR_DCR) & ~IA64_DCR_PP);
-		ia64_srlz_i();
-
-		/*
-		 * update local cpuinfo
-		 */
-		PFM_CPUINFO_CLEAR(PFM_CPUINFO_DCR_PP);
-
-		/*
-		 * stop monitoring, does srlz.i
-		 */
-		pfm_clear_psr_pp();
-
-		/*
-		 * stop monitoring in the caller
-		 */
-		ia64_psr(regs)->pp = 0;
-
-		return 0;
-	}
-	/*
-	 * per-task mode
-	 */
-
-	if (task == current) {
-		/* stop monitoring  at kernel level */
-		pfm_clear_psr_up();
-
-		/*
-	 	 * stop monitoring at the user level
-	 	 */
-		ia64_psr(regs)->up = 0;
-	} else {
-		tregs = task_pt_regs(task);
-
-		/*
-	 	 * stop monitoring at the user level
-	 	 */
-		ia64_psr(tregs)->up = 0;
-
-		/*
-		 * monitoring disabled in kernel at next reschedule
-		 */
-		ctx->ctx_saved_psr_up = 0;
-		DPRINT(("task=[%d]\n", task_pid_nr(task)));
-	}
-	return 0;
-}
-
-
-static int
-pfm_start(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	struct pt_regs *tregs;
-	int state, is_system;
-
-	state     = ctx->ctx_state;
-	is_system = ctx->ctx_fl_system;
-
-	if (state != PFM_CTX_LOADED) return -EINVAL;
-
-	/*
- 	 * In system wide and when the context is loaded, access can only happen
- 	 * when the caller is running on the CPU being monitored by the session.
- 	 * It does not have to be the owner (ctx_task) of the context per se.
- 	 */
-	if (is_system && ctx->ctx_cpu != smp_processor_id()) {
-		DPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));
-		return -EBUSY;
-	}
-
-	/*
-	 * in system mode, we need to update the PMU directly
-	 * and the user level state of the caller, which may not
-	 * necessarily be the creator of the context.
-	 */
-	if (is_system) {
-
-		/*
-		 * set user level psr.pp for the caller
-		 */
-		ia64_psr(regs)->pp = 1;
-
-		/*
-		 * now update the local PMU and cpuinfo
-		 */
-		PFM_CPUINFO_SET(PFM_CPUINFO_DCR_PP);
-
-		/*
-		 * start monitoring at kernel level
-		 */
-		pfm_set_psr_pp();
-
-		/* enable dcr pp */
-		ia64_setreg(_IA64_REG_CR_DCR, ia64_getreg(_IA64_REG_CR_DCR) | IA64_DCR_PP);
-		ia64_srlz_i();
-
-		return 0;
-	}
-
-	/*
-	 * per-process mode
-	 */
-
-	if (ctx->ctx_task == current) {
-
-		/* start monitoring at kernel level */
-		pfm_set_psr_up();
-
-		/*
-		 * activate monitoring at user level
-		 */
-		ia64_psr(regs)->up = 1;
-
-	} else {
-		tregs = task_pt_regs(ctx->ctx_task);
-
-		/*
-		 * start monitoring at the kernel level the next
-		 * time the task is scheduled
-		 */
-		ctx->ctx_saved_psr_up = IA64_PSR_UP;
-
-		/*
-		 * activate monitoring at user level
-		 */
-		ia64_psr(tregs)->up = 1;
-	}
-	return 0;
-}
-
-static int
-pfm_get_pmc_reset(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	pfarg_reg_t *req = (pfarg_reg_t *)arg;
-	unsigned int cnum;
-	int i;
-	int ret = -EINVAL;
-
-	for (i = 0; i < count; i++, req++) {
-
-		cnum = req->reg_num;
-
-		if (!PMC_IS_IMPL(cnum)) goto abort_mission;
-
-		req->reg_value = PMC_DFL_VAL(cnum);
-
-		PFM_REG_RETFLAG_SET(req->reg_flags, 0);
-
-		DPRINT(("pmc_reset_val pmc[%u]=0x%lx\n", cnum, req->reg_value));
-	}
-	return 0;
-
-abort_mission:
-	PFM_REG_RETFLAG_SET(req->reg_flags, PFM_REG_RETFL_EINVAL);
-	return ret;
-}
-
-static int
-pfm_check_task_exist(pfm_context_t *ctx)
-{
-	struct task_struct *g, *t;
-	int ret = -ESRCH;
-
-	read_lock(&tasklist_lock);
-
-	do_each_thread (g, t) {
-		if (t->thread.pfm_context == ctx) {
-			ret = 0;
-			goto out;
-		}
-	} while_each_thread (g, t);
-out:
-	read_unlock(&tasklist_lock);
-
-	DPRINT(("pfm_check_task_exist: ret=%d ctx=%p\n", ret, ctx));
-
-	return ret;
-}
-
-static int
-pfm_context_load(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	struct task_struct *task;
-	struct thread_struct *thread;
-	struct pfm_context_t *old;
-	unsigned long flags;
-#ifndef CONFIG_SMP
-	struct task_struct *owner_task = NULL;
-#endif
-	pfarg_load_t *req = (pfarg_load_t *)arg;
-	unsigned long *pmcs_source, *pmds_source;
-	int the_cpu;
-	int ret = 0;
-	int state, is_system, set_dbregs = 0;
-
-	state     = ctx->ctx_state;
-	is_system = ctx->ctx_fl_system;
-	/*
-	 * can only load from unloaded or terminated state
-	 */
-	if (state != PFM_CTX_UNLOADED) {
-		DPRINT(("cannot load to [%d], invalid ctx_state=%d\n",
-			req->load_pid,
-			ctx->ctx_state));
-		return -EBUSY;
-	}
-
-	DPRINT(("load_pid [%d] using_dbreg=%d\n", req->load_pid, ctx->ctx_fl_using_dbreg));
-
-	if (CTX_OVFL_NOBLOCK(ctx) == 0 && req->load_pid == current->pid) {
-		DPRINT(("cannot use blocking mode on self\n"));
-		return -EINVAL;
-	}
-
-	ret = pfm_get_task(ctx, req->load_pid, &task);
-	if (ret) {
-		DPRINT(("load_pid [%d] get_task=%d\n", req->load_pid, ret));
-		return ret;
-	}
-
-	ret = -EINVAL;
-
-	/*
-	 * system wide is self monitoring only
-	 */
-	if (is_system && task != current) {
-		DPRINT(("system wide is self monitoring only load_pid=%d\n",
-			req->load_pid));
-		goto error;
-	}
-
-	thread = &task->thread;
-
-	ret = 0;
-	/*
-	 * cannot load a context which is using range restrictions,
-	 * into a task that is being debugged.
-	 */
-	if (ctx->ctx_fl_using_dbreg) {
-		if (thread->flags & IA64_THREAD_DBG_VALID) {
-			ret = -EBUSY;
-			DPRINT(("load_pid [%d] task is debugged, cannot load range restrictions\n", req->load_pid));
-			goto error;
-		}
-		LOCK_PFS(flags);
-
-		if (is_system) {
-			if (pfm_sessions.pfs_ptrace_use_dbregs) {
-				DPRINT(("cannot load [%d] dbregs in use\n",
-							task_pid_nr(task)));
-				ret = -EBUSY;
-			} else {
-				pfm_sessions.pfs_sys_use_dbregs++;
-				DPRINT(("load [%d] increased sys_use_dbreg=%u\n", task_pid_nr(task), pfm_sessions.pfs_sys_use_dbregs));
-				set_dbregs = 1;
-			}
-		}
-
-		UNLOCK_PFS(flags);
-
-		if (ret) goto error;
-	}
-
-	/*
-	 * SMP system-wide monitoring implies self-monitoring.
-	 *
-	 * The programming model expects the task to
-	 * be pinned on a CPU throughout the session.
-	 * Here we take note of the current CPU at the
-	 * time the context is loaded. No call from
-	 * another CPU will be allowed.
-	 *
-	 * The pinning via shed_setaffinity()
-	 * must be done by the calling task prior
-	 * to this call.
-	 *
-	 * systemwide: keep track of CPU this session is supposed to run on
-	 */
-	the_cpu = ctx->ctx_cpu = smp_processor_id();
-
-	ret = -EBUSY;
-	/*
-	 * now reserve the session
-	 */
-	ret = pfm_reserve_session(current, is_system, the_cpu);
-	if (ret) goto error;
-
-	/*
-	 * task is necessarily stopped at this point.
-	 *
-	 * If the previous context was zombie, then it got removed in
-	 * pfm_save_regs(). Therefore we should not see it here.
-	 * If we see a context, then this is an active context
-	 *
-	 * XXX: needs to be atomic
-	 */
-	DPRINT(("before cmpxchg() old_ctx=%p new_ctx=%p\n",
-		thread->pfm_context, ctx));
-
-	ret = -EBUSY;
-	old = ia64_cmpxchg(acq, &thread->pfm_context, NULL, ctx, sizeof(pfm_context_t *));
-	if (old != NULL) {
-		DPRINT(("load_pid [%d] already has a context\n", req->load_pid));
-		goto error_unres;
-	}
-
-	pfm_reset_msgq(ctx);
-
-	ctx->ctx_state = PFM_CTX_LOADED;
-
-	/*
-	 * link context to task
-	 */
-	ctx->ctx_task = task;
-
-	if (is_system) {
-		/*
-		 * we load as stopped
-		 */
-		PFM_CPUINFO_SET(PFM_CPUINFO_SYST_WIDE);
-		PFM_CPUINFO_CLEAR(PFM_CPUINFO_DCR_PP);
-
-		if (ctx->ctx_fl_excl_idle) PFM_CPUINFO_SET(PFM_CPUINFO_EXCL_IDLE);
-	} else {
-		thread->flags |= IA64_THREAD_PM_VALID;
-	}
-
-	/*
-	 * propagate into thread-state
-	 */
-	pfm_copy_pmds(task, ctx);
-	pfm_copy_pmcs(task, ctx);
-
-	pmcs_source = ctx->th_pmcs;
-	pmds_source = ctx->th_pmds;
-
-	/*
-	 * always the case for system-wide
-	 */
-	if (task == current) {
-
-		if (is_system == 0) {
-
-			/* allow user level control */
-			ia64_psr(regs)->sp = 0;
-			DPRINT(("clearing psr.sp for [%d]\n", task_pid_nr(task)));
-
-			SET_LAST_CPU(ctx, smp_processor_id());
-			INC_ACTIVATION();
-			SET_ACTIVATION(ctx);
-#ifndef CONFIG_SMP
-			/*
-			 * push the other task out, if any
-			 */
-			owner_task = GET_PMU_OWNER();
-			if (owner_task) pfm_lazy_save_regs(owner_task);
-#endif
-		}
-		/*
-		 * load all PMD from ctx to PMU (as opposed to thread state)
-		 * restore all PMC from ctx to PMU
-		 */
-		pfm_restore_pmds(pmds_source, ctx->ctx_all_pmds[0]);
-		pfm_restore_pmcs(pmcs_source, ctx->ctx_all_pmcs[0]);
-
-		ctx->ctx_reload_pmcs[0] = 0UL;
-		ctx->ctx_reload_pmds[0] = 0UL;
-
-		/*
-		 * guaranteed safe by earlier check against DBG_VALID
-		 */
-		if (ctx->ctx_fl_using_dbreg) {
-			pfm_restore_ibrs(ctx->ctx_ibrs, pmu_conf->num_ibrs);
-			pfm_restore_dbrs(ctx->ctx_dbrs, pmu_conf->num_dbrs);
-		}
-		/*
-		 * set new ownership
-		 */
-		SET_PMU_OWNER(task, ctx);
-
-		DPRINT(("context loaded on PMU for [%d]\n", task_pid_nr(task)));
-	} else {
-		/*
-		 * when not current, task MUST be stopped, so this is safe
-		 */
-		regs = task_pt_regs(task);
-
-		/* force a full reload */
-		ctx->ctx_last_activation = PFM_INVALID_ACTIVATION;
-		SET_LAST_CPU(ctx, -1);
-
-		/* initial saved psr (stopped) */
-		ctx->ctx_saved_psr_up = 0UL;
-		ia64_psr(regs)->up = ia64_psr(regs)->pp = 0;
-	}
-
-	ret = 0;
-
-error_unres:
-	if (ret) pfm_unreserve_session(ctx, ctx->ctx_fl_system, the_cpu);
-error:
-	/*
-	 * we must undo the dbregs setting (for system-wide)
-	 */
-	if (ret && set_dbregs) {
-		LOCK_PFS(flags);
-		pfm_sessions.pfs_sys_use_dbregs--;
-		UNLOCK_PFS(flags);
-	}
-	/*
-	 * release task, there is now a link with the context
-	 */
-	if (is_system == 0 && task != current) {
-		pfm_put_task(task);
-
-		if (ret == 0) {
-			ret = pfm_check_task_exist(ctx);
-			if (ret) {
-				ctx->ctx_state = PFM_CTX_UNLOADED;
-				ctx->ctx_task  = NULL;
-			}
-		}
-	}
-	return ret;
-}
-
-/*
- * in this function, we do not need to increase the use count
- * for the task via get_task_struct(), because we hold the
- * context lock. If the task were to disappear while having
- * a context attached, it would go through pfm_exit_thread()
- * which also grabs the context lock  and would therefore be blocked
- * until we are here.
- */
-static void pfm_flush_pmds(struct task_struct *, pfm_context_t *ctx);
-
-static int
-pfm_context_unload(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)
-{
-	struct task_struct *task = PFM_CTX_TASK(ctx);
-	struct pt_regs *tregs;
-	int prev_state, is_system;
-	int ret;
-
-	DPRINT(("ctx_state=%d task [%d]\n", ctx->ctx_state, task ? task_pid_nr(task) : -1));
-
-	prev_state = ctx->ctx_state;
-	is_system  = ctx->ctx_fl_system;
-
-	/*
-	 * unload only when necessary
-	 */
-	if (prev_state == PFM_CTX_UNLOADED) {
-		DPRINT(("ctx_state=%d, nothing to do\n", prev_state));
-		return 0;
-	}
-
-	/*
-	 * clear psr and dcr bits
-	 */
-	ret = pfm_stop(ctx, NULL, 0, regs);
-	if (ret) return ret;
-
-	ctx->ctx_state = PFM_CTX_UNLOADED;
-
-	/*
-	 * in system mode, we need to update the PMU directly
-	 * and the user level state of the caller, which may not
-	 * necessarily be the creator of the context.
-	 */
-	if (is_system) {
-
-		/*
-		 * Update cpuinfo
-		 *
-		 * local PMU is taken care of in pfm_stop()
-		 */
-		PFM_CPUINFO_CLEAR(PFM_CPUINFO_SYST_WIDE);
-		PFM_CPUINFO_CLEAR(PFM_CPUINFO_EXCL_IDLE);
-
-		/*
-		 * save PMDs in context
-		 * release ownership
-		 */
-		pfm_flush_pmds(current, ctx);
-
-		/*
-		 * at this point we are done with the PMU
-		 * so we can unreserve the resource.
-		 */
-		if (prev_state != PFM_CTX_ZOMBIE) 
-			pfm_unreserve_session(ctx, 1 , ctx->ctx_cpu);
-
-		/*
-		 * disconnect context from task
-		 */
-		task->thread.pfm_context = NULL;
-		/*
-		 * disconnect task from context
-		 */
-		ctx->ctx_task = NULL;
-
-		/*
-		 * There is nothing more to cleanup here.
-		 */
-		return 0;
-	}
-
-	/*
-	 * per-task mode
-	 */
-	tregs = task == current ? regs : task_pt_regs(task);
-
-	if (task == current) {
-		/*
-		 * cancel user level control
-		 */
-		ia64_psr(regs)->sp = 1;
-
-		DPRINT(("setting psr.sp for [%d]\n", task_pid_nr(task)));
-	}
-	/*
-	 * save PMDs to context
-	 * release ownership
-	 */
-	pfm_flush_pmds(task, ctx);
-
-	/*
-	 * at this point we are done with the PMU
-	 * so we can unreserve the resource.
-	 *
-	 * when state was ZOMBIE, we have already unreserved.
-	 */
-	if (prev_state != PFM_CTX_ZOMBIE) 
-		pfm_unreserve_session(ctx, 0 , ctx->ctx_cpu);
-
-	/*
-	 * reset activation counter and psr
-	 */
-	ctx->ctx_last_activation = PFM_INVALID_ACTIVATION;
-	SET_LAST_CPU(ctx, -1);
-
-	/*
-	 * PMU state will not be restored
-	 */
-	task->thread.flags &= ~IA64_THREAD_PM_VALID;
-
-	/*
-	 * break links between context and task
-	 */
-	task->thread.pfm_context  = NULL;
-	ctx->ctx_task             = NULL;
-
-	PFM_SET_WORK_PENDING(task, 0);
-
-	ctx->ctx_fl_trap_reason  = PFM_TRAP_REASON_NONE;
-	ctx->ctx_fl_can_restart  = 0;
-	ctx->ctx_fl_going_zombie = 0;
-
-	DPRINT(("disconnected [%d] from context\n", task_pid_nr(task)));
-
-	return 0;
-}
-
-
-/*
- * called only from exit_thread(): task == current
- * we come here only if current has a context attached (loaded or masked)
- */
-void
-pfm_exit_thread(struct task_struct *task)
-{
-	pfm_context_t *ctx;
-	unsigned long flags;
-	struct pt_regs *regs = task_pt_regs(task);
-	int ret, state;
-	int free_ok = 0;
-
-	ctx = PFM_GET_CTX(task);
-
-	PROTECT_CTX(ctx, flags);
-
-	DPRINT(("state=%d task [%d]\n", ctx->ctx_state, task_pid_nr(task)));
-
-	state = ctx->ctx_state;
-	switch(state) {
-		case PFM_CTX_UNLOADED:
-			/*
-	 		 * only comes to this function if pfm_context is not NULL, i.e., cannot
-			 * be in unloaded state
-	 		 */
-			printk(KERN_ERR "perfmon: pfm_exit_thread [%d] ctx unloaded\n", task_pid_nr(task));
-			break;
-		case PFM_CTX_LOADED:
-		case PFM_CTX_MASKED:
-			ret = pfm_context_unload(ctx, NULL, 0, regs);
-			if (ret) {
-				printk(KERN_ERR "perfmon: pfm_exit_thread [%d] state=%d unload failed %d\n", task_pid_nr(task), state, ret);
-			}
-			DPRINT(("ctx unloaded for current state was %d\n", state));
-
-			pfm_end_notify_user(ctx);
-			break;
-		case PFM_CTX_ZOMBIE:
-			ret = pfm_context_unload(ctx, NULL, 0, regs);
-			if (ret) {
-				printk(KERN_ERR "perfmon: pfm_exit_thread [%d] state=%d unload failed %d\n", task_pid_nr(task), state, ret);
-			}
-			free_ok = 1;
-			break;
-		default:
-			printk(KERN_ERR "perfmon: pfm_exit_thread [%d] unexpected state=%d\n", task_pid_nr(task), state);
-			break;
-	}
-	UNPROTECT_CTX(ctx, flags);
-
-	{ u64 psr = pfm_get_psr();
-	  BUG_ON(psr & (IA64_PSR_UP|IA64_PSR_PP));
-	  BUG_ON(GET_PMU_OWNER());
-	  BUG_ON(ia64_psr(regs)->up);
-	  BUG_ON(ia64_psr(regs)->pp);
-	}
-
-	/*
-	 * All memory free operations (especially for vmalloc'ed memory)
-	 * MUST be done with interrupts ENABLED.
-	 */
-	if (free_ok) pfm_context_free(ctx);
-}
-
-/*
- * functions MUST be listed in the increasing order of their index (see permfon.h)
- */
-#define PFM_CMD(name, flags, arg_count, arg_type, getsz) { name, #name, flags, arg_count, sizeof(arg_type), getsz }
-#define PFM_CMD_S(name, flags) { name, #name, flags, 0, 0, NULL }
-#define PFM_CMD_PCLRWS	(PFM_CMD_FD|PFM_CMD_ARG_RW|PFM_CMD_STOP)
-#define PFM_CMD_PCLRW	(PFM_CMD_FD|PFM_CMD_ARG_RW)
-#define PFM_CMD_NONE	{ NULL, "no-cmd", 0, 0, 0, NULL}
-
-static pfm_cmd_desc_t pfm_cmd_tab[]={
-/* 0  */PFM_CMD_NONE,
-/* 1  */PFM_CMD(pfm_write_pmcs, PFM_CMD_PCLRWS, PFM_CMD_ARG_MANY, pfarg_reg_t, NULL),
-/* 2  */PFM_CMD(pfm_write_pmds, PFM_CMD_PCLRWS, PFM_CMD_ARG_MANY, pfarg_reg_t, NULL),
-/* 3  */PFM_CMD(pfm_read_pmds, PFM_CMD_PCLRWS, PFM_CMD_ARG_MANY, pfarg_reg_t, NULL),
-/* 4  */PFM_CMD_S(pfm_stop, PFM_CMD_PCLRWS),
-/* 5  */PFM_CMD_S(pfm_start, PFM_CMD_PCLRWS),
-/* 6  */PFM_CMD_NONE,
-/* 7  */PFM_CMD_NONE,
-/* 8  */PFM_CMD(pfm_context_create, PFM_CMD_ARG_RW, 1, pfarg_context_t, pfm_ctx_getsize),
-/* 9  */PFM_CMD_NONE,
-/* 10 */PFM_CMD_S(pfm_restart, PFM_CMD_PCLRW),
-/* 11 */PFM_CMD_NONE,
-/* 12 */PFM_CMD(pfm_get_features, PFM_CMD_ARG_RW, 1, pfarg_features_t, NULL),
-/* 13 */PFM_CMD(pfm_debug, 0, 1, unsigned int, NULL),
-/* 14 */PFM_CMD_NONE,
-/* 15 */PFM_CMD(pfm_get_pmc_reset, PFM_CMD_ARG_RW, PFM_CMD_ARG_MANY, pfarg_reg_t, NULL),
-/* 16 */PFM_CMD(pfm_context_load, PFM_CMD_PCLRWS, 1, pfarg_load_t, NULL),
-/* 17 */PFM_CMD_S(pfm_context_unload, PFM_CMD_PCLRWS),
-/* 18 */PFM_CMD_NONE,
-/* 19 */PFM_CMD_NONE,
-/* 20 */PFM_CMD_NONE,
-/* 21 */PFM_CMD_NONE,
-/* 22 */PFM_CMD_NONE,
-/* 23 */PFM_CMD_NONE,
-/* 24 */PFM_CMD_NONE,
-/* 25 */PFM_CMD_NONE,
-/* 26 */PFM_CMD_NONE,
-/* 27 */PFM_CMD_NONE,
-/* 28 */PFM_CMD_NONE,
-/* 29 */PFM_CMD_NONE,
-/* 30 */PFM_CMD_NONE,
-/* 31 */PFM_CMD_NONE,
-/* 32 */PFM_CMD(pfm_write_ibrs, PFM_CMD_PCLRWS, PFM_CMD_ARG_MANY, pfarg_dbreg_t, NULL),
-/* 33 */PFM_CMD(pfm_write_dbrs, PFM_CMD_PCLRWS, PFM_CMD_ARG_MANY, pfarg_dbreg_t, NULL)
-};
-#define PFM_CMD_COUNT	(sizeof(pfm_cmd_tab)/sizeof(pfm_cmd_desc_t))
-
-static int
-pfm_check_task_state(pfm_context_t *ctx, int cmd, unsigned long flags)
-{
-	struct task_struct *task;
-	int state, old_state;
-
-recheck:
-	state = ctx->ctx_state;
-	task  = ctx->ctx_task;
-
-	if (task == NULL) {
-		DPRINT(("context %d no task, state=%d\n", ctx->ctx_fd, state));
-		return 0;
-	}
-
-	DPRINT(("context %d state=%d [%d] task_state=%ld must_stop=%d\n",
-		ctx->ctx_fd,
-		state,
-		task_pid_nr(task),
-		task->state, PFM_CMD_STOPPED(cmd)));
-
-	/*
-	 * self-monitoring always ok.
-	 *
-	 * for system-wide the caller can either be the creator of the
-	 * context (to one to which the context is attached to) OR
-	 * a task running on the same CPU as the session.
-	 */
-	if (task == current || ctx->ctx_fl_system) return 0;
-
-	/*
-	 * we are monitoring another thread
-	 */
-	switch(state) {
-		case PFM_CTX_UNLOADED:
-			/*
-			 * if context is UNLOADED we are safe to go
-			 */
-			return 0;
-		case PFM_CTX_ZOMBIE:
-			/*
-			 * no command can operate on a zombie context
-			 */
-			DPRINT(("cmd %d state zombie cannot operate on context\n", cmd));
-			return -EINVAL;
-		case PFM_CTX_MASKED:
-			/*
-			 * PMU state has been saved to software even though
-			 * the thread may still be running.
-			 */
-			if (cmd != PFM_UNLOAD_CONTEXT) return 0;
-	}
-
-	/*
-	 * context is LOADED or MASKED. Some commands may need to have 
-	 * the task stopped.
-	 *
-	 * We could lift this restriction for UP but it would mean that
-	 * the user has no guarantee the task would not run between
-	 * two successive calls to perfmonctl(). That's probably OK.
-	 * If this user wants to ensure the task does not run, then
-	 * the task must be stopped.
-	 */
-	if (PFM_CMD_STOPPED(cmd)) {
-		if (!task_is_stopped_or_traced(task)) {
-			DPRINT(("[%d] task not in stopped state\n", task_pid_nr(task)));
-			return -EBUSY;
-		}
-		/*
-		 * task is now stopped, wait for ctxsw out
-		 *
-		 * This is an interesting point in the code.
-		 * We need to unprotect the context because
-		 * the pfm_save_regs() routines needs to grab
-		 * the same lock. There are danger in doing
-		 * this because it leaves a window open for
-		 * another task to get access to the context
-		 * and possibly change its state. The one thing
-		 * that is not possible is for the context to disappear
-		 * because we are protected by the VFS layer, i.e.,
-		 * get_fd()/put_fd().
-		 */
-		old_state = state;
-
-		UNPROTECT_CTX(ctx, flags);
-
-		wait_task_inactive(task, 0);
-
-		PROTECT_CTX(ctx, flags);
-
-		/*
-		 * we must recheck to verify if state has changed
-		 */
-		if (ctx->ctx_state != old_state) {
-			DPRINT(("old_state=%d new_state=%d\n", old_state, ctx->ctx_state));
-			goto recheck;
-		}
-	}
-	return 0;
-}
-
-/*
- * system-call entry point (must return long)
- */
-asmlinkage long
-sys_perfmonctl (int fd, int cmd, void __user *arg, int count)
-{
-	struct file *file = NULL;
-	pfm_context_t *ctx = NULL;
-	unsigned long flags = 0UL;
-	void *args_k = NULL;
-	long ret; /* will expand int return types */
-	size_t base_sz, sz, xtra_sz = 0;
-	int narg, completed_args = 0, call_made = 0, cmd_flags;
-	int (*func)(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs);
-	int (*getsize)(void *arg, size_t *sz);
-#define PFM_MAX_ARGSIZE	4096
-
-	/*
-	 * reject any call if perfmon was disabled at initialization
-	 */
-	if (unlikely(pmu_conf == NULL)) return -ENOSYS;
-
-	if (unlikely(cmd < 0 || cmd >= PFM_CMD_COUNT)) {
-		DPRINT(("invalid cmd=%d\n", cmd));
-		return -EINVAL;
-	}
-
-	func      = pfm_cmd_tab[cmd].cmd_func;
-	narg      = pfm_cmd_tab[cmd].cmd_narg;
-	base_sz   = pfm_cmd_tab[cmd].cmd_argsize;
-	getsize   = pfm_cmd_tab[cmd].cmd_getsize;
-	cmd_flags = pfm_cmd_tab[cmd].cmd_flags;
-
-	if (unlikely(func == NULL)) {
-		DPRINT(("invalid cmd=%d\n", cmd));
-		return -EINVAL;
-	}
-
-	DPRINT(("cmd=%s idx=%d narg=0x%x argsz=%lu count=%d\n",
-		PFM_CMD_NAME(cmd),
-		cmd,
-		narg,
-		base_sz,
-		count));
-
-	/*
-	 * check if number of arguments matches what the command expects
-	 */
-	if (unlikely((narg == PFM_CMD_ARG_MANY && count <= 0) || (narg > 0 && narg != count)))
-		return -EINVAL;
-
-restart_args:
-	sz = xtra_sz + base_sz*count;
-	/*
-	 * limit abuse to min page size
-	 */
-	if (unlikely(sz > PFM_MAX_ARGSIZE)) {
-		printk(KERN_ERR "perfmon: [%d] argument too big %lu\n", task_pid_nr(current), sz);
-		return -E2BIG;
-	}
-
-	/*
-	 * allocate default-sized argument buffer
-	 */
-	if (likely(count && args_k == NULL)) {
-		args_k = kmalloc(PFM_MAX_ARGSIZE, GFP_KERNEL);
-		if (args_k == NULL) return -ENOMEM;
-	}
-
-	ret = -EFAULT;
-
-	/*
-	 * copy arguments
-	 *
-	 * assume sz = 0 for command without parameters
-	 */
-	if (sz && copy_from_user(args_k, arg, sz)) {
-		DPRINT(("cannot copy_from_user %lu bytes @%p\n", sz, arg));
-		goto error_args;
-	}
-
-	/*
-	 * check if command supports extra parameters
-	 */
-	if (completed_args == 0 && getsize) {
-		/*
-		 * get extra parameters size (based on main argument)
-		 */
-		ret = (*getsize)(args_k, &xtra_sz);
-		if (ret) goto error_args;
-
-		completed_args = 1;
-
-		DPRINT(("restart_args sz=%lu xtra_sz=%lu\n", sz, xtra_sz));
-
-		/* retry if necessary */
-		if (likely(xtra_sz)) goto restart_args;
-	}
-
-	if (unlikely((cmd_flags & PFM_CMD_FD) == 0)) goto skip_fd;
-
-	ret = -EBADF;
-
-	file = fget(fd);
-	if (unlikely(file == NULL)) {
-		DPRINT(("invalid fd %d\n", fd));
-		goto error_args;
-	}
-	if (unlikely(PFM_IS_FILE(file) == 0)) {
-		DPRINT(("fd %d not related to perfmon\n", fd));
-		goto error_args;
-	}
-
-	ctx = (pfm_context_t *)file->private_data;
-	if (unlikely(ctx == NULL)) {
-		DPRINT(("no context for fd %d\n", fd));
-		goto error_args;
-	}
-	prefetch(&ctx->ctx_state);
-
-	PROTECT_CTX(ctx, flags);
-
-	/*
-	 * check task is stopped
-	 */
-	ret = pfm_check_task_state(ctx, cmd, flags);
-	if (unlikely(ret)) goto abort_locked;
-
-skip_fd:
-	ret = (*func)(ctx, args_k, count, task_pt_regs(current));
-
-	call_made = 1;
-
-abort_locked:
-	if (likely(ctx)) {
-		DPRINT(("context unlocked\n"));
-		UNPROTECT_CTX(ctx, flags);
-	}
-
-	/* copy argument back to user, if needed */
-	if (call_made && PFM_CMD_RW_ARG(cmd) && copy_to_user(arg, args_k, base_sz*count)) ret = -EFAULT;
-
-error_args:
-	if (file)
-		fput(file);
-
-	kfree(args_k);
-
-	DPRINT(("cmd=%s ret=%ld\n", PFM_CMD_NAME(cmd), ret));
-
-	return ret;
-}
-
-static void
-pfm_resume_after_ovfl(pfm_context_t *ctx, unsigned long ovfl_regs, struct pt_regs *regs)
-{
-	pfm_buffer_fmt_t *fmt = ctx->ctx_buf_fmt;
-	pfm_ovfl_ctrl_t rst_ctrl;
-	int state;
-	int ret = 0;
-
-	state = ctx->ctx_state;
-	/*
-	 * Unlock sampling buffer and reset index atomically
-	 * XXX: not really needed when blocking
-	 */
-	if (CTX_HAS_SMPL(ctx)) {
-
-		rst_ctrl.bits.mask_monitoring = 0;
-		rst_ctrl.bits.reset_ovfl_pmds = 0;
-
-		if (state == PFM_CTX_LOADED)
-			ret = pfm_buf_fmt_restart_active(fmt, current, &rst_ctrl, ctx->ctx_smpl_hdr, regs);
-		else
-			ret = pfm_buf_fmt_restart(fmt, current, &rst_ctrl, ctx->ctx_smpl_hdr, regs);
-	} else {
-		rst_ctrl.bits.mask_monitoring = 0;
-		rst_ctrl.bits.reset_ovfl_pmds = 1;
-	}
-
-	if (ret == 0) {
-		if (rst_ctrl.bits.reset_ovfl_pmds) {
-			pfm_reset_regs(ctx, &ovfl_regs, PFM_PMD_LONG_RESET);
-		}
-		if (rst_ctrl.bits.mask_monitoring == 0) {
-			DPRINT(("resuming monitoring\n"));
-			if (ctx->ctx_state == PFM_CTX_MASKED) pfm_restore_monitoring(current);
-		} else {
-			DPRINT(("stopping monitoring\n"));
-			//pfm_stop_monitoring(current, regs);
-		}
-		ctx->ctx_state = PFM_CTX_LOADED;
-	}
-}
-
-/*
- * context MUST BE LOCKED when calling
- * can only be called for current
- */
-static void
-pfm_context_force_terminate(pfm_context_t *ctx, struct pt_regs *regs)
-{
-	int ret;
-
-	DPRINT(("entering for [%d]\n", task_pid_nr(current)));
-
-	ret = pfm_context_unload(ctx, NULL, 0, regs);
-	if (ret) {
-		printk(KERN_ERR "pfm_context_force_terminate: [%d] unloaded failed with %d\n", task_pid_nr(current), ret);
-	}
-
-	/*
-	 * and wakeup controlling task, indicating we are now disconnected
-	 */
-	wake_up_interruptible(&ctx->ctx_zombieq);
-
-	/*
-	 * given that context is still locked, the controlling
-	 * task will only get access when we return from
-	 * pfm_handle_work().
-	 */
-}
-
-static int pfm_ovfl_notify_user(pfm_context_t *ctx, unsigned long ovfl_pmds);
-
- /*
-  * pfm_handle_work() can be called with interrupts enabled
-  * (TIF_NEED_RESCHED) or disabled. The down_interruptible
-  * call may sleep, therefore we must re-enable interrupts
-  * to avoid deadlocks. It is safe to do so because this function
-  * is called ONLY when returning to user level (pUStk=1), in which case
-  * there is no risk of kernel stack overflow due to deep
-  * interrupt nesting.
-  */
-void
-pfm_handle_work(void)
-{
-	pfm_context_t *ctx;
-	struct pt_regs *regs;
-	unsigned long flags, dummy_flags;
-	unsigned long ovfl_regs;
-	unsigned int reason;
-	int ret;
-
-	ctx = PFM_GET_CTX(current);
-	if (ctx == NULL) {
-		printk(KERN_ERR "perfmon: [%d] has no PFM context\n",
-			task_pid_nr(current));
-		return;
-	}
-
-	PROTECT_CTX(ctx, flags);
-
-	PFM_SET_WORK_PENDING(current, 0);
-
-	regs = task_pt_regs(current);
-
-	/*
-	 * extract reason for being here and clear
-	 */
-	reason = ctx->ctx_fl_trap_reason;
-	ctx->ctx_fl_trap_reason = PFM_TRAP_REASON_NONE;
-	ovfl_regs = ctx->ctx_ovfl_regs[0];
-
-	DPRINT(("reason=%d state=%d\n", reason, ctx->ctx_state));
-
-	/*
-	 * must be done before we check for simple-reset mode
-	 */
-	if (ctx->ctx_fl_going_zombie || ctx->ctx_state == PFM_CTX_ZOMBIE)
-		goto do_zombie;
-
-	//if (CTX_OVFL_NOBLOCK(ctx)) goto skip_blocking;
-	if (reason == PFM_TRAP_REASON_RESET)
-		goto skip_blocking;
-
-	/*
-	 * restore interrupt mask to what it was on entry.
-	 * Could be enabled/diasbled.
-	 */
-	UNPROTECT_CTX(ctx, flags);
-
-	/*
-	 * force interrupt enable because of down_interruptible()
-	 */
-	local_irq_enable();
-
-	DPRINT(("before block sleeping\n"));
-
-	/*
-	 * may go through without blocking on SMP systems
-	 * if restart has been received already by the time we call down()
-	 */
-	ret = wait_for_completion_interruptible(&ctx->ctx_restart_done);
-
-	DPRINT(("after block sleeping ret=%d\n", ret));
-
-	/*
-	 * lock context and mask interrupts again
-	 * We save flags into a dummy because we may have
-	 * altered interrupts mask compared to entry in this
-	 * function.
-	 */
-	PROTECT_CTX(ctx, dummy_flags);
-
-	/*
-	 * we need to read the ovfl_regs only after wake-up
-	 * because we may have had pfm_write_pmds() in between
-	 * and that can changed PMD values and therefore 
-	 * ovfl_regs is reset for these new PMD values.
-	 */
-	ovfl_regs = ctx->ctx_ovfl_regs[0];
-
-	if (ctx->ctx_fl_going_zombie) {
-do_zombie:
-		DPRINT(("context is zombie, bailing out\n"));
-		pfm_context_force_terminate(ctx, regs);
-		goto nothing_to_do;
-	}
-	/*
-	 * in case of interruption of down() we don't restart anything
-	 */
-	if (ret < 0)
-		goto nothing_to_do;
-
-skip_blocking:
-	pfm_resume_after_ovfl(ctx, ovfl_regs, regs);
-	ctx->ctx_ovfl_regs[0] = 0UL;
-
-nothing_to_do:
-	/*
-	 * restore flags as they were upon entry
-	 */
-	UNPROTECT_CTX(ctx, flags);
-}
-
-static int
-pfm_notify_user(pfm_context_t *ctx, pfm_msg_t *msg)
-{
-	if (ctx->ctx_state == PFM_CTX_ZOMBIE) {
-		DPRINT(("ignoring overflow notification, owner is zombie\n"));
-		return 0;
-	}
-
-	DPRINT(("waking up somebody\n"));
-
-	if (msg) wake_up_interruptible(&ctx->ctx_msgq_wait);
-
-	/*
-	 * safe, we are not in intr handler, nor in ctxsw when
-	 * we come here
-	 */
-	kill_fasync (&ctx->ctx_async_queue, SIGIO, POLL_IN);
-
-	return 0;
-}
-
-static int
-pfm_ovfl_notify_user(pfm_context_t *ctx, unsigned long ovfl_pmds)
-{
-	pfm_msg_t *msg = NULL;
-
-	if (ctx->ctx_fl_no_msg == 0) {
-		msg = pfm_get_new_msg(ctx);
-		if (msg == NULL) {
-			printk(KERN_ERR "perfmon: pfm_ovfl_notify_user no more notification msgs\n");
-			return -1;
-		}
-
-		msg->pfm_ovfl_msg.msg_type         = PFM_MSG_OVFL;
-		msg->pfm_ovfl_msg.msg_ctx_fd       = ctx->ctx_fd;
-		msg->pfm_ovfl_msg.msg_active_set   = 0;
-		msg->pfm_ovfl_msg.msg_ovfl_pmds[0] = ovfl_pmds;
-		msg->pfm_ovfl_msg.msg_ovfl_pmds[1] = 0UL;
-		msg->pfm_ovfl_msg.msg_ovfl_pmds[2] = 0UL;
-		msg->pfm_ovfl_msg.msg_ovfl_pmds[3] = 0UL;
-		msg->pfm_ovfl_msg.msg_tstamp       = 0UL;
-	}
-
-	DPRINT(("ovfl msg: msg=%p no_msg=%d fd=%d ovfl_pmds=0x%lx\n",
-		msg,
-		ctx->ctx_fl_no_msg,
-		ctx->ctx_fd,
-		ovfl_pmds));
-
-	return pfm_notify_user(ctx, msg);
-}
-
-static int
-pfm_end_notify_user(pfm_context_t *ctx)
-{
-	pfm_msg_t *msg;
-
-	msg = pfm_get_new_msg(ctx);
-	if (msg == NULL) {
-		printk(KERN_ERR "perfmon: pfm_end_notify_user no more notification msgs\n");
-		return -1;
-	}
-	/* no leak */
-	memset(msg, 0, sizeof(*msg));
-
-	msg->pfm_end_msg.msg_type    = PFM_MSG_END;
-	msg->pfm_end_msg.msg_ctx_fd  = ctx->ctx_fd;
-	msg->pfm_ovfl_msg.msg_tstamp = 0UL;
-
-	DPRINT(("end msg: msg=%p no_msg=%d ctx_fd=%d\n",
-		msg,
-		ctx->ctx_fl_no_msg,
-		ctx->ctx_fd));
-
-	return pfm_notify_user(ctx, msg);
-}
-
-/*
- * main overflow processing routine.
- * it can be called from the interrupt path or explicitly during the context switch code
- */
-static void pfm_overflow_handler(struct task_struct *task, pfm_context_t *ctx,
-				unsigned long pmc0, struct pt_regs *regs)
-{
-	pfm_ovfl_arg_t *ovfl_arg;
-	unsigned long mask;
-	unsigned long old_val, ovfl_val, new_val;
-	unsigned long ovfl_notify = 0UL, ovfl_pmds = 0UL, smpl_pmds = 0UL, reset_pmds;
-	unsigned long tstamp;
-	pfm_ovfl_ctrl_t	ovfl_ctrl;
-	unsigned int i, has_smpl;
-	int must_notify = 0;
-
-	if (unlikely(ctx->ctx_state == PFM_CTX_ZOMBIE)) goto stop_monitoring;
-
-	/*
-	 * sanity test. Should never happen
-	 */
-	if (unlikely((pmc0 & 0x1) == 0)) goto sanity_check;
-
-	tstamp   = ia64_get_itc();
-	mask     = pmc0 >> PMU_FIRST_COUNTER;
-	ovfl_val = pmu_conf->ovfl_val;
-	has_smpl = CTX_HAS_SMPL(ctx);
-
-	DPRINT_ovfl(("pmc0=0x%lx pid=%d iip=0x%lx, %s "
-		     "used_pmds=0x%lx\n",
-			pmc0,
-			task ? task_pid_nr(task): -1,
-			(regs ? regs->cr_iip : 0),
-			CTX_OVFL_NOBLOCK(ctx) ? "nonblocking" : "blocking",
-			ctx->ctx_used_pmds[0]));
-
-
-	/*
-	 * first we update the virtual counters
-	 * assume there was a prior ia64_srlz_d() issued
-	 */
-	for (i = PMU_FIRST_COUNTER; mask ; i++, mask >>= 1) {
-
-		/* skip pmd which did not overflow */
-		if ((mask & 0x1) == 0) continue;
-
-		/*
-		 * Note that the pmd is not necessarily 0 at this point as qualified events
-		 * may have happened before the PMU was frozen. The residual count is not
-		 * taken into consideration here but will be with any read of the pmd via
-		 * pfm_read_pmds().
-		 */
-		old_val              = new_val = ctx->ctx_pmds[i].val;
-		new_val             += 1 + ovfl_val;
-		ctx->ctx_pmds[i].val = new_val;
-
-		/*
-		 * check for overflow condition
-		 */
-		if (likely(old_val > new_val)) {
-			ovfl_pmds |= 1UL << i;
-			if (PMC_OVFL_NOTIFY(ctx, i)) ovfl_notify |= 1UL << i;
-		}
-
-		DPRINT_ovfl(("ctx_pmd[%d].val=0x%lx old_val=0x%lx pmd=0x%lx ovfl_pmds=0x%lx ovfl_notify=0x%lx\n",
-			i,
-			new_val,
-			old_val,
-			ia64_get_pmd(i) & ovfl_val,
-			ovfl_pmds,
-			ovfl_notify));
-	}
-
-	/*
-	 * there was no 64-bit overflow, nothing else to do
-	 */
-	if (ovfl_pmds == 0UL) return;
-
-	/* 
-	 * reset all control bits
-	 */
-	ovfl_ctrl.val = 0;
-	reset_pmds    = 0UL;
-
-	/*
-	 * if a sampling format module exists, then we "cache" the overflow by 
-	 * calling the module's handler() routine.
-	 */
-	if (has_smpl) {
-		unsigned long start_cycles, end_cycles;
-		unsigned long pmd_mask;
-		int j, k, ret = 0;
-		int this_cpu = smp_processor_id();
-
-		pmd_mask = ovfl_pmds >> PMU_FIRST_COUNTER;
-		ovfl_arg = &ctx->ctx_ovfl_arg;
-
-		prefetch(ctx->ctx_smpl_hdr);
-
-		for(i=PMU_FIRST_COUNTER; pmd_mask && ret == 0; i++, pmd_mask >>=1) {
-
-			mask = 1UL << i;
-
-			if ((pmd_mask & 0x1) == 0) continue;
-
-			ovfl_arg->ovfl_pmd      = (unsigned char )i;
-			ovfl_arg->ovfl_notify   = ovfl_notify & mask ? 1 : 0;
-			ovfl_arg->active_set    = 0;
-			ovfl_arg->ovfl_ctrl.val = 0; /* module must fill in all fields */
-			ovfl_arg->smpl_pmds[0]  = smpl_pmds = ctx->ctx_pmds[i].smpl_pmds[0];
-
-			ovfl_arg->pmd_value      = ctx->ctx_pmds[i].val;
-			ovfl_arg->pmd_last_reset = ctx->ctx_pmds[i].lval;
-			ovfl_arg->pmd_eventid    = ctx->ctx_pmds[i].eventid;
-
-			/*
-		 	 * copy values of pmds of interest. Sampling format may copy them
-		 	 * into sampling buffer.
-		 	 */
-			if (smpl_pmds) {
-				for(j=0, k=0; smpl_pmds; j++, smpl_pmds >>=1) {
-					if ((smpl_pmds & 0x1) == 0) continue;
-					ovfl_arg->smpl_pmds_values[k++] = PMD_IS_COUNTING(j) ?  pfm_read_soft_counter(ctx, j) : ia64_get_pmd(j);
-					DPRINT_ovfl(("smpl_pmd[%d]=pmd%u=0x%lx\n", k-1, j, ovfl_arg->smpl_pmds_values[k-1]));
-				}
-			}
-
-			pfm_stats[this_cpu].pfm_smpl_handler_calls++;
-
-			start_cycles = ia64_get_itc();
-
-			/*
-		 	 * call custom buffer format record (handler) routine
-		 	 */
-			ret = (*ctx->ctx_buf_fmt->fmt_handler)(task, ctx->ctx_smpl_hdr, ovfl_arg, regs, tstamp);
-
-			end_cycles = ia64_get_itc();
-
-			/*
-			 * For those controls, we take the union because they have
-			 * an all or nothing behavior.
-			 */
-			ovfl_ctrl.bits.notify_user     |= ovfl_arg->ovfl_ctrl.bits.notify_user;
-			ovfl_ctrl.bits.block_task      |= ovfl_arg->ovfl_ctrl.bits.block_task;
-			ovfl_ctrl.bits.mask_monitoring |= ovfl_arg->ovfl_ctrl.bits.mask_monitoring;
-			/*
-			 * build the bitmask of pmds to reset now
-			 */
-			if (ovfl_arg->ovfl_ctrl.bits.reset_ovfl_pmds) reset_pmds |= mask;
-
-			pfm_stats[this_cpu].pfm_smpl_handler_cycles += end_cycles - start_cycles;
-		}
-		/*
-		 * when the module cannot handle the rest of the overflows, we abort right here
-		 */
-		if (ret && pmd_mask) {
-			DPRINT(("handler aborts leftover ovfl_pmds=0x%lx\n",
-				pmd_mask<<PMU_FIRST_COUNTER));
-		}
-		/*
-		 * remove the pmds we reset now from the set of pmds to reset in pfm_restart()
-		 */
-		ovfl_pmds &= ~reset_pmds;
-	} else {
-		/*
-		 * when no sampling module is used, then the default
-		 * is to notify on overflow if requested by user
-		 */
-		ovfl_ctrl.bits.notify_user     = ovfl_notify ? 1 : 0;
-		ovfl_ctrl.bits.block_task      = ovfl_notify ? 1 : 0;
-		ovfl_ctrl.bits.mask_monitoring = ovfl_notify ? 1 : 0; /* XXX: change for saturation */
-		ovfl_ctrl.bits.reset_ovfl_pmds = ovfl_notify ? 0 : 1;
-		/*
-		 * if needed, we reset all overflowed pmds
-		 */
-		if (ovfl_notify == 0) reset_pmds = ovfl_pmds;
-	}
-
-	DPRINT_ovfl(("ovfl_pmds=0x%lx reset_pmds=0x%lx\n", ovfl_pmds, reset_pmds));
-
-	/*
-	 * reset the requested PMD registers using the short reset values
-	 */
-	if (reset_pmds) {
-		unsigned long bm = reset_pmds;
-		pfm_reset_regs(ctx, &bm, PFM_PMD_SHORT_RESET);
-	}
-
-	if (ovfl_notify && ovfl_ctrl.bits.notify_user) {
-		/*
-		 * keep track of what to reset when unblocking
-		 */
-		ctx->ctx_ovfl_regs[0] = ovfl_pmds;
-
-		/*
-		 * check for blocking context 
-		 */
-		if (CTX_OVFL_NOBLOCK(ctx) == 0 && ovfl_ctrl.bits.block_task) {
-
-			ctx->ctx_fl_trap_reason = PFM_TRAP_REASON_BLOCK;
-
-			/*
-			 * set the perfmon specific checking pending work for the task
-			 */
-			PFM_SET_WORK_PENDING(task, 1);
-
-			/*
-			 * when coming from ctxsw, current still points to the
-			 * previous task, therefore we must work with task and not current.
-			 */
-			set_notify_resume(task);
-		}
-		/*
-		 * defer until state is changed (shorten spin window). the context is locked
-		 * anyway, so the signal receiver would come spin for nothing.
-		 */
-		must_notify = 1;
-	}
-
-	DPRINT_ovfl(("owner [%d] pending=%ld reason=%u ovfl_pmds=0x%lx ovfl_notify=0x%lx masked=%d\n",
-			GET_PMU_OWNER() ? task_pid_nr(GET_PMU_OWNER()) : -1,
-			PFM_GET_WORK_PENDING(task),
-			ctx->ctx_fl_trap_reason,
-			ovfl_pmds,
-			ovfl_notify,
-			ovfl_ctrl.bits.mask_monitoring ? 1 : 0));
-	/*
-	 * in case monitoring must be stopped, we toggle the psr bits
-	 */
-	if (ovfl_ctrl.bits.mask_monitoring) {
-		pfm_mask_monitoring(task);
-		ctx->ctx_state = PFM_CTX_MASKED;
-		ctx->ctx_fl_can_restart = 1;
-	}
-
-	/*
-	 * send notification now
-	 */
-	if (must_notify) pfm_ovfl_notify_user(ctx, ovfl_notify);
-
-	return;
-
-sanity_check:
-	printk(KERN_ERR "perfmon: CPU%d overflow handler [%d] pmc0=0x%lx\n",
-			smp_processor_id(),
-			task ? task_pid_nr(task) : -1,
-			pmc0);
-	return;
-
-stop_monitoring:
-	/*
-	 * in SMP, zombie context is never restored but reclaimed in pfm_load_regs().
-	 * Moreover, zombies are also reclaimed in pfm_save_regs(). Therefore we can
-	 * come here as zombie only if the task is the current task. In which case, we
-	 * can access the PMU  hardware directly.
-	 *
-	 * Note that zombies do have PM_VALID set. So here we do the minimal.
-	 *
-	 * In case the context was zombified it could not be reclaimed at the time
-	 * the monitoring program exited. At this point, the PMU reservation has been
-	 * returned, the sampiing buffer has been freed. We must convert this call
-	 * into a spurious interrupt. However, we must also avoid infinite overflows
-	 * by stopping monitoring for this task. We can only come here for a per-task
-	 * context. All we need to do is to stop monitoring using the psr bits which
-	 * are always task private. By re-enabling secure montioring, we ensure that
-	 * the monitored task will not be able to re-activate monitoring.
-	 * The task will eventually be context switched out, at which point the context
-	 * will be reclaimed (that includes releasing ownership of the PMU).
-	 *
-	 * So there might be a window of time where the number of per-task session is zero
-	 * yet one PMU might have a owner and get at most one overflow interrupt for a zombie
-	 * context. This is safe because if a per-task session comes in, it will push this one
-	 * out and by the virtue on pfm_save_regs(), this one will disappear. If a system wide
-	 * session is force on that CPU, given that we use task pinning, pfm_save_regs() will
-	 * also push our zombie context out.
-	 *
-	 * Overall pretty hairy stuff....
-	 */
-	DPRINT(("ctx is zombie for [%d], converted to spurious\n", task ? task_pid_nr(task): -1));
-	pfm_clear_psr_up();
-	ia64_psr(regs)->up = 0;
-	ia64_psr(regs)->sp = 1;
-	return;
-}
-
-static int
-pfm_do_interrupt_handler(void *arg, struct pt_regs *regs)
-{
-	struct task_struct *task;
-	pfm_context_t *ctx;
-	unsigned long flags;
-	u64 pmc0;
-	int this_cpu = smp_processor_id();
-	int retval = 0;
-
-	pfm_stats[this_cpu].pfm_ovfl_intr_count++;
-
-	/*
-	 * srlz.d done before arriving here
-	 */
-	pmc0 = ia64_get_pmc(0);
-
-	task = GET_PMU_OWNER();
-	ctx  = GET_PMU_CTX();
-
-	/*
-	 * if we have some pending bits set
-	 * assumes : if any PMC0.bit[63-1] is set, then PMC0.fr = 1
-	 */
-	if (PMC0_HAS_OVFL(pmc0) && task) {
-		/*
-		 * we assume that pmc0.fr is always set here
-		 */
-
-		/* sanity check */
-		if (!ctx) goto report_spurious1;
-
-		if (ctx->ctx_fl_system == 0 && (task->thread.flags & IA64_THREAD_PM_VALID) == 0) 
-			goto report_spurious2;
-
-		PROTECT_CTX_NOPRINT(ctx, flags);
-
-		pfm_overflow_handler(task, ctx, pmc0, regs);
-
-		UNPROTECT_CTX_NOPRINT(ctx, flags);
-
-	} else {
-		pfm_stats[this_cpu].pfm_spurious_ovfl_intr_count++;
-		retval = -1;
-	}
-	/*
-	 * keep it unfrozen at all times
-	 */
-	pfm_unfreeze_pmu();
-
-	return retval;
-
-report_spurious1:
-	printk(KERN_INFO "perfmon: spurious overflow interrupt on CPU%d: process %d has no PFM context\n",
-		this_cpu, task_pid_nr(task));
-	pfm_unfreeze_pmu();
-	return -1;
-report_spurious2:
-	printk(KERN_INFO "perfmon: spurious overflow interrupt on CPU%d: process %d, invalid flag\n", 
-		this_cpu, 
-		task_pid_nr(task));
-	pfm_unfreeze_pmu();
-	return -1;
-}
-
-static irqreturn_t
-pfm_interrupt_handler(int irq, void *arg)
-{
-	unsigned long start_cycles, total_cycles;
-	unsigned long min, max;
-	int this_cpu;
-	int ret;
-	struct pt_regs *regs = get_irq_regs();
-
-	this_cpu = get_cpu();
-	if (likely(!pfm_alt_intr_handler)) {
-		min = pfm_stats[this_cpu].pfm_ovfl_intr_cycles_min;
-		max = pfm_stats[this_cpu].pfm_ovfl_intr_cycles_max;
-
-		start_cycles = ia64_get_itc();
-
-		ret = pfm_do_interrupt_handler(arg, regs);
-
-		total_cycles = ia64_get_itc();
-
-		/*
-		 * don't measure spurious interrupts
-		 */
-		if (likely(ret == 0)) {
-			total_cycles -= start_cycles;
-
-			if (total_cycles < min) pfm_stats[this_cpu].pfm_ovfl_intr_cycles_min = total_cycles;
-			if (total_cycles > max) pfm_stats[this_cpu].pfm_ovfl_intr_cycles_max = total_cycles;
-
-			pfm_stats[this_cpu].pfm_ovfl_intr_cycles += total_cycles;
-		}
-	}
-	else {
-		(*pfm_alt_intr_handler->handler)(irq, arg, regs);
-	}
-
-	put_cpu();
-	return IRQ_HANDLED;
-}
-
-/*
- * /proc/perfmon interface, for debug only
- */
-
-#define PFM_PROC_SHOW_HEADER	((void *)(long)nr_cpu_ids+1)
-
-static void *
-pfm_proc_start(struct seq_file *m, loff_t *pos)
-{
-	if (*pos == 0) {
-		return PFM_PROC_SHOW_HEADER;
-	}
-
-	while (*pos <= nr_cpu_ids) {
-		if (cpu_online(*pos - 1)) {
-			return (void *)*pos;
-		}
-		++*pos;
-	}
-	return NULL;
-}
-
-static void *
-pfm_proc_next(struct seq_file *m, void *v, loff_t *pos)
-{
-	++*pos;
-	return pfm_proc_start(m, pos);
-}
-
-static void
-pfm_proc_stop(struct seq_file *m, void *v)
-{
-}
-
-static void
-pfm_proc_show_header(struct seq_file *m)
-{
-	struct list_head * pos;
-	pfm_buffer_fmt_t * entry;
-	unsigned long flags;
-
- 	seq_printf(m,
-		"perfmon version           : %u.%u\n"
-		"model                     : %s\n"
-		"fastctxsw                 : %s\n"
-		"expert mode               : %s\n"
-		"ovfl_mask                 : 0x%lx\n"
-		"PMU flags                 : 0x%x\n",
-		PFM_VERSION_MAJ, PFM_VERSION_MIN,
-		pmu_conf->pmu_name,
-		pfm_sysctl.fastctxsw > 0 ? "Yes": "No",
-		pfm_sysctl.expert_mode > 0 ? "Yes": "No",
-		pmu_conf->ovfl_val,
-		pmu_conf->flags);
-
-  	LOCK_PFS(flags);
-
- 	seq_printf(m,
- 		"proc_sessions             : %u\n"
- 		"sys_sessions              : %u\n"
- 		"sys_use_dbregs            : %u\n"
- 		"ptrace_use_dbregs         : %u\n",
- 		pfm_sessions.pfs_task_sessions,
- 		pfm_sessions.pfs_sys_sessions,
- 		pfm_sessions.pfs_sys_use_dbregs,
- 		pfm_sessions.pfs_ptrace_use_dbregs);
-
-  	UNLOCK_PFS(flags);
-
-	spin_lock(&pfm_buffer_fmt_lock);
-
-	list_for_each(pos, &pfm_buffer_fmt_list) {
-		entry = list_entry(pos, pfm_buffer_fmt_t, fmt_list);
-		seq_printf(m, "format                    : %02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x-%02x %s\n",
-			entry->fmt_uuid[0],
-			entry->fmt_uuid[1],
-			entry->fmt_uuid[2],
-			entry->fmt_uuid[3],
-			entry->fmt_uuid[4],
-			entry->fmt_uuid[5],
-			entry->fmt_uuid[6],
-			entry->fmt_uuid[7],
-			entry->fmt_uuid[8],
-			entry->fmt_uuid[9],
-			entry->fmt_uuid[10],
-			entry->fmt_uuid[11],
-			entry->fmt_uuid[12],
-			entry->fmt_uuid[13],
-			entry->fmt_uuid[14],
-			entry->fmt_uuid[15],
-			entry->fmt_name);
-	}
-	spin_unlock(&pfm_buffer_fmt_lock);
-
-}
-
-static int
-pfm_proc_show(struct seq_file *m, void *v)
-{
-	unsigned long psr;
-	unsigned int i;
-	int cpu;
-
-	if (v == PFM_PROC_SHOW_HEADER) {
-		pfm_proc_show_header(m);
-		return 0;
-	}
-
-	/* show info for CPU (v - 1) */
-
-	cpu = (long)v - 1;
-	seq_printf(m,
-		"CPU%-2d overflow intrs      : %lu\n"
-		"CPU%-2d overflow cycles     : %lu\n"
-		"CPU%-2d overflow min        : %lu\n"
-		"CPU%-2d overflow max        : %lu\n"
-		"CPU%-2d smpl handler calls  : %lu\n"
-		"CPU%-2d smpl handler cycles : %lu\n"
-		"CPU%-2d spurious intrs      : %lu\n"
-		"CPU%-2d replay   intrs      : %lu\n"
-		"CPU%-2d syst_wide           : %d\n"
-		"CPU%-2d dcr_pp              : %d\n"
-		"CPU%-2d exclude idle        : %d\n"
-		"CPU%-2d owner               : %d\n"
-		"CPU%-2d context             : %p\n"
-		"CPU%-2d activations         : %lu\n",
-		cpu, pfm_stats[cpu].pfm_ovfl_intr_count,
-		cpu, pfm_stats[cpu].pfm_ovfl_intr_cycles,
-		cpu, pfm_stats[cpu].pfm_ovfl_intr_cycles_min,
-		cpu, pfm_stats[cpu].pfm_ovfl_intr_cycles_max,
-		cpu, pfm_stats[cpu].pfm_smpl_handler_calls,
-		cpu, pfm_stats[cpu].pfm_smpl_handler_cycles,
-		cpu, pfm_stats[cpu].pfm_spurious_ovfl_intr_count,
-		cpu, pfm_stats[cpu].pfm_replay_ovfl_intr_count,
-		cpu, pfm_get_cpu_data(pfm_syst_info, cpu) & PFM_CPUINFO_SYST_WIDE ? 1 : 0,
-		cpu, pfm_get_cpu_data(pfm_syst_info, cpu) & PFM_CPUINFO_DCR_PP ? 1 : 0,
-		cpu, pfm_get_cpu_data(pfm_syst_info, cpu) & PFM_CPUINFO_EXCL_IDLE ? 1 : 0,
-		cpu, pfm_get_cpu_data(pmu_owner, cpu) ? pfm_get_cpu_data(pmu_owner, cpu)->pid: -1,
-		cpu, pfm_get_cpu_data(pmu_ctx, cpu),
-		cpu, pfm_get_cpu_data(pmu_activation_number, cpu));
-
-	if (num_online_cpus() == 1 && pfm_sysctl.debug > 0) {
-
-		psr = pfm_get_psr();
-
-		ia64_srlz_d();
-
-		seq_printf(m, 
-			"CPU%-2d psr                 : 0x%lx\n"
-			"CPU%-2d pmc0                : 0x%lx\n", 
-			cpu, psr,
-			cpu, ia64_get_pmc(0));
-
-		for (i=0; PMC_IS_LAST(i) == 0;  i++) {
-			if (PMC_IS_COUNTING(i) == 0) continue;
-   			seq_printf(m, 
-				"CPU%-2d pmc%u                : 0x%lx\n"
-   				"CPU%-2d pmd%u                : 0x%lx\n", 
-				cpu, i, ia64_get_pmc(i),
-				cpu, i, ia64_get_pmd(i));
-  		}
-	}
-	return 0;
-}
-
-const struct seq_operations pfm_seq_ops = {
-	.start =	pfm_proc_start,
- 	.next =		pfm_proc_next,
- 	.stop =		pfm_proc_stop,
- 	.show =		pfm_proc_show
-};
-
-static int
-pfm_proc_open(struct inode *inode, struct file *file)
-{
-	return seq_open(file, &pfm_seq_ops);
-}
-
-
-/*
- * we come here as soon as local_cpu_data->pfm_syst_wide is set. this happens
- * during pfm_enable() hence before pfm_start(). We cannot assume monitoring
- * is active or inactive based on mode. We must rely on the value in
- * local_cpu_data->pfm_syst_info
- */
-void
-pfm_syst_wide_update_task(struct task_struct *task, unsigned long info, int is_ctxswin)
-{
-	struct pt_regs *regs;
-	unsigned long dcr;
-	unsigned long dcr_pp;
-
-	dcr_pp = info & PFM_CPUINFO_DCR_PP ? 1 : 0;
-
-	/*
-	 * pid 0 is guaranteed to be the idle task. There is one such task with pid 0
-	 * on every CPU, so we can rely on the pid to identify the idle task.
-	 */
-	if ((info & PFM_CPUINFO_EXCL_IDLE) == 0 || task->pid) {
-		regs = task_pt_regs(task);
-		ia64_psr(regs)->pp = is_ctxswin ? dcr_pp : 0;
-		return;
-	}
-	/*
-	 * if monitoring has started
-	 */
-	if (dcr_pp) {
-		dcr = ia64_getreg(_IA64_REG_CR_DCR);
-		/*
-		 * context switching in?
-		 */
-		if (is_ctxswin) {
-			/* mask monitoring for the idle task */
-			ia64_setreg(_IA64_REG_CR_DCR, dcr & ~IA64_DCR_PP);
-			pfm_clear_psr_pp();
-			ia64_srlz_i();
-			return;
-		}
-		/*
-		 * context switching out
-		 * restore monitoring for next task
-		 *
-		 * Due to inlining this odd if-then-else construction generates
-		 * better code.
-		 */
-		ia64_setreg(_IA64_REG_CR_DCR, dcr |IA64_DCR_PP);
-		pfm_set_psr_pp();
-		ia64_srlz_i();
-	}
-}
-
-#ifdef CONFIG_SMP
-
-static void
-pfm_force_cleanup(pfm_context_t *ctx, struct pt_regs *regs)
-{
-	struct task_struct *task = ctx->ctx_task;
-
-	ia64_psr(regs)->up = 0;
-	ia64_psr(regs)->sp = 1;
-
-	if (GET_PMU_OWNER() == task) {
-		DPRINT(("cleared ownership for [%d]\n",
-					task_pid_nr(ctx->ctx_task)));
-		SET_PMU_OWNER(NULL, NULL);
-	}
-
-	/*
-	 * disconnect the task from the context and vice-versa
-	 */
-	PFM_SET_WORK_PENDING(task, 0);
-
-	task->thread.pfm_context  = NULL;
-	task->thread.flags       &= ~IA64_THREAD_PM_VALID;
-
-	DPRINT(("force cleanup for [%d]\n",  task_pid_nr(task)));
-}
-
-
-/*
- * in 2.6, interrupts are masked when we come here and the runqueue lock is held
- */
-void
-pfm_save_regs(struct task_struct *task)
-{
-	pfm_context_t *ctx;
-	unsigned long flags;
-	u64 psr;
-
-
-	ctx = PFM_GET_CTX(task);
-	if (ctx == NULL) return;
-
-	/*
- 	 * we always come here with interrupts ALREADY disabled by
- 	 * the scheduler. So we simply need to protect against concurrent
-	 * access, not CPU concurrency.
-	 */
-	flags = pfm_protect_ctx_ctxsw(ctx);
-
-	if (ctx->ctx_state == PFM_CTX_ZOMBIE) {
-		struct pt_regs *regs = task_pt_regs(task);
-
-		pfm_clear_psr_up();
-
-		pfm_force_cleanup(ctx, regs);
-
-		BUG_ON(ctx->ctx_smpl_hdr);
-
-		pfm_unprotect_ctx_ctxsw(ctx, flags);
-
-		pfm_context_free(ctx);
-		return;
-	}
-
-	/*
-	 * save current PSR: needed because we modify it
-	 */
-	ia64_srlz_d();
-	psr = pfm_get_psr();
-
-	BUG_ON(psr & (IA64_PSR_I));
-
-	/*
-	 * stop monitoring:
-	 * This is the last instruction which may generate an overflow
-	 *
-	 * We do not need to set psr.sp because, it is irrelevant in kernel.
-	 * It will be restored from ipsr when going back to user level
-	 */
-	pfm_clear_psr_up();
-
-	/*
-	 * keep a copy of psr.up (for reload)
-	 */
-	ctx->ctx_saved_psr_up = psr & IA64_PSR_UP;
-
-	/*
-	 * release ownership of this PMU.
-	 * PM interrupts are masked, so nothing
-	 * can happen.
-	 */
-	SET_PMU_OWNER(NULL, NULL);
-
-	/*
-	 * we systematically save the PMD as we have no
-	 * guarantee we will be schedule at that same
-	 * CPU again.
-	 */
-	pfm_save_pmds(ctx->th_pmds, ctx->ctx_used_pmds[0]);
-
-	/*
-	 * save pmc0 ia64_srlz_d() done in pfm_save_pmds()
-	 * we will need it on the restore path to check
-	 * for pending overflow.
-	 */
-	ctx->th_pmcs[0] = ia64_get_pmc(0);
-
-	/*
-	 * unfreeze PMU if had pending overflows
-	 */
-	if (ctx->th_pmcs[0] & ~0x1UL) pfm_unfreeze_pmu();
-
-	/*
-	 * finally, allow context access.
-	 * interrupts will still be masked after this call.
-	 */
-	pfm_unprotect_ctx_ctxsw(ctx, flags);
-}
-
-#else /* !CONFIG_SMP */
-void
-pfm_save_regs(struct task_struct *task)
-{
-	pfm_context_t *ctx;
-	u64 psr;
-
-	ctx = PFM_GET_CTX(task);
-	if (ctx == NULL) return;
-
-	/*
-	 * save current PSR: needed because we modify it
-	 */
-	psr = pfm_get_psr();
-
-	BUG_ON(psr & (IA64_PSR_I));
-
-	/*
-	 * stop monitoring:
-	 * This is the last instruction which may generate an overflow
-	 *
-	 * We do not need to set psr.sp because, it is irrelevant in kernel.
-	 * It will be restored from ipsr when going back to user level
-	 */
-	pfm_clear_psr_up();
-
-	/*
-	 * keep a copy of psr.up (for reload)
-	 */
-	ctx->ctx_saved_psr_up = psr & IA64_PSR_UP;
-}
-
-static void
-pfm_lazy_save_regs (struct task_struct *task)
-{
-	pfm_context_t *ctx;
-	unsigned long flags;
-
-	{ u64 psr  = pfm_get_psr();
-	  BUG_ON(psr & IA64_PSR_UP);
-	}
-
-	ctx = PFM_GET_CTX(task);
-
-	/*
-	 * we need to mask PMU overflow here to
-	 * make sure that we maintain pmc0 until
-	 * we save it. overflow interrupts are
-	 * treated as spurious if there is no
-	 * owner.
-	 *
-	 * XXX: I don't think this is necessary
-	 */
-	PROTECT_CTX(ctx,flags);
-
-	/*
-	 * release ownership of this PMU.
-	 * must be done before we save the registers.
-	 *
-	 * after this call any PMU interrupt is treated
-	 * as spurious.
-	 */
-	SET_PMU_OWNER(NULL, NULL);
-
-	/*
-	 * save all the pmds we use
-	 */
-	pfm_save_pmds(ctx->th_pmds, ctx->ctx_used_pmds[0]);
-
-	/*
-	 * save pmc0 ia64_srlz_d() done in pfm_save_pmds()
-	 * it is needed to check for pended overflow
-	 * on the restore path
-	 */
-	ctx->th_pmcs[0] = ia64_get_pmc(0);
-
-	/*
-	 * unfreeze PMU if had pending overflows
-	 */
-	if (ctx->th_pmcs[0] & ~0x1UL) pfm_unfreeze_pmu();
-
-	/*
-	 * now get can unmask PMU interrupts, they will
-	 * be treated as purely spurious and we will not
-	 * lose any information
-	 */
-	UNPROTECT_CTX(ctx,flags);
-}
-#endif /* CONFIG_SMP */
-
-#ifdef CONFIG_SMP
-/*
- * in 2.6, interrupts are masked when we come here and the runqueue lock is held
- */
-void
-pfm_load_regs (struct task_struct *task)
-{
-	pfm_context_t *ctx;
-	unsigned long pmc_mask = 0UL, pmd_mask = 0UL;
-	unsigned long flags;
-	u64 psr, psr_up;
-	int need_irq_resend;
-
-	ctx = PFM_GET_CTX(task);
-	if (unlikely(ctx == NULL)) return;
-
-	BUG_ON(GET_PMU_OWNER());
-
-	/*
-	 * possible on unload
-	 */
-	if (unlikely((task->thread.flags & IA64_THREAD_PM_VALID) == 0)) return;
-
-	/*
- 	 * we always come here with interrupts ALREADY disabled by
- 	 * the scheduler. So we simply need to protect against concurrent
-	 * access, not CPU concurrency.
-	 */
-	flags = pfm_protect_ctx_ctxsw(ctx);
-	psr   = pfm_get_psr();
-
-	need_irq_resend = pmu_conf->flags & PFM_PMU_IRQ_RESEND;
-
-	BUG_ON(psr & (IA64_PSR_UP|IA64_PSR_PP));
-	BUG_ON(psr & IA64_PSR_I);
-
-	if (unlikely(ctx->ctx_state == PFM_CTX_ZOMBIE)) {
-		struct pt_regs *regs = task_pt_regs(task);
-
-		BUG_ON(ctx->ctx_smpl_hdr);
-
-		pfm_force_cleanup(ctx, regs);
-
-		pfm_unprotect_ctx_ctxsw(ctx, flags);
-
-		/*
-		 * this one (kmalloc'ed) is fine with interrupts disabled
-		 */
-		pfm_context_free(ctx);
-
-		return;
-	}
-
-	/*
-	 * we restore ALL the debug registers to avoid picking up
-	 * stale state.
-	 */
-	if (ctx->ctx_fl_using_dbreg) {
-		pfm_restore_ibrs(ctx->ctx_ibrs, pmu_conf->num_ibrs);
-		pfm_restore_dbrs(ctx->ctx_dbrs, pmu_conf->num_dbrs);
-	}
-	/*
-	 * retrieve saved psr.up
-	 */
-	psr_up = ctx->ctx_saved_psr_up;
-
-	/*
-	 * if we were the last user of the PMU on that CPU,
-	 * then nothing to do except restore psr
-	 */
-	if (GET_LAST_CPU(ctx) == smp_processor_id() && ctx->ctx_last_activation == GET_ACTIVATION()) {
-
-		/*
-		 * retrieve partial reload masks (due to user modifications)
-		 */
-		pmc_mask = ctx->ctx_reload_pmcs[0];
-		pmd_mask = ctx->ctx_reload_pmds[0];
-
-	} else {
-		/*
-	 	 * To avoid leaking information to the user level when psr.sp=0,
-	 	 * we must reload ALL implemented pmds (even the ones we don't use).
-	 	 * In the kernel we only allow PFM_READ_PMDS on registers which
-	 	 * we initialized or requested (sampling) so there is no risk there.
-	 	 */
-		pmd_mask = pfm_sysctl.fastctxsw ?  ctx->ctx_used_pmds[0] : ctx->ctx_all_pmds[0];
-
-		/*
-	 	 * ALL accessible PMCs are systematically reloaded, unused registers
-	 	 * get their default (from pfm_reset_pmu_state()) values to avoid picking
-	 	 * up stale configuration.
-	 	 *
-	 	 * PMC0 is never in the mask. It is always restored separately.
-	 	 */
-		pmc_mask = ctx->ctx_all_pmcs[0];
-	}
-	/*
-	 * when context is MASKED, we will restore PMC with plm=0
-	 * and PMD with stale information, but that's ok, nothing
-	 * will be captured.
-	 *
-	 * XXX: optimize here
-	 */
-	if (pmd_mask) pfm_restore_pmds(ctx->th_pmds, pmd_mask);
-	if (pmc_mask) pfm_restore_pmcs(ctx->th_pmcs, pmc_mask);
-
-	/*
-	 * check for pending overflow at the time the state
-	 * was saved.
-	 */
-	if (unlikely(PMC0_HAS_OVFL(ctx->th_pmcs[0]))) {
-		/*
-		 * reload pmc0 with the overflow information
-		 * On McKinley PMU, this will trigger a PMU interrupt
-		 */
-		ia64_set_pmc(0, ctx->th_pmcs[0]);
-		ia64_srlz_d();
-		ctx->th_pmcs[0] = 0UL;
-
-		/*
-		 * will replay the PMU interrupt
-		 */
-		if (need_irq_resend) ia64_resend_irq(IA64_PERFMON_VECTOR);
-
-		pfm_stats[smp_processor_id()].pfm_replay_ovfl_intr_count++;
-	}
-
-	/*
-	 * we just did a reload, so we reset the partial reload fields
-	 */
-	ctx->ctx_reload_pmcs[0] = 0UL;
-	ctx->ctx_reload_pmds[0] = 0UL;
-
-	SET_LAST_CPU(ctx, smp_processor_id());
-
-	/*
-	 * dump activation value for this PMU
-	 */
-	INC_ACTIVATION();
-	/*
-	 * record current activation for this context
-	 */
-	SET_ACTIVATION(ctx);
-
-	/*
-	 * establish new ownership. 
-	 */
-	SET_PMU_OWNER(task, ctx);
-
-	/*
-	 * restore the psr.up bit. measurement
-	 * is active again.
-	 * no PMU interrupt can happen at this point
-	 * because we still have interrupts disabled.
-	 */
-	if (likely(psr_up)) pfm_set_psr_up();
-
-	/*
-	 * allow concurrent access to context
-	 */
-	pfm_unprotect_ctx_ctxsw(ctx, flags);
-}
-#else /*  !CONFIG_SMP */
-/*
- * reload PMU state for UP kernels
- * in 2.5 we come here with interrupts disabled
- */
-void
-pfm_load_regs (struct task_struct *task)
-{
-	pfm_context_t *ctx;
-	struct task_struct *owner;
-	unsigned long pmd_mask, pmc_mask;
-	u64 psr, psr_up;
-	int need_irq_resend;
-
-	owner = GET_PMU_OWNER();
-	ctx   = PFM_GET_CTX(task);
-	psr   = pfm_get_psr();
-
-	BUG_ON(psr & (IA64_PSR_UP|IA64_PSR_PP));
-	BUG_ON(psr & IA64_PSR_I);
-
-	/*
-	 * we restore ALL the debug registers to avoid picking up
-	 * stale state.
-	 *
-	 * This must be done even when the task is still the owner
-	 * as the registers may have been modified via ptrace()
-	 * (not perfmon) by the previous task.
-	 */
-	if (ctx->ctx_fl_using_dbreg) {
-		pfm_restore_ibrs(ctx->ctx_ibrs, pmu_conf->num_ibrs);
-		pfm_restore_dbrs(ctx->ctx_dbrs, pmu_conf->num_dbrs);
-	}
-
-	/*
-	 * retrieved saved psr.up
-	 */
-	psr_up = ctx->ctx_saved_psr_up;
-	need_irq_resend = pmu_conf->flags & PFM_PMU_IRQ_RESEND;
-
-	/*
-	 * short path, our state is still there, just
-	 * need to restore psr and we go
-	 *
-	 * we do not touch either PMC nor PMD. the psr is not touched
-	 * by the overflow_handler. So we are safe w.r.t. to interrupt
-	 * concurrency even without interrupt masking.
-	 */
-	if (likely(owner == task)) {
-		if (likely(psr_up)) pfm_set_psr_up();
-		return;
-	}
-
-	/*
-	 * someone else is still using the PMU, first push it out and
-	 * then we'll be able to install our stuff !
-	 *
-	 * Upon return, there will be no owner for the current PMU
-	 */
-	if (owner) pfm_lazy_save_regs(owner);
-
-	/*
-	 * To avoid leaking information to the user level when psr.sp=0,
-	 * we must reload ALL implemented pmds (even the ones we don't use).
-	 * In the kernel we only allow PFM_READ_PMDS on registers which
-	 * we initialized or requested (sampling) so there is no risk there.
-	 */
-	pmd_mask = pfm_sysctl.fastctxsw ?  ctx->ctx_used_pmds[0] : ctx->ctx_all_pmds[0];
-
-	/*
-	 * ALL accessible PMCs are systematically reloaded, unused registers
-	 * get their default (from pfm_reset_pmu_state()) values to avoid picking
-	 * up stale configuration.
-	 *
-	 * PMC0 is never in the mask. It is always restored separately
-	 */
-	pmc_mask = ctx->ctx_all_pmcs[0];
-
-	pfm_restore_pmds(ctx->th_pmds, pmd_mask);
-	pfm_restore_pmcs(ctx->th_pmcs, pmc_mask);
-
-	/*
-	 * check for pending overflow at the time the state
-	 * was saved.
-	 */
-	if (unlikely(PMC0_HAS_OVFL(ctx->th_pmcs[0]))) {
-		/*
-		 * reload pmc0 with the overflow information
-		 * On McKinley PMU, this will trigger a PMU interrupt
-		 */
-		ia64_set_pmc(0, ctx->th_pmcs[0]);
-		ia64_srlz_d();
-
-		ctx->th_pmcs[0] = 0UL;
-
-		/*
-		 * will replay the PMU interrupt
-		 */
-		if (need_irq_resend) ia64_resend_irq(IA64_PERFMON_VECTOR);
-
-		pfm_stats[smp_processor_id()].pfm_replay_ovfl_intr_count++;
-	}
-
-	/*
-	 * establish new ownership. 
-	 */
-	SET_PMU_OWNER(task, ctx);
-
-	/*
-	 * restore the psr.up bit. measurement
-	 * is active again.
-	 * no PMU interrupt can happen at this point
-	 * because we still have interrupts disabled.
-	 */
-	if (likely(psr_up)) pfm_set_psr_up();
-}
-#endif /* CONFIG_SMP */
-
-/*
- * this function assumes monitoring is stopped
- */
-static void
-pfm_flush_pmds(struct task_struct *task, pfm_context_t *ctx)
-{
-	u64 pmc0;
-	unsigned long mask2, val, pmd_val, ovfl_val;
-	int i, can_access_pmu = 0;
-	int is_self;
-
-	/*
-	 * is the caller the task being monitored (or which initiated the
-	 * session for system wide measurements)
-	 */
-	is_self = ctx->ctx_task == task ? 1 : 0;
-
-	/*
-	 * can access PMU is task is the owner of the PMU state on the current CPU
-	 * or if we are running on the CPU bound to the context in system-wide mode
-	 * (that is not necessarily the task the context is attached to in this mode).
-	 * In system-wide we always have can_access_pmu true because a task running on an
-	 * invalid processor is flagged earlier in the call stack (see pfm_stop).
-	 */
-	can_access_pmu = (GET_PMU_OWNER() == task) || (ctx->ctx_fl_system && ctx->ctx_cpu == smp_processor_id());
-	if (can_access_pmu) {
-		/*
-		 * Mark the PMU as not owned
-		 * This will cause the interrupt handler to do nothing in case an overflow
-		 * interrupt was in-flight
-		 * This also guarantees that pmc0 will contain the final state
-		 * It virtually gives us full control on overflow processing from that point
-		 * on.
-		 */
-		SET_PMU_OWNER(NULL, NULL);
-		DPRINT(("releasing ownership\n"));
-
-		/*
-		 * read current overflow status:
-		 *
-		 * we are guaranteed to read the final stable state
-		 */
-		ia64_srlz_d();
-		pmc0 = ia64_get_pmc(0); /* slow */
-
-		/*
-		 * reset freeze bit, overflow status information destroyed
-		 */
-		pfm_unfreeze_pmu();
-	} else {
-		pmc0 = ctx->th_pmcs[0];
-		/*
-		 * clear whatever overflow status bits there were
-		 */
-		ctx->th_pmcs[0] = 0;
-	}
-	ovfl_val = pmu_conf->ovfl_val;
-	/*
-	 * we save all the used pmds
-	 * we take care of overflows for counting PMDs
-	 *
-	 * XXX: sampling situation is not taken into account here
-	 */
-	mask2 = ctx->ctx_used_pmds[0];
-
-	DPRINT(("is_self=%d ovfl_val=0x%lx mask2=0x%lx\n", is_self, ovfl_val, mask2));
-
-	for (i = 0; mask2; i++, mask2>>=1) {
-
-		/* skip non used pmds */
-		if ((mask2 & 0x1) == 0) continue;
-
-		/*
-		 * can access PMU always true in system wide mode
-		 */
-		val = pmd_val = can_access_pmu ? ia64_get_pmd(i) : ctx->th_pmds[i];
-
-		if (PMD_IS_COUNTING(i)) {
-			DPRINT(("[%d] pmd[%d] ctx_pmd=0x%lx hw_pmd=0x%lx\n",
-				task_pid_nr(task),
-				i,
-				ctx->ctx_pmds[i].val,
-				val & ovfl_val));
-
-			/*
-			 * we rebuild the full 64 bit value of the counter
-			 */
-			val = ctx->ctx_pmds[i].val + (val & ovfl_val);
-
-			/*
-			 * now everything is in ctx_pmds[] and we need
-			 * to clear the saved context from save_regs() such that
-			 * pfm_read_pmds() gets the correct value
-			 */
-			pmd_val = 0UL;
-
-			/*
-			 * take care of overflow inline
-			 */
-			if (pmc0 & (1UL << i)) {
-				val += 1 + ovfl_val;
-				DPRINT(("[%d] pmd[%d] overflowed\n", task_pid_nr(task), i));
-			}
-		}
-
-		DPRINT(("[%d] ctx_pmd[%d]=0x%lx  pmd_val=0x%lx\n", task_pid_nr(task), i, val, pmd_val));
-
-		if (is_self) ctx->th_pmds[i] = pmd_val;
-
-		ctx->ctx_pmds[i].val = val;
-	}
-}
-
-static struct irqaction perfmon_irqaction = {
-	.handler = pfm_interrupt_handler,
-	.flags   = IRQF_DISABLED,
-	.name    = "perfmon"
-};
-
-static void
-pfm_alt_save_pmu_state(void *data)
-{
-	struct pt_regs *regs;
-
-	regs = task_pt_regs(current);
-
-	DPRINT(("called\n"));
-
-	/*
-	 * should not be necessary but
-	 * let's take not risk
-	 */
-	pfm_clear_psr_up();
-	pfm_clear_psr_pp();
-	ia64_psr(regs)->pp = 0;
-
-	/*
-	 * This call is required
-	 * May cause a spurious interrupt on some processors
-	 */
-	pfm_freeze_pmu();
-
-	ia64_srlz_d();
-}
-
-void
-pfm_alt_restore_pmu_state(void *data)
-{
-	struct pt_regs *regs;
-
-	regs = task_pt_regs(current);
-
-	DPRINT(("called\n"));
-
-	/*
-	 * put PMU back in state expected
-	 * by perfmon
-	 */
-	pfm_clear_psr_up();
-	pfm_clear_psr_pp();
-	ia64_psr(regs)->pp = 0;
-
-	/*
-	 * perfmon runs with PMU unfrozen at all times
-	 */
-	pfm_unfreeze_pmu();
-
-	ia64_srlz_d();
-}
-
-int
-pfm_install_alt_pmu_interrupt(pfm_intr_handler_desc_t *hdl)
-{
-	int ret, i;
-	int reserve_cpu;
-
-	/* some sanity checks */
-	if (hdl == NULL || hdl->handler == NULL) return -EINVAL;
-
-	/* do the easy test first */
-	if (pfm_alt_intr_handler) return -EBUSY;
-
-	/* one at a time in the install or remove, just fail the others */
-	if (!spin_trylock(&pfm_alt_install_check)) {
-		return -EBUSY;
-	}
-
-	/* reserve our session */
-	for_each_online_cpu(reserve_cpu) {
-		ret = pfm_reserve_session(NULL, 1, reserve_cpu);
-		if (ret) goto cleanup_reserve;
-	}
-
-	/* save the current system wide pmu states */
-	ret = on_each_cpu(pfm_alt_save_pmu_state, NULL, 1);
-	if (ret) {
-		DPRINT(("on_each_cpu() failed: %d\n", ret));
-		goto cleanup_reserve;
-	}
-
-	/* officially change to the alternate interrupt handler */
-	pfm_alt_intr_handler = hdl;
-
-	spin_unlock(&pfm_alt_install_check);
-
-	return 0;
-
-cleanup_reserve:
-	for_each_online_cpu(i) {
-		/* don't unreserve more than we reserved */
-		if (i >= reserve_cpu) break;
-
-		pfm_unreserve_session(NULL, 1, i);
-	}
-
-	spin_unlock(&pfm_alt_install_check);
-
-	return ret;
-}
-EXPORT_SYMBOL_GPL(pfm_install_alt_pmu_interrupt);
-
-int
-pfm_remove_alt_pmu_interrupt(pfm_intr_handler_desc_t *hdl)
-{
-	int i;
-	int ret;
-
-	if (hdl == NULL) return -EINVAL;
-
-	/* cannot remove someone else's handler! */
-	if (pfm_alt_intr_handler != hdl) return -EINVAL;
-
-	/* one at a time in the install or remove, just fail the others */
-	if (!spin_trylock(&pfm_alt_install_check)) {
-		return -EBUSY;
-	}
-
-	pfm_alt_intr_handler = NULL;
-
-	ret = on_each_cpu(pfm_alt_restore_pmu_state, NULL, 1);
-	if (ret) {
-		DPRINT(("on_each_cpu() failed: %d\n", ret));
-	}
-
-	for_each_online_cpu(i) {
-		pfm_unreserve_session(NULL, 1, i);
-	}
-
-	spin_unlock(&pfm_alt_install_check);
-
-	return 0;
-}
-EXPORT_SYMBOL_GPL(pfm_remove_alt_pmu_interrupt);
-
-/*
- * perfmon initialization routine, called from the initcall() table
- */
-static int init_pfm_fs(void);
-
-static int __init
-pfm_probe_pmu(void)
-{
-	pmu_config_t **p;
-	int family;
-
-	family = local_cpu_data->family;
-	p      = pmu_confs;
-
-	while(*p) {
-		if ((*p)->probe) {
-			if ((*p)->probe() == 0) goto found;
-		} else if ((*p)->pmu_family == family || (*p)->pmu_family == 0xff) {
-			goto found;
-		}
-		p++;
-	}
-	return -1;
-found:
-	pmu_conf = *p;
-	return 0;
-}
-
-static const struct file_operations pfm_proc_fops = {
-	.open		= pfm_proc_open,
-	.read		= seq_read,
-	.llseek		= seq_lseek,
-	.release	= seq_release,
-};
-
-int __init
-pfm_init(void)
-{
-	unsigned int n, n_counters, i;
-
-	printk("perfmon: version %u.%u IRQ %u\n",
-		PFM_VERSION_MAJ,
-		PFM_VERSION_MIN,
-		IA64_PERFMON_VECTOR);
-
-	if (pfm_probe_pmu()) {
-		printk(KERN_INFO "perfmon: disabled, there is no support for processor family %d\n", 
-				local_cpu_data->family);
-		return -ENODEV;
-	}
-
-	/*
-	 * compute the number of implemented PMD/PMC from the
-	 * description tables
-	 */
-	n = 0;
-	for (i=0; PMC_IS_LAST(i) == 0;  i++) {
-		if (PMC_IS_IMPL(i) == 0) continue;
-		pmu_conf->impl_pmcs[i>>6] |= 1UL << (i&63);
-		n++;
-	}
-	pmu_conf->num_pmcs = n;
-
-	n = 0; n_counters = 0;
-	for (i=0; PMD_IS_LAST(i) == 0;  i++) {
-		if (PMD_IS_IMPL(i) == 0) continue;
-		pmu_conf->impl_pmds[i>>6] |= 1UL << (i&63);
-		n++;
-		if (PMD_IS_COUNTING(i)) n_counters++;
-	}
-	pmu_conf->num_pmds      = n;
-	pmu_conf->num_counters  = n_counters;
-
-	/*
-	 * sanity checks on the number of debug registers
-	 */
-	if (pmu_conf->use_rr_dbregs) {
-		if (pmu_conf->num_ibrs > IA64_NUM_DBG_REGS) {
-			printk(KERN_INFO "perfmon: unsupported number of code debug registers (%u)\n", pmu_conf->num_ibrs);
-			pmu_conf = NULL;
-			return -1;
-		}
-		if (pmu_conf->num_dbrs > IA64_NUM_DBG_REGS) {
-			printk(KERN_INFO "perfmon: unsupported number of data debug registers (%u)\n", pmu_conf->num_ibrs);
-			pmu_conf = NULL;
-			return -1;
-		}
-	}
-
-	printk("perfmon: %s PMU detected, %u PMCs, %u PMDs, %u counters (%lu bits)\n",
-	       pmu_conf->pmu_name,
-	       pmu_conf->num_pmcs,
-	       pmu_conf->num_pmds,
-	       pmu_conf->num_counters,
-	       ffz(pmu_conf->ovfl_val));
-
-	/* sanity check */
-	if (pmu_conf->num_pmds >= PFM_NUM_PMD_REGS || pmu_conf->num_pmcs >= PFM_NUM_PMC_REGS) {
-		printk(KERN_ERR "perfmon: not enough pmc/pmd, perfmon disabled\n");
-		pmu_conf = NULL;
-		return -1;
-	}
-
-	/*
-	 * create /proc/perfmon (mostly for debugging purposes)
-	 */
-	perfmon_dir = proc_create("perfmon", S_IRUGO, NULL, &pfm_proc_fops);
-	if (perfmon_dir == NULL) {
-		printk(KERN_ERR "perfmon: cannot create /proc entry, perfmon disabled\n");
-		pmu_conf = NULL;
-		return -1;
-	}
-
-	/*
-	 * create /proc/sys/kernel/perfmon (for debugging purposes)
-	 */
-	pfm_sysctl_header = register_sysctl_table(pfm_sysctl_root);
-
-	/*
-	 * initialize all our spinlocks
-	 */
-	spin_lock_init(&pfm_sessions.pfs_lock);
-	spin_lock_init(&pfm_buffer_fmt_lock);
-
-	init_pfm_fs();
-
-	for(i=0; i < NR_CPUS; i++) pfm_stats[i].pfm_ovfl_intr_cycles_min = ~0UL;
-
-	return 0;
-}
-
-__initcall(pfm_init);
-
-/*
- * this function is called before pfm_init()
- */
-void
-pfm_init_percpu (void)
-{
-	static int first_time=1;
-	/*
-	 * make sure no measurement is active
-	 * (may inherit programmed PMCs from EFI).
-	 */
-	pfm_clear_psr_pp();
-	pfm_clear_psr_up();
-
-	/*
-	 * we run with the PMU not frozen at all times
-	 */
-	pfm_unfreeze_pmu();
-
-	if (first_time) {
-		register_percpu_irq(IA64_PERFMON_VECTOR, &perfmon_irqaction);
-		first_time=0;
-	}
-
-	ia64_setreg(_IA64_REG_CR_PMV, IA64_PERFMON_VECTOR);
-	ia64_srlz_d();
-}
-
-/*
- * used for debug purposes only
- */
-void
-dump_pmu_state(const char *from)
-{
-	struct task_struct *task;
-	struct pt_regs *regs;
-	pfm_context_t *ctx;
-	unsigned long psr, dcr, info, flags;
-	int i, this_cpu;
-
-	local_irq_save(flags);
-
-	this_cpu = smp_processor_id();
-	regs     = task_pt_regs(current);
-	info     = PFM_CPUINFO_GET();
-	dcr      = ia64_getreg(_IA64_REG_CR_DCR);
-
-	if (info == 0 && ia64_psr(regs)->pp == 0 && (dcr & IA64_DCR_PP) == 0) {
-		local_irq_restore(flags);
-		return;
-	}
-
-	printk("CPU%d from %s() current [%d] iip=0x%lx %s\n", 
-		this_cpu, 
-		from, 
-		task_pid_nr(current),
-		regs->cr_iip,
-		current->comm);
-
-	task = GET_PMU_OWNER();
-	ctx  = GET_PMU_CTX();
-
-	printk("->CPU%d owner [%d] ctx=%p\n", this_cpu, task ? task_pid_nr(task) : -1, ctx);
-
-	psr = pfm_get_psr();
-
-	printk("->CPU%d pmc0=0x%lx psr.pp=%d psr.up=%d dcr.pp=%d syst_info=0x%lx user_psr.up=%d user_psr.pp=%d\n", 
-		this_cpu,
-		ia64_get_pmc(0),
-		psr & IA64_PSR_PP ? 1 : 0,
-		psr & IA64_PSR_UP ? 1 : 0,
-		dcr & IA64_DCR_PP ? 1 : 0,
-		info,
-		ia64_psr(regs)->up,
-		ia64_psr(regs)->pp);
-
-	ia64_psr(regs)->up = 0;
-	ia64_psr(regs)->pp = 0;
-
-	for (i=1; PMC_IS_LAST(i) == 0; i++) {
-		if (PMC_IS_IMPL(i) == 0) continue;
-		printk("->CPU%d pmc[%d]=0x%lx thread_pmc[%d]=0x%lx\n", this_cpu, i, ia64_get_pmc(i), i, ctx->th_pmcs[i]);
-	}
-
-	for (i=1; PMD_IS_LAST(i) == 0; i++) {
-		if (PMD_IS_IMPL(i) == 0) continue;
-		printk("->CPU%d pmd[%d]=0x%lx thread_pmd[%d]=0x%lx\n", this_cpu, i, ia64_get_pmd(i), i, ctx->th_pmds[i]);
-	}
-
-	if (ctx) {
-		printk("->CPU%d ctx_state=%d vaddr=%p addr=%p fd=%d ctx_task=[%d] saved_psr_up=0x%lx\n",
-				this_cpu,
-				ctx->ctx_state,
-				ctx->ctx_smpl_vaddr,
-				ctx->ctx_smpl_hdr,
-				ctx->ctx_msgq_head,
-				ctx->ctx_msgq_tail,
-				ctx->ctx_saved_psr_up);
-	}
-	local_irq_restore(flags);
-}
-
-/*
- * called from process.c:copy_thread(). task is new child.
- */
-void
-pfm_inherit(struct task_struct *task, struct pt_regs *regs)
-{
-	struct thread_struct *thread;
-
-	DPRINT(("perfmon: pfm_inherit clearing state for [%d]\n", task_pid_nr(task)));
-
-	thread = &task->thread;
-
-	/*
-	 * cut links inherited from parent (current)
-	 */
-	thread->pfm_context = NULL;
-
-	PFM_SET_WORK_PENDING(task, 0);
-
-	/*
-	 * the psr bits are already set properly in copy_threads()
-	 */
-}
-#else  /* !CONFIG_PERFMON */
-asmlinkage long
-sys_perfmonctl (int fd, int cmd, void *arg, int count)
-{
-	return -ENOSYS;
-}
-#endif /* CONFIG_PERFMON */
Index: linux-2.6.31-master/arch/ia64/kernel/perfmon_default_smpl.c
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/perfmon_default_smpl.c
+++ /dev/null
@@ -1,296 +0,0 @@
-/*
- * Copyright (C) 2002-2003 Hewlett-Packard Co
- *               Stephane Eranian <eranian@hpl.hp.com>
- *
- * This file implements the default sampling buffer format
- * for the Linux/ia64 perfmon-2 subsystem.
- */
-#include <linux/kernel.h>
-#include <linux/types.h>
-#include <linux/module.h>
-#include <linux/init.h>
-#include <asm/delay.h>
-#include <linux/smp.h>
-
-#include <asm/perfmon.h>
-#include <asm/perfmon_default_smpl.h>
-
-MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
-MODULE_DESCRIPTION("perfmon default sampling format");
-MODULE_LICENSE("GPL");
-
-#define DEFAULT_DEBUG 1
-
-#ifdef DEFAULT_DEBUG
-#define DPRINT(a) \
-	do { \
-		if (unlikely(pfm_sysctl.debug >0)) { printk("%s.%d: CPU%d ", __func__, __LINE__, smp_processor_id()); printk a; } \
-	} while (0)
-
-#define DPRINT_ovfl(a) \
-	do { \
-		if (unlikely(pfm_sysctl.debug > 0 && pfm_sysctl.debug_ovfl >0)) { printk("%s.%d: CPU%d ", __func__, __LINE__, smp_processor_id()); printk a; } \
-	} while (0)
-
-#else
-#define DPRINT(a)
-#define DPRINT_ovfl(a)
-#endif
-
-static int
-default_validate(struct task_struct *task, unsigned int flags, int cpu, void *data)
-{
-	pfm_default_smpl_arg_t *arg = (pfm_default_smpl_arg_t*)data;
-	int ret = 0;
-
-	if (data == NULL) {
-		DPRINT(("[%d] no argument passed\n", task_pid_nr(task)));
-		return -EINVAL;
-	}
-
-	DPRINT(("[%d] validate flags=0x%x CPU%d\n", task_pid_nr(task), flags, cpu));
-
-	/*
-	 * must hold at least the buffer header + one minimally sized entry
-	 */
-	if (arg->buf_size < PFM_DEFAULT_SMPL_MIN_BUF_SIZE) return -EINVAL;
-
-	DPRINT(("buf_size=%lu\n", arg->buf_size));
-
-	return ret;
-}
-
-static int
-default_get_size(struct task_struct *task, unsigned int flags, int cpu, void *data, unsigned long *size)
-{
-	pfm_default_smpl_arg_t *arg = (pfm_default_smpl_arg_t *)data;
-
-	/*
-	 * size has been validated in default_validate
-	 */
-	*size = arg->buf_size;
-
-	return 0;
-}
-
-static int
-default_init(struct task_struct *task, void *buf, unsigned int flags, int cpu, void *data)
-{
-	pfm_default_smpl_hdr_t *hdr;
-	pfm_default_smpl_arg_t *arg = (pfm_default_smpl_arg_t *)data;
-
-	hdr = (pfm_default_smpl_hdr_t *)buf;
-
-	hdr->hdr_version      = PFM_DEFAULT_SMPL_VERSION;
-	hdr->hdr_buf_size     = arg->buf_size;
-	hdr->hdr_cur_offs     = sizeof(*hdr);
-	hdr->hdr_overflows    = 0UL;
-	hdr->hdr_count        = 0UL;
-
-	DPRINT(("[%d] buffer=%p buf_size=%lu hdr_size=%lu hdr_version=%u cur_offs=%lu\n",
-		task_pid_nr(task),
-		buf,
-		hdr->hdr_buf_size,
-		sizeof(*hdr),
-		hdr->hdr_version,
-		hdr->hdr_cur_offs));
-
-	return 0;
-}
-
-static int
-default_handler(struct task_struct *task, void *buf, pfm_ovfl_arg_t *arg, struct pt_regs *regs, unsigned long stamp)
-{
-	pfm_default_smpl_hdr_t *hdr;
-	pfm_default_smpl_entry_t *ent;
-	void *cur, *last;
-	unsigned long *e, entry_size;
-	unsigned int npmds, i;
-	unsigned char ovfl_pmd;
-	unsigned char ovfl_notify;
-
-	if (unlikely(buf == NULL || arg == NULL|| regs == NULL || task == NULL)) {
-		DPRINT(("[%d] invalid arguments buf=%p arg=%p\n", task->pid, buf, arg));
-		return -EINVAL;
-	}
-
-	hdr         = (pfm_default_smpl_hdr_t *)buf;
-	cur         = buf+hdr->hdr_cur_offs;
-	last        = buf+hdr->hdr_buf_size;
-	ovfl_pmd    = arg->ovfl_pmd;
-	ovfl_notify = arg->ovfl_notify;
-
-	/*
-	 * precheck for sanity
-	 */
-	if ((last - cur) < PFM_DEFAULT_MAX_ENTRY_SIZE) goto full;
-
-	npmds = hweight64(arg->smpl_pmds[0]);
-
-	ent = (pfm_default_smpl_entry_t *)cur;
-
-	prefetch(arg->smpl_pmds_values);
-
-	entry_size = sizeof(*ent) + (npmds << 3);
-
-	/* position for first pmd */
-	e = (unsigned long *)(ent+1);
-
-	hdr->hdr_count++;
-
-	DPRINT_ovfl(("[%d] count=%lu cur=%p last=%p free_bytes=%lu ovfl_pmd=%d ovfl_notify=%d npmds=%u\n",
-			task->pid,
-			hdr->hdr_count,
-			cur, last,
-			last-cur,
-			ovfl_pmd,
-			ovfl_notify, npmds));
-
-	/*
-	 * current = task running at the time of the overflow.
-	 *
-	 * per-task mode:
-	 * 	- this is ususally the task being monitored.
-	 * 	  Under certain conditions, it might be a different task
-	 *
-	 * system-wide:
-	 * 	- this is not necessarily the task controlling the session
-	 */
-	ent->pid            = current->pid;
-	ent->ovfl_pmd  	    = ovfl_pmd;
-	ent->last_reset_val = arg->pmd_last_reset; //pmd[0].reg_last_reset_val;
-
-	/*
-	 * where did the fault happen (includes slot number)
-	 */
-	ent->ip = regs->cr_iip | ((regs->cr_ipsr >> 41) & 0x3);
-
-	ent->tstamp    = stamp;
-	ent->cpu       = smp_processor_id();
-	ent->set       = arg->active_set;
-	ent->tgid      = current->tgid;
-
-	/*
-	 * selectively store PMDs in increasing index number
-	 */
-	if (npmds) {
-		unsigned long *val = arg->smpl_pmds_values;
-		for(i=0; i < npmds; i++) {
-			*e++ = *val++;
-		}
-	}
-
-	/*
-	 * update position for next entry
-	 */
-	hdr->hdr_cur_offs += entry_size;
-	cur               += entry_size;
-
-	/*
-	 * post check to avoid losing the last sample
-	 */
-	if ((last - cur) < PFM_DEFAULT_MAX_ENTRY_SIZE) goto full;
-
-	/*
-	 * keep same ovfl_pmds, ovfl_notify
-	 */
-	arg->ovfl_ctrl.bits.notify_user     = 0;
-	arg->ovfl_ctrl.bits.block_task      = 0;
-	arg->ovfl_ctrl.bits.mask_monitoring = 0;
-	arg->ovfl_ctrl.bits.reset_ovfl_pmds = 1; /* reset before returning from interrupt handler */
-
-	return 0;
-full:
-	DPRINT_ovfl(("sampling buffer full free=%lu, count=%lu, ovfl_notify=%d\n", last-cur, hdr->hdr_count, ovfl_notify));
-
-	/*
-	 * increment number of buffer overflow.
-	 * important to detect duplicate set of samples.
-	 */
-	hdr->hdr_overflows++;
-
-	/*
-	 * if no notification requested, then we saturate the buffer
-	 */
-	if (ovfl_notify == 0) {
-		arg->ovfl_ctrl.bits.notify_user     = 0;
-		arg->ovfl_ctrl.bits.block_task      = 0;
-		arg->ovfl_ctrl.bits.mask_monitoring = 1;
-		arg->ovfl_ctrl.bits.reset_ovfl_pmds = 0;
-	} else {
-		arg->ovfl_ctrl.bits.notify_user     = 1;
-		arg->ovfl_ctrl.bits.block_task      = 1; /* ignored for non-blocking context */
-		arg->ovfl_ctrl.bits.mask_monitoring = 1;
-		arg->ovfl_ctrl.bits.reset_ovfl_pmds = 0; /* no reset now */
-	}
-	return -1; /* we are full, sorry */
-}
-
-static int
-default_restart(struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs)
-{
-	pfm_default_smpl_hdr_t *hdr;
-
-	hdr = (pfm_default_smpl_hdr_t *)buf;
-
-	hdr->hdr_count    = 0UL;
-	hdr->hdr_cur_offs = sizeof(*hdr);
-
-	ctrl->bits.mask_monitoring = 0;
-	ctrl->bits.reset_ovfl_pmds = 1; /* uses long-reset values */
-
-	return 0;
-}
-
-static int
-default_exit(struct task_struct *task, void *buf, struct pt_regs *regs)
-{
-	DPRINT(("[%d] exit(%p)\n", task_pid_nr(task), buf));
-	return 0;
-}
-
-static pfm_buffer_fmt_t default_fmt={
- 	.fmt_name 	    = "default_format",
- 	.fmt_uuid	    = PFM_DEFAULT_SMPL_UUID,
- 	.fmt_arg_size	    = sizeof(pfm_default_smpl_arg_t),
- 	.fmt_validate	    = default_validate,
- 	.fmt_getsize	    = default_get_size,
- 	.fmt_init	    = default_init,
- 	.fmt_handler	    = default_handler,
- 	.fmt_restart	    = default_restart,
- 	.fmt_restart_active = default_restart,
- 	.fmt_exit	    = default_exit,
-};
-
-static int __init
-pfm_default_smpl_init_module(void)
-{
-	int ret;
-
-	ret = pfm_register_buffer_fmt(&default_fmt);
-	if (ret == 0) {
-		printk("perfmon_default_smpl: %s v%u.%u registered\n",
-			default_fmt.fmt_name,
-			PFM_DEFAULT_SMPL_VERSION_MAJ,
-			PFM_DEFAULT_SMPL_VERSION_MIN);
-	} else {
-		printk("perfmon_default_smpl: %s cannot register ret=%d\n",
-			default_fmt.fmt_name,
-			ret);
-	}
-
-	return ret;
-}
-
-static void __exit
-pfm_default_smpl_cleanup_module(void)
-{
-	int ret;
-	ret = pfm_unregister_buffer_fmt(default_fmt.fmt_uuid);
-
-	printk("perfmon_default_smpl: unregister %s=%d\n", default_fmt.fmt_name, ret);
-}
-
-module_init(pfm_default_smpl_init_module);
-module_exit(pfm_default_smpl_cleanup_module);
-
Index: linux-2.6.31-master/arch/ia64/kernel/perfmon_generic.h
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/perfmon_generic.h
+++ /dev/null
@@ -1,45 +0,0 @@
-/*
- * This file contains the generic PMU register description tables
- * and pmc checker used by perfmon.c.
- *
- * Copyright (C) 2002-2003  Hewlett Packard Co
- *               Stephane Eranian <eranian@hpl.hp.com>
- */
-
-static pfm_reg_desc_t pfm_gen_pmc_desc[PMU_MAX_PMCS]={
-/* pmc0  */ { PFM_REG_CONTROL , 0, 0x1UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc1  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc2  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc3  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc4  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {RDEP(4),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc5  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {RDEP(5),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc6  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {RDEP(6),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc7  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {RDEP(7),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-static pfm_reg_desc_t pfm_gen_pmd_desc[PMU_MAX_PMDS]={
-/* pmd0  */ { PFM_REG_NOTIMPL , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}},
-/* pmd1  */ { PFM_REG_NOTIMPL , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}},
-/* pmd2  */ { PFM_REG_NOTIMPL , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}},
-/* pmd3  */ { PFM_REG_NOTIMPL , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}},
-/* pmd4  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(4),0UL, 0UL, 0UL}},
-/* pmd5  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(5),0UL, 0UL, 0UL}},
-/* pmd6  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(6),0UL, 0UL, 0UL}},
-/* pmd7  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(7),0UL, 0UL, 0UL}},
-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-/*
- * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
- */
-static pmu_config_t pmu_conf_gen={
-	.pmu_name   = "Generic",
-	.pmu_family = 0xff, /* any */
-	.ovfl_val   = (1UL << 32) - 1,
-	.num_ibrs   = 0, /* does not use */
-	.num_dbrs   = 0, /* does not use */
-	.pmd_desc   = pfm_gen_pmd_desc,
-	.pmc_desc   = pfm_gen_pmc_desc
-};
-
Index: linux-2.6.31-master/arch/ia64/kernel/perfmon_itanium.h
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/perfmon_itanium.h
+++ /dev/null
@@ -1,115 +0,0 @@
-/*
- * This file contains the Itanium PMU register description tables
- * and pmc checker used by perfmon.c.
- *
- * Copyright (C) 2002-2003  Hewlett Packard Co
- *               Stephane Eranian <eranian@hpl.hp.com>
- */
-static int pfm_ita_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs);
-
-static pfm_reg_desc_t pfm_ita_pmc_desc[PMU_MAX_PMCS]={
-/* pmc0  */ { PFM_REG_CONTROL , 0, 0x1UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc1  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc2  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc3  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc4  */ { PFM_REG_COUNTING, 6, 0x0UL, -1UL, NULL, NULL, {RDEP(4),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc5  */ { PFM_REG_COUNTING, 6, 0x0UL, -1UL, NULL, NULL, {RDEP(5),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc6  */ { PFM_REG_COUNTING, 6, 0x0UL, -1UL, NULL, NULL, {RDEP(6),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc7  */ { PFM_REG_COUNTING, 6, 0x0UL, -1UL, NULL, NULL, {RDEP(7),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc8  */ { PFM_REG_CONFIG  , 0, 0xf00000003ffffff8UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc9  */ { PFM_REG_CONFIG  , 0, 0xf00000003ffffff8UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc10 */ { PFM_REG_MONITOR , 6, 0x0UL, -1UL, NULL, NULL, {RDEP(0)|RDEP(1),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc11 */ { PFM_REG_MONITOR , 6, 0x0000000010000000UL, -1UL, NULL, pfm_ita_pmc_check, {RDEP(2)|RDEP(3)|RDEP(17),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc12 */ { PFM_REG_MONITOR , 6, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc13 */ { PFM_REG_CONFIG  , 0, 0x0003ffff00000001UL, -1UL, NULL, pfm_ita_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-static pfm_reg_desc_t pfm_ita_pmd_desc[PMU_MAX_PMDS]={
-/* pmd0  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(1),0UL, 0UL, 0UL}, {RDEP(10),0UL, 0UL, 0UL}},
-/* pmd1  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(0),0UL, 0UL, 0UL}, {RDEP(10),0UL, 0UL, 0UL}},
-/* pmd2  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(3)|RDEP(17),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
-/* pmd3  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(2)|RDEP(17),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
-/* pmd4  */ { PFM_REG_COUNTING, 0, 0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(4),0UL, 0UL, 0UL}},
-/* pmd5  */ { PFM_REG_COUNTING, 0, 0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(5),0UL, 0UL, 0UL}},
-/* pmd6  */ { PFM_REG_COUNTING, 0, 0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(6),0UL, 0UL, 0UL}},
-/* pmd7  */ { PFM_REG_COUNTING, 0, 0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(7),0UL, 0UL, 0UL}},
-/* pmd8  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd9  */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd10 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd11 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd12 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd13 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd14 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd15 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd16 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd17 */ { PFM_REG_BUFFER  , 0, 0UL, -1UL, NULL, NULL, {RDEP(2)|RDEP(3),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
-	    { PFM_REG_END     , 0, 0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-static int
-pfm_ita_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs)
-{
-	int ret;
-	int is_loaded;
-
-	/* sanitfy check */
-	if (ctx == NULL) return -EINVAL;
-
-	is_loaded = ctx->ctx_state == PFM_CTX_LOADED || ctx->ctx_state == PFM_CTX_MASKED;
-
-	/*
-	 * we must clear the (instruction) debug registers if pmc13.ta bit is cleared
-	 * before they are written (fl_using_dbreg==0) to avoid picking up stale information.
-	 */
-	if (cnum == 13 && is_loaded && ((*val & 0x1) == 0UL) && ctx->ctx_fl_using_dbreg == 0) {
-
-		DPRINT(("pmc[%d]=0x%lx has active pmc13.ta cleared, clearing ibr\n", cnum, *val));
-
-		/* don't mix debug with perfmon */
-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
-
-		/*
-		 * a count of 0 will mark the debug registers as in use and also
-		 * ensure that they are properly cleared.
-		 */
-		ret = pfm_write_ibr_dbr(1, ctx, NULL, 0, regs);
-		if (ret) return ret;
-	}
-
-	/*
-	 * we must clear the (data) debug registers if pmc11.pt bit is cleared
-	 * before they are written (fl_using_dbreg==0) to avoid picking up stale information.
-	 */
-	if (cnum == 11 && is_loaded && ((*val >> 28)& 0x1) == 0 && ctx->ctx_fl_using_dbreg == 0) {
-
-		DPRINT(("pmc[%d]=0x%lx has active pmc11.pt cleared, clearing dbr\n", cnum, *val));
-
-		/* don't mix debug with perfmon */
-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
-
-		/*
-		 * a count of 0 will mark the debug registers as in use and also
-		 * ensure that they are properly cleared.
-		 */
-		ret = pfm_write_ibr_dbr(0, ctx, NULL, 0, regs);
-		if (ret) return ret;
-	}
-	return 0;
-}
-
-/*
- * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
- */
-static pmu_config_t pmu_conf_ita={
-	.pmu_name      = "Itanium",
-	.pmu_family    = 0x7,
-	.ovfl_val      = (1UL << 32) - 1,
-	.pmd_desc      = pfm_ita_pmd_desc,
-	.pmc_desc      = pfm_ita_pmc_desc,
-	.num_ibrs      = 8,
-	.num_dbrs      = 8,
-	.use_rr_dbregs = 1, /* debug register are use for range retrictions */
-};
-
-
Index: linux-2.6.31-master/arch/ia64/kernel/perfmon_mckinley.h
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/perfmon_mckinley.h
+++ /dev/null
@@ -1,187 +0,0 @@
-/*
- * This file contains the McKinley PMU register description tables
- * and pmc checker used by perfmon.c.
- *
- * Copyright (C) 2002-2003  Hewlett Packard Co
- *               Stephane Eranian <eranian@hpl.hp.com>
- */
-static int pfm_mck_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs);
-
-static pfm_reg_desc_t pfm_mck_pmc_desc[PMU_MAX_PMCS]={
-/* pmc0  */ { PFM_REG_CONTROL , 0, 0x1UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc1  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc2  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc3  */ { PFM_REG_CONTROL , 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc4  */ { PFM_REG_COUNTING, 6, 0x0000000000800000UL, 0xfffff7fUL, NULL, pfm_mck_pmc_check, {RDEP(4),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc5  */ { PFM_REG_COUNTING, 6, 0x0UL, 0xfffff7fUL, NULL,  pfm_mck_pmc_check, {RDEP(5),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc6  */ { PFM_REG_COUNTING, 6, 0x0UL, 0xfffff7fUL, NULL,  pfm_mck_pmc_check, {RDEP(6),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc7  */ { PFM_REG_COUNTING, 6, 0x0UL, 0xfffff7fUL, NULL,  pfm_mck_pmc_check, {RDEP(7),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc8  */ { PFM_REG_CONFIG  , 0, 0xffffffff3fffffffUL, 0xffffffff3ffffffbUL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc9  */ { PFM_REG_CONFIG  , 0, 0xffffffff3ffffffcUL, 0xffffffff3ffffffbUL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc10 */ { PFM_REG_MONITOR , 4, 0x0UL, 0xffffUL, NULL, pfm_mck_pmc_check, {RDEP(0)|RDEP(1),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc11 */ { PFM_REG_MONITOR , 6, 0x0UL, 0x30f01cf, NULL,  pfm_mck_pmc_check, {RDEP(2)|RDEP(3)|RDEP(17),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc12 */ { PFM_REG_MONITOR , 6, 0x0UL, 0xffffUL, NULL,  pfm_mck_pmc_check, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc13 */ { PFM_REG_CONFIG  , 0, 0x00002078fefefefeUL, 0x1e00018181818UL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc14 */ { PFM_REG_CONFIG  , 0, 0x0db60db60db60db6UL, 0x2492UL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-/* pmc15 */ { PFM_REG_CONFIG  , 0, 0x00000000fffffff0UL, 0xfUL, NULL, pfm_mck_pmc_check, {0UL,0UL, 0UL, 0UL}, {0UL,0UL, 0UL, 0UL}},
-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-static pfm_reg_desc_t pfm_mck_pmd_desc[PMU_MAX_PMDS]={
-/* pmd0  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(1),0UL, 0UL, 0UL}, {RDEP(10),0UL, 0UL, 0UL}},
-/* pmd1  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(0),0UL, 0UL, 0UL}, {RDEP(10),0UL, 0UL, 0UL}},
-/* pmd2  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(3)|RDEP(17),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
-/* pmd3  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(2)|RDEP(17),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
-/* pmd4  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(4),0UL, 0UL, 0UL}},
-/* pmd5  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(5),0UL, 0UL, 0UL}},
-/* pmd6  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(6),0UL, 0UL, 0UL}},
-/* pmd7  */ { PFM_REG_COUNTING, 0, 0x0UL, -1UL, NULL, NULL, {0UL,0UL, 0UL, 0UL}, {RDEP(7),0UL, 0UL, 0UL}},
-/* pmd8  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd9  */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd10 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd11 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd12 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(13)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd13 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(14)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd14 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(15)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd15 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(16),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd16 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15),0UL, 0UL, 0UL}, {RDEP(12),0UL, 0UL, 0UL}},
-/* pmd17 */ { PFM_REG_BUFFER  , 0, 0x0UL, -1UL, NULL, NULL, {RDEP(2)|RDEP(3),0UL, 0UL, 0UL}, {RDEP(11),0UL, 0UL, 0UL}},
-	    { PFM_REG_END     , 0, 0x0UL, -1UL, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-/*
- * PMC reserved fields must have their power-up values preserved
- */
-static int
-pfm_mck_reserved(unsigned int cnum, unsigned long *val, struct pt_regs *regs)
-{
-	unsigned long tmp1, tmp2, ival = *val;
-
-	/* remove reserved areas from user value */
-	tmp1 = ival & PMC_RSVD_MASK(cnum);
-
-	/* get reserved fields values */
-	tmp2 = PMC_DFL_VAL(cnum) & ~PMC_RSVD_MASK(cnum);
-
-	*val = tmp1 | tmp2;
-
-	DPRINT(("pmc[%d]=0x%lx, mask=0x%lx, reset=0x%lx, val=0x%lx\n",
-		  cnum, ival, PMC_RSVD_MASK(cnum), PMC_DFL_VAL(cnum), *val));
-	return 0;
-}
-
-/*
- * task can be NULL if the context is unloaded
- */
-static int
-pfm_mck_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs)
-{
-	int ret = 0, check_case1 = 0;
-	unsigned long val8 = 0, val14 = 0, val13 = 0;
-	int is_loaded;
-
-	/* first preserve the reserved fields */
-	pfm_mck_reserved(cnum, val, regs);
-
-	/* sanitfy check */
-	if (ctx == NULL) return -EINVAL;
-
-	is_loaded = ctx->ctx_state == PFM_CTX_LOADED || ctx->ctx_state == PFM_CTX_MASKED;
-
-	/*
-	 * we must clear the debug registers if pmc13 has a value which enable
-	 * memory pipeline event constraints. In this case we need to clear the
-	 * the debug registers if they have not yet been accessed. This is required
-	 * to avoid picking stale state.
-	 * PMC13 is "active" if:
-	 * 	one of the pmc13.cfg_dbrpXX field is different from 0x3
-	 * AND
-	 * 	at the corresponding pmc13.ena_dbrpXX is set.
-	 */
-	DPRINT(("cnum=%u val=0x%lx, using_dbreg=%d loaded=%d\n", cnum, *val, ctx->ctx_fl_using_dbreg, is_loaded));
-
-	if (cnum == 13 && is_loaded
-	    && (*val & 0x1e00000000000UL) && (*val & 0x18181818UL) != 0x18181818UL && ctx->ctx_fl_using_dbreg == 0) {
-
-		DPRINT(("pmc[%d]=0x%lx has active pmc13 settings, clearing dbr\n", cnum, *val));
-
-		/* don't mix debug with perfmon */
-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
-
-		/*
-		 * a count of 0 will mark the debug registers as in use and also
-		 * ensure that they are properly cleared.
-		 */
-		ret = pfm_write_ibr_dbr(PFM_DATA_RR, ctx, NULL, 0, regs);
-		if (ret) return ret;
-	}
-	/*
-	 * we must clear the (instruction) debug registers if any pmc14.ibrpX bit is enabled
-	 * before they are (fl_using_dbreg==0) to avoid picking up stale information.
-	 */
-	if (cnum == 14 && is_loaded && ((*val & 0x2222UL) != 0x2222UL) && ctx->ctx_fl_using_dbreg == 0) {
-
-		DPRINT(("pmc[%d]=0x%lx has active pmc14 settings, clearing ibr\n", cnum, *val));
-
-		/* don't mix debug with perfmon */
-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
-
-		/*
-		 * a count of 0 will mark the debug registers as in use and also
-		 * ensure that they are properly cleared.
-		 */
-		ret = pfm_write_ibr_dbr(PFM_CODE_RR, ctx, NULL, 0, regs);
-		if (ret) return ret;
-
-	}
-
-	switch(cnum) {
-		case  4: *val |= 1UL << 23; /* force power enable bit */
-			 break;
-		case  8: val8 = *val;
-			 val13 = ctx->ctx_pmcs[13];
-			 val14 = ctx->ctx_pmcs[14];
-			 check_case1 = 1;
-			 break;
-		case 13: val8  = ctx->ctx_pmcs[8];
-			 val13 = *val;
-			 val14 = ctx->ctx_pmcs[14];
-			 check_case1 = 1;
-			 break;
-		case 14: val8  = ctx->ctx_pmcs[8];
-			 val13 = ctx->ctx_pmcs[13];
-			 val14 = *val;
-			 check_case1 = 1;
-			 break;
-	}
-	/* check illegal configuration which can produce inconsistencies in tagging
-	 * i-side events in L1D and L2 caches
-	 */
-	if (check_case1) {
-		ret =   ((val13 >> 45) & 0xf) == 0
-		   && ((val8 & 0x1) == 0)
-		   && ((((val14>>1) & 0x3) == 0x2 || ((val14>>1) & 0x3) == 0x0)
-		       ||(((val14>>4) & 0x3) == 0x2 || ((val14>>4) & 0x3) == 0x0));
-
-		if (ret) DPRINT((KERN_DEBUG "perfmon: failure check_case1\n"));
-	}
-
-	return ret ? -EINVAL : 0;
-}
-
-/*
- * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
- */
-static pmu_config_t pmu_conf_mck={
-	.pmu_name      = "Itanium 2",
-	.pmu_family    = 0x1f,
-	.flags	       = PFM_PMU_IRQ_RESEND,
-	.ovfl_val      = (1UL << 47) - 1,
-	.pmd_desc      = pfm_mck_pmd_desc,
-	.pmc_desc      = pfm_mck_pmc_desc,
-	.num_ibrs       = 8,
-	.num_dbrs       = 8,
-	.use_rr_dbregs = 1 /* debug register are use for range restrictions */
-};
-
-
Index: linux-2.6.31-master/arch/ia64/kernel/perfmon_montecito.h
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/perfmon_montecito.h
+++ /dev/null
@@ -1,269 +0,0 @@
-/*
- * This file contains the Montecito PMU register description tables
- * and pmc checker used by perfmon.c.
- *
- * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
- *               Contributed by Stephane Eranian <eranian@hpl.hp.com>
- */
-static int pfm_mont_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs);
-
-#define RDEP_MONT_ETB	(RDEP(38)|RDEP(39)|RDEP(48)|RDEP(49)|RDEP(50)|RDEP(51)|RDEP(52)|RDEP(53)|RDEP(54)|\
-			 RDEP(55)|RDEP(56)|RDEP(57)|RDEP(58)|RDEP(59)|RDEP(60)|RDEP(61)|RDEP(62)|RDEP(63))
-#define RDEP_MONT_DEAR  (RDEP(32)|RDEP(33)|RDEP(36))
-#define RDEP_MONT_IEAR  (RDEP(34)|RDEP(35))
-
-static pfm_reg_desc_t pfm_mont_pmc_desc[PMU_MAX_PMCS]={
-/* pmc0  */ { PFM_REG_CONTROL , 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc1  */ { PFM_REG_CONTROL , 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc2  */ { PFM_REG_CONTROL , 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc3  */ { PFM_REG_CONTROL , 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc4  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(4),0, 0, 0}, {0,0, 0, 0}},
-/* pmc5  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(5),0, 0, 0}, {0,0, 0, 0}},
-/* pmc6  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(6),0, 0, 0}, {0,0, 0, 0}},
-/* pmc7  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(7),0, 0, 0}, {0,0, 0, 0}},
-/* pmc8  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(8),0, 0, 0}, {0,0, 0, 0}},
-/* pmc9  */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(9),0, 0, 0}, {0,0, 0, 0}},
-/* pmc10 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(10),0, 0, 0}, {0,0, 0, 0}},
-/* pmc11 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(11),0, 0, 0}, {0,0, 0, 0}},
-/* pmc12 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(12),0, 0, 0}, {0,0, 0, 0}},
-/* pmc13 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(13),0, 0, 0}, {0,0, 0, 0}},
-/* pmc14 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(14),0, 0, 0}, {0,0, 0, 0}},
-/* pmc15 */ { PFM_REG_COUNTING, 6, 0x2000000, 0x7c7fff7f, NULL, pfm_mont_pmc_check, {RDEP(15),0, 0, 0}, {0,0, 0, 0}},
-/* pmc16 */ { PFM_REG_NOTIMPL, },
-/* pmc17 */ { PFM_REG_NOTIMPL, },
-/* pmc18 */ { PFM_REG_NOTIMPL, },
-/* pmc19 */ { PFM_REG_NOTIMPL, },
-/* pmc20 */ { PFM_REG_NOTIMPL, },
-/* pmc21 */ { PFM_REG_NOTIMPL, },
-/* pmc22 */ { PFM_REG_NOTIMPL, },
-/* pmc23 */ { PFM_REG_NOTIMPL, },
-/* pmc24 */ { PFM_REG_NOTIMPL, },
-/* pmc25 */ { PFM_REG_NOTIMPL, },
-/* pmc26 */ { PFM_REG_NOTIMPL, },
-/* pmc27 */ { PFM_REG_NOTIMPL, },
-/* pmc28 */ { PFM_REG_NOTIMPL, },
-/* pmc29 */ { PFM_REG_NOTIMPL, },
-/* pmc30 */ { PFM_REG_NOTIMPL, },
-/* pmc31 */ { PFM_REG_NOTIMPL, },
-/* pmc32 */ { PFM_REG_CONFIG,  0, 0x30f01ffffffffffUL, 0x30f01ffffffffffUL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc33 */ { PFM_REG_CONFIG,  0, 0x0,  0x1ffffffffffUL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc34 */ { PFM_REG_CONFIG,  0, 0xf01ffffffffffUL, 0xf01ffffffffffUL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc35 */ { PFM_REG_CONFIG,  0, 0x0,  0x1ffffffffffUL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc36 */ { PFM_REG_CONFIG,  0, 0xfffffff0, 0xf, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc37 */ { PFM_REG_MONITOR, 4, 0x0, 0x3fff, NULL, pfm_mont_pmc_check, {RDEP_MONT_IEAR, 0, 0, 0}, {0, 0, 0, 0}},
-/* pmc38 */ { PFM_REG_CONFIG,  0, 0xdb6, 0x2492, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc39 */ { PFM_REG_MONITOR, 6, 0x0, 0xffcf, NULL, pfm_mont_pmc_check, {RDEP_MONT_ETB,0, 0, 0}, {0,0, 0, 0}},
-/* pmc40 */ { PFM_REG_MONITOR, 6, 0x2000000, 0xf01cf, NULL, pfm_mont_pmc_check, {RDEP_MONT_DEAR,0, 0, 0}, {0,0, 0, 0}},
-/* pmc41 */ { PFM_REG_CONFIG,  0, 0x00002078fefefefeUL, 0x1e00018181818UL, NULL, pfm_mont_pmc_check, {0,0, 0, 0}, {0,0, 0, 0}},
-/* pmc42 */ { PFM_REG_MONITOR, 6, 0x0, 0x7ff4f, NULL, pfm_mont_pmc_check, {RDEP_MONT_ETB,0, 0, 0}, {0,0, 0, 0}},
-	    { PFM_REG_END    , 0, 0x0, -1, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-static pfm_reg_desc_t pfm_mont_pmd_desc[PMU_MAX_PMDS]={
-/* pmd0  */ { PFM_REG_NOTIMPL, }, 
-/* pmd1  */ { PFM_REG_NOTIMPL, },
-/* pmd2  */ { PFM_REG_NOTIMPL, },
-/* pmd3  */ { PFM_REG_NOTIMPL, },
-/* pmd4  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(4),0, 0, 0}},
-/* pmd5  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(5),0, 0, 0}},
-/* pmd6  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(6),0, 0, 0}},
-/* pmd7  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(7),0, 0, 0}},
-/* pmd8  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(8),0, 0, 0}}, 
-/* pmd9  */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(9),0, 0, 0}},
-/* pmd10 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(10),0, 0, 0}},
-/* pmd11 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(11),0, 0, 0}},
-/* pmd12 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(12),0, 0, 0}},
-/* pmd13 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(13),0, 0, 0}},
-/* pmd14 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(14),0, 0, 0}},
-/* pmd15 */ { PFM_REG_COUNTING, 0, 0x0, -1, NULL, NULL, {0,0, 0, 0}, {RDEP(15),0, 0, 0}},
-/* pmd16 */ { PFM_REG_NOTIMPL, },
-/* pmd17 */ { PFM_REG_NOTIMPL, },
-/* pmd18 */ { PFM_REG_NOTIMPL, },
-/* pmd19 */ { PFM_REG_NOTIMPL, },
-/* pmd20 */ { PFM_REG_NOTIMPL, },
-/* pmd21 */ { PFM_REG_NOTIMPL, },
-/* pmd22 */ { PFM_REG_NOTIMPL, },
-/* pmd23 */ { PFM_REG_NOTIMPL, },
-/* pmd24 */ { PFM_REG_NOTIMPL, },
-/* pmd25 */ { PFM_REG_NOTIMPL, },
-/* pmd26 */ { PFM_REG_NOTIMPL, },
-/* pmd27 */ { PFM_REG_NOTIMPL, },
-/* pmd28 */ { PFM_REG_NOTIMPL, },
-/* pmd29 */ { PFM_REG_NOTIMPL, },
-/* pmd30 */ { PFM_REG_NOTIMPL, },
-/* pmd31 */ { PFM_REG_NOTIMPL, },
-/* pmd32 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(33)|RDEP(36),0, 0, 0}, {RDEP(40),0, 0, 0}},
-/* pmd33 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(32)|RDEP(36),0, 0, 0}, {RDEP(40),0, 0, 0}},
-/* pmd34 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(35),0, 0, 0}, {RDEP(37),0, 0, 0}},
-/* pmd35 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(34),0, 0, 0}, {RDEP(37),0, 0, 0}},
-/* pmd36 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP(32)|RDEP(33),0, 0, 0}, {RDEP(40),0, 0, 0}},
-/* pmd37 */ { PFM_REG_NOTIMPL, },
-/* pmd38 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd39 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd40 */ { PFM_REG_NOTIMPL, },
-/* pmd41 */ { PFM_REG_NOTIMPL, },
-/* pmd42 */ { PFM_REG_NOTIMPL, },
-/* pmd43 */ { PFM_REG_NOTIMPL, },
-/* pmd44 */ { PFM_REG_NOTIMPL, },
-/* pmd45 */ { PFM_REG_NOTIMPL, },
-/* pmd46 */ { PFM_REG_NOTIMPL, },
-/* pmd47 */ { PFM_REG_NOTIMPL, },
-/* pmd48 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd49 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd50 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd51 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd52 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd53 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd54 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd55 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd56 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd57 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd58 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd59 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd60 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd61 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd62 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-/* pmd63 */ { PFM_REG_BUFFER, 0, 0x0, -1, NULL, NULL, {RDEP_MONT_ETB,0, 0, 0}, {RDEP(39),0, 0, 0}},
-	    { PFM_REG_END   , 0, 0x0, -1, NULL, NULL, {0,}, {0,}}, /* end marker */
-};
-
-/*
- * PMC reserved fields must have their power-up values preserved
- */
-static int
-pfm_mont_reserved(unsigned int cnum, unsigned long *val, struct pt_regs *regs)
-{
-	unsigned long tmp1, tmp2, ival = *val;
-
-	/* remove reserved areas from user value */
-	tmp1 = ival & PMC_RSVD_MASK(cnum);
-
-	/* get reserved fields values */
-	tmp2 = PMC_DFL_VAL(cnum) & ~PMC_RSVD_MASK(cnum);
-
-	*val = tmp1 | tmp2;
-
-	DPRINT(("pmc[%d]=0x%lx, mask=0x%lx, reset=0x%lx, val=0x%lx\n",
-		  cnum, ival, PMC_RSVD_MASK(cnum), PMC_DFL_VAL(cnum), *val));
-	return 0;
-}
-
-/*
- * task can be NULL if the context is unloaded
- */
-static int
-pfm_mont_pmc_check(struct task_struct *task, pfm_context_t *ctx, unsigned int cnum, unsigned long *val, struct pt_regs *regs)
-{
-	int ret = 0;
-	unsigned long val32 = 0, val38 = 0, val41 = 0;
-	unsigned long tmpval;
-	int check_case1 = 0;
-	int is_loaded;
-
-	/* first preserve the reserved fields */
-	pfm_mont_reserved(cnum, val, regs);
-
-	tmpval = *val;
-
-	/* sanity check */
-	if (ctx == NULL) return -EINVAL;
-
-	is_loaded = ctx->ctx_state == PFM_CTX_LOADED || ctx->ctx_state == PFM_CTX_MASKED;
-
-	/*
-	 * we must clear the debug registers if pmc41 has a value which enable
-	 * memory pipeline event constraints. In this case we need to clear the
-	 * the debug registers if they have not yet been accessed. This is required
-	 * to avoid picking stale state.
-	 * PMC41 is "active" if:
-	 * 	one of the pmc41.cfg_dtagXX field is different from 0x3
-	 * AND
-	 * 	at the corresponding pmc41.en_dbrpXX is set.
-	 * AND
-	 *	ctx_fl_using_dbreg == 0  (i.e., dbr not yet used)
-	 */
-	DPRINT(("cnum=%u val=0x%lx, using_dbreg=%d loaded=%d\n", cnum, tmpval, ctx->ctx_fl_using_dbreg, is_loaded));
-
-	if (cnum == 41 && is_loaded 
-	    && (tmpval & 0x1e00000000000UL) && (tmpval & 0x18181818UL) != 0x18181818UL && ctx->ctx_fl_using_dbreg == 0) {
-
-		DPRINT(("pmc[%d]=0x%lx has active pmc41 settings, clearing dbr\n", cnum, tmpval));
-
-		/* don't mix debug with perfmon */
-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
-
-		/*
-		 * a count of 0 will mark the debug registers if:
-		 * AND
-		 */
-		ret = pfm_write_ibr_dbr(PFM_DATA_RR, ctx, NULL, 0, regs);
-		if (ret) return ret;
-	}
-	/*
-	 * we must clear the (instruction) debug registers if:
-	 * 	pmc38.ig_ibrpX is 0 (enabled)
-	 * AND
-	 *	ctx_fl_using_dbreg == 0  (i.e., dbr not yet used)
-	 */
-	if (cnum == 38 && is_loaded && ((tmpval & 0x492UL) != 0x492UL) && ctx->ctx_fl_using_dbreg == 0) {
-
-		DPRINT(("pmc38=0x%lx has active pmc38 settings, clearing ibr\n", tmpval));
-
-		/* don't mix debug with perfmon */
-		if (task && (task->thread.flags & IA64_THREAD_DBG_VALID) != 0) return -EINVAL;
-
-		/*
-		 * a count of 0 will mark the debug registers as in use and also
-		 * ensure that they are properly cleared.
-		 */
-		ret = pfm_write_ibr_dbr(PFM_CODE_RR, ctx, NULL, 0, regs);
-		if (ret) return ret;
-
-	}
-	switch(cnum) {
-		case  32: val32 = *val;
-			  val38 = ctx->ctx_pmcs[38];
-			  val41 = ctx->ctx_pmcs[41];
-			  check_case1 = 1;
-			  break;
-		case  38: val38 = *val;
-			  val32 = ctx->ctx_pmcs[32];
-			  val41 = ctx->ctx_pmcs[41];
-			  check_case1 = 1;
-			  break;
-		case  41: val41 = *val;
-			  val32 = ctx->ctx_pmcs[32];
-			  val38 = ctx->ctx_pmcs[38];
-			  check_case1 = 1;
-			  break;
-	}
-	/* check illegal configuration which can produce inconsistencies in tagging
-	 * i-side events in L1D and L2 caches
-	 */
-	if (check_case1) {
-		ret =   (((val41 >> 45) & 0xf) == 0 && ((val32>>57) & 0x1) == 0)
-		     && ((((val38>>1) & 0x3) == 0x2 || ((val38>>1) & 0x3) == 0)
-		     ||  (((val38>>4) & 0x3) == 0x2 || ((val38>>4) & 0x3) == 0));
-		if (ret) {
-			DPRINT(("invalid config pmc38=0x%lx pmc41=0x%lx pmc32=0x%lx\n", val38, val41, val32));
-			return -EINVAL;
-		}
-	}
-	*val = tmpval;
-	return 0;
-}
-
-/*
- * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
- */
-static pmu_config_t pmu_conf_mont={
-	.pmu_name        = "Montecito",
-	.pmu_family      = 0x20,
-	.flags           = PFM_PMU_IRQ_RESEND,
-	.ovfl_val        = (1UL << 47) - 1,
-	.pmd_desc        = pfm_mont_pmd_desc,
-	.pmc_desc        = pfm_mont_pmc_desc,
-	.num_ibrs        = 8,
-	.num_dbrs        = 8,
-	.use_rr_dbregs   = 1 /* debug register are use for range retrictions */
-};
Index: linux-2.6.31-master/arch/ia64/kernel/process.c
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/process.c
+++ linux-2.6.31-master/arch/ia64/kernel/process.c
@@ -29,6 +29,7 @@
 #include <linux/kdebug.h>
 #include <linux/utsname.h>
 #include <linux/tracehook.h>
+#include <linux/perfmon_kern.h>
 
 #include <asm/cpu.h>
 #include <asm/delay.h>
@@ -46,10 +47,6 @@
 
 #include "entry.h"
 
-#ifdef CONFIG_PERFMON
-# include <asm/perfmon.h>
-#endif
-
 #include "sigframe.h"
 
 void (*ia64_mark_idle)(int);
@@ -168,6 +165,10 @@ console_print(const char *s)
 	printk(KERN_EMERG "%s", s);
 }
 
+/*
+ * do_notify_resume_user():
+ *	Called from notify_resume_user at entry.S, with interrupts disabled.
+ */
 void
 do_notify_resume_user(sigset_t *unused, struct sigscratch *scr, long in_syscall)
 {
@@ -181,14 +182,9 @@ do_notify_resume_user(sigset_t *unused, 
 		return;
 	}
 
-#ifdef CONFIG_PERFMON
-	if (current->thread.pfm_needs_checking)
-		/*
-		 * Note: pfm_handle_work() allow us to call it with interrupts
-		 * disabled, and may enable interrupts within the function.
-		 */
-		pfm_handle_work();
-#endif
+	/* process perfmon asynchronous work (e.g. block thread or reset) */
+	if (test_thread_flag(TIF_PERFMON_WORK))
+		pfm_handle_work(task_pt_regs(current));
 
 	/* deal with pending signal delivery */
 	if (test_thread_flag(TIF_SIGPENDING)) {
@@ -212,22 +208,15 @@ do_notify_resume_user(sigset_t *unused, 
 	local_irq_disable();	/* force interrupt disable */
 }
 
-static int pal_halt        = 1;
 static int can_do_pal_halt = 1;
 
 static int __init nohalt_setup(char * str)
 {
-	pal_halt = can_do_pal_halt = 0;
+	can_do_pal_halt = 0;
 	return 1;
 }
 __setup("nohalt", nohalt_setup);
 
-void
-update_pal_halt_status(int status)
-{
-	can_do_pal_halt = pal_halt && status;
-}
-
 /*
  * We use this if we don't have any better idle routine..
  */
@@ -236,6 +225,22 @@ default_idle (void)
 {
 	local_irq_enable();
 	while (!need_resched()) {
+#ifdef CONFIG_PERFMON
+		u64 psr = 0;
+		/*
+		 * If requested, we stop the PMU to avoid
+		 * measuring across the core idle loop.
+		 *
+		 * dcr.pp is not modified on purpose
+		 * it is used when coming out of
+		 * safe_halt() via interrupt
+		 */
+		if ((__get_cpu_var(pfm_syst_info) & PFM_ITA_CPUINFO_IDLE_EXCL)) {
+			psr = ia64_getreg(_IA64_REG_PSR);
+			if (psr & IA64_PSR_PP)
+				ia64_rsm(IA64_PSR_PP);
+		}
+#endif
 		if (can_do_pal_halt) {
 			local_irq_disable();
 			if (!need_resched()) {
@@ -244,6 +249,12 @@ default_idle (void)
 			local_irq_enable();
 		} else
 			cpu_relax();
+#ifdef CONFIG_PERFMON
+		if ((__get_cpu_var(pfm_syst_info) & PFM_ITA_CPUINFO_IDLE_EXCL)) {
+			if (psr & IA64_PSR_PP)
+				ia64_ssm(IA64_PSR_PP);
+		}
+#endif
 	}
 }
 
@@ -343,22 +354,9 @@ cpu_idle (void)
 void
 ia64_save_extra (struct task_struct *task)
 {
-#ifdef CONFIG_PERFMON
-	unsigned long info;
-#endif
-
 	if ((task->thread.flags & IA64_THREAD_DBG_VALID) != 0)
 		ia64_save_debug_regs(&task->thread.dbr[0]);
 
-#ifdef CONFIG_PERFMON
-	if ((task->thread.flags & IA64_THREAD_PM_VALID) != 0)
-		pfm_save_regs(task);
-
-	info = __get_cpu_var(pfm_syst_info);
-	if (info & PFM_CPUINFO_SYST_WIDE)
-		pfm_syst_wide_update_task(task, info, 0);
-#endif
-
 #ifdef CONFIG_IA32_SUPPORT
 	if (IS_IA32_PROCESS(task_pt_regs(task)))
 		ia32_save_state(task);
@@ -368,22 +366,9 @@ ia64_save_extra (struct task_struct *tas
 void
 ia64_load_extra (struct task_struct *task)
 {
-#ifdef CONFIG_PERFMON
-	unsigned long info;
-#endif
-
 	if ((task->thread.flags & IA64_THREAD_DBG_VALID) != 0)
 		ia64_load_debug_regs(&task->thread.dbr[0]);
 
-#ifdef CONFIG_PERFMON
-	if ((task->thread.flags & IA64_THREAD_PM_VALID) != 0)
-		pfm_load_regs(task);
-
-	info = __get_cpu_var(pfm_syst_info);
-	if (info & PFM_CPUINFO_SYST_WIDE) 
-		pfm_syst_wide_update_task(task, info, 1);
-#endif
-
 #ifdef CONFIG_IA32_SUPPORT
 	if (IS_IA32_PROCESS(task_pt_regs(task)))
 		ia32_load_state(task);
@@ -509,8 +494,7 @@ copy_thread(unsigned long clone_flags,
 	 * call behavior where scratch registers are preserved across
 	 * system calls (unless used by the system call itself).
 	 */
-#	define THREAD_FLAGS_TO_CLEAR	(IA64_THREAD_FPH_VALID | IA64_THREAD_DBG_VALID \
-					 | IA64_THREAD_PM_VALID)
+#	define THREAD_FLAGS_TO_CLEAR	(IA64_THREAD_FPH_VALID | IA64_THREAD_DBG_VALID)
 #	define THREAD_FLAGS_TO_SET	0
 	p->thread.flags = ((current->thread.flags & ~THREAD_FLAGS_TO_CLEAR)
 			   | THREAD_FLAGS_TO_SET);
@@ -532,10 +516,8 @@ copy_thread(unsigned long clone_flags,
 	}
 #endif
 
-#ifdef CONFIG_PERFMON
-	if (current->thread.pfm_context)
-		pfm_inherit(p, child_ptregs);
-#endif
+	pfm_copy_thread(p);
+
 	return retval;
 }
 
@@ -744,15 +726,13 @@ exit_thread (void)
 {
 
 	ia64_drop_fpu(current);
-#ifdef CONFIG_PERFMON
-       /* if needed, stop monitoring and flush state to perfmon context */
-	if (current->thread.pfm_context)
-		pfm_exit_thread(current);
+
+	/* if needed, stop monitoring and flush state to perfmon context */
+	pfm_exit_thread();
 
 	/* free debug register resources */
-	if (current->thread.flags & IA64_THREAD_DBG_VALID)
-		pfm_release_debug_registers(current);
-#endif
+	pfm_release_dbregs(current);
+
 	if (IS_IA32_PROCESS(task_pt_regs(current)))
 		ia32_drop_ia64_partial_page_list(current);
 }
Index: linux-2.6.31-master/arch/ia64/kernel/ptrace.c
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/ptrace.c
+++ linux-2.6.31-master/arch/ia64/kernel/ptrace.c
@@ -19,6 +19,7 @@
 #include <linux/security.h>
 #include <linux/audit.h>
 #include <linux/signal.h>
+#include <linux/perfmon_kern.h>
 #include <linux/regset.h>
 #include <linux/elf.h>
 #include <linux/tracehook.h>
@@ -30,9 +31,6 @@
 #include <asm/system.h>
 #include <asm/uaccess.h>
 #include <asm/unwind.h>
-#ifdef CONFIG_PERFMON
-#include <asm/perfmon.h>
-#endif
 
 #include "entry.h"
 
@@ -2104,7 +2102,6 @@ access_uarea(struct task_struct *child, 
 				"address 0x%lx\n", addr);
 		return -1;
 	}
-#ifdef CONFIG_PERFMON
 	/*
 	 * Check if debug registers are used by perfmon. This
 	 * test must be done once we know that we can do the
@@ -2122,9 +2119,8 @@ access_uarea(struct task_struct *child, 
 	 * IA64_THREAD_DBG_VALID. The registers are restored
 	 * by the PMU context switch code.
 	 */
-	if (pfm_use_debug_registers(child))
+	if (pfm_use_dbregs(child))
 		return -1;
-#endif
 
 	if (!(child->thread.flags & IA64_THREAD_DBG_VALID)) {
 		child->thread.flags |= IA64_THREAD_DBG_VALID;
Index: linux-2.6.31-master/arch/ia64/kernel/setup.c
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/setup.c
+++ linux-2.6.31-master/arch/ia64/kernel/setup.c
@@ -45,6 +45,7 @@
 #include <linux/cpufreq.h>
 #include <linux/kexec.h>
 #include <linux/crash_dump.h>
+#include <linux/perfmon_kern.h>
 
 #include <asm/ia32.h>
 #include <asm/machvec.h>
@@ -1080,6 +1081,8 @@ cpu_init (void)
 	}
 	platform_cpu_init();
 	pm_idle = default_idle;
+
+	pfm_init_percpu();
 }
 
 void __init
Index: linux-2.6.31-master/arch/ia64/kernel/smpboot.c
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/smpboot.c
+++ linux-2.6.31-master/arch/ia64/kernel/smpboot.c
@@ -39,6 +39,7 @@
 #include <linux/efi.h>
 #include <linux/percpu.h>
 #include <linux/bitops.h>
+#include <linux/perfmon_kern.h>
 
 #include <asm/atomic.h>
 #include <asm/cache.h>
@@ -375,10 +376,6 @@ smp_callin (void)
 	extern void ia64_init_itm(void);
 	extern volatile int time_keeper_id;
 
-#ifdef CONFIG_PERFMON
-	extern void pfm_init_percpu(void);
-#endif
-
 	cpuid = smp_processor_id();
 	phys_id = hard_smp_processor_id();
 	itc_master = time_keeper_id;
@@ -405,10 +402,6 @@ smp_callin (void)
 
 	ia64_mca_cmc_vector_setup();	/* Setup vector on AP */
 
-#ifdef CONFIG_PERFMON
-	pfm_init_percpu();
-#endif
-
 	local_irq_enable();
 
 	if (!(sal_platform_features & IA64_SAL_PLATFORM_FEATURE_ITC_DRIFT)) {
@@ -744,6 +737,7 @@ int __cpu_disable(void)
 	fixup_irqs();
 	local_flush_tlb_all();
 	cpu_clear(cpu, cpu_callin_map);
+	pfm_cpu_disable();
 	return 0;
 }
 
Index: linux-2.6.31-master/arch/ia64/kernel/sys_ia64.c
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/kernel/sys_ia64.c
+++ linux-2.6.31-master/arch/ia64/kernel/sys_ia64.c
@@ -293,3 +293,11 @@ sys_pciconfig_write (unsigned long bus, 
 }
 
 #endif /* CONFIG_PCI */
+
+#ifndef CONFIG_IA64_PERFMON_COMPAT
+asmlinkage long
+sys_perfmonctl (int fd, int cmd, void __user *arg, int count)
+{
+	return -ENOSYS;
+}
+#endif
Index: linux-2.6.31-master/arch/ia64/lib/Makefile
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/lib/Makefile
+++ linux-2.6.31-master/arch/ia64/lib/Makefile
@@ -13,7 +13,6 @@ lib-y := __divsi3.o __udivsi3.o __modsi3
 
 obj-$(CONFIG_ITANIUM)	+= copy_page.o copy_user.o memcpy.o
 obj-$(CONFIG_MCKINLEY)	+= copy_page_mck.o memcpy_mck.o
-lib-$(CONFIG_PERFMON)	+= carta_random.o
 
 AFLAGS___divdi3.o	=
 AFLAGS___udivdi3.o	= -DUNSIGNED
Index: linux-2.6.31-master/arch/ia64/oprofile/perfmon.c
===================================================================
--- linux-2.6.31-master.orig/arch/ia64/oprofile/perfmon.c
+++ linux-2.6.31-master/arch/ia64/oprofile/perfmon.c
@@ -10,25 +10,30 @@
 #include <linux/kernel.h>
 #include <linux/oprofile.h>
 #include <linux/sched.h>
-#include <asm/perfmon.h>
+#include <linux/module.h>
+#include <linux/perfmon_kern.h>
 #include <asm/ptrace.h>
 #include <asm/errno.h>
 
 static int allow_ints;
 
 static int
-perfmon_handler(struct task_struct *task, void *buf, pfm_ovfl_arg_t *arg,
-                struct pt_regs *regs, unsigned long stamp)
+perfmon_handler(struct pfm_context *ctx,
+		unsigned long ip, u64 stamp, void *data)
 {
-	int event = arg->pmd_eventid;
+	struct pt_regs *regs;
+	struct pfm_ovfl_arg *arg;
+
+ 	regs = data;
+	arg = &ctx->ovfl_arg;
  
-	arg->ovfl_ctrl.bits.reset_ovfl_pmds = 1;
+	arg->ovfl_ctrl = PFM_OVFL_CTRL_RESET;
 
 	/* the owner of the oprofile event buffer may have exited
 	 * without perfmon being shutdown (e.g. SIGSEGV)
 	 */
 	if (allow_ints)
-		oprofile_add_sample(regs, event);
+		oprofile_add_sample(regs, arg->pmd_eventid);
 	return 0;
 }
 
@@ -45,14 +50,11 @@ static void perfmon_stop(void)
 	allow_ints = 0;
 }
 
-
-#define OPROFILE_FMT_UUID { \
-	0x77, 0x7a, 0x6e, 0x61, 0x20, 0x65, 0x73, 0x69, 0x74, 0x6e, 0x72, 0x20, 0x61, 0x65, 0x0a, 0x6c }
-
-static pfm_buffer_fmt_t oprofile_fmt = {
- 	.fmt_name 	    = "oprofile_format",
- 	.fmt_uuid	    = OPROFILE_FMT_UUID,
- 	.fmt_handler	    = perfmon_handler,
+static struct pfm_smpl_fmt oprofile_fmt = {
+	.fmt_name = "OProfile",
+	.fmt_handler = perfmon_handler,
+	.fmt_flags = PFM_FMT_BUILTIN_FLAG,
+	.owner = THIS_MODULE
 };
 
 
@@ -77,7 +79,7 @@ static int using_perfmon;
 
 int perfmon_init(struct oprofile_operations *ops)
 {
-	int ret = pfm_register_buffer_fmt(&oprofile_fmt);
+	int ret = pfm_fmt_register(&oprofile_fmt);
 	if (ret)
 		return -ENODEV;
 
@@ -90,10 +92,10 @@ int perfmon_init(struct oprofile_operati
 }
 
 
-void perfmon_exit(void)
+void __exit perfmon_exit(void)
 {
 	if (!using_perfmon)
 		return;
 
-	pfm_unregister_buffer_fmt(oprofile_fmt.fmt_uuid);
+	pfm_fmt_unregister(&oprofile_fmt);
 }
Index: linux-2.6.31-master/arch/ia64/perfmon/Kconfig
===================================================================
--- /dev/null
+++ linux-2.6.31-master/arch/ia64/perfmon/Kconfig
@@ -0,0 +1,67 @@
+menu "Hardware Performance Monitoring support"
+config PERFMON
+	bool "Perfmon2 performance monitoring interface"
+	default n
+	help
+	Enables the perfmon2 interface to access the hardware
+	performance counters. See <http://perfmon2.sf.net/> for
+	more details.
+
+config PERFMON_DEBUG
+	bool "Perfmon debugging"
+	default n
+	depends on PERFMON
+	help
+	Enables perfmon debugging support
+
+config PERFMON_DEBUG_FS
+	bool "Enable perfmon statistics reporting via debugfs"
+	default y
+	depends on PERFMON && DEBUG_FS
+	help
+	Enable collection and reporting of perfmon timing statistics under
+	debugfs. This is used for debugging and performance analysis of the
+	subsystem. The debugfs filesystem must be mounted.
+
+config IA64_PERFMON_COMPAT
+	bool "Enable old perfmon-2 compatbility mode"
+	default n
+	depends on PERFMON
+	help
+	Enable this option to allow performance tools which used the old
+	perfmon-2 interface to continue to work. Old tools are those using
+	the obsolete commands and arguments. Check your programs and look
+	in include/asm-ia64/perfmon_compat.h for more information.
+
+config IA64_PERFMON_GENERIC
+	tristate "Generic IA-64 PMU support"
+	depends on PERFMON
+	default n
+	help
+	Enables generic IA-64 PMU support.
+	The generic PMU is defined by the IA-64 architecture document.
+	This option should only be necessary when running with a PMU that
+	is not yet explicitely supported. Even then, there is no guarantee
+	that this support will work.
+
+config IA64_PERFMON_ITANIUM
+	tristate "Itanium (Merced) Performance Monitoring support"
+	depends on PERFMON
+	default n
+	help
+	Enables Itanium (Merced) PMU support.
+
+config IA64_PERFMON_MCKINLEY
+	tristate "Itanium 2 (McKinley) Performance Monitoring  support"
+	depends on PERFMON
+	default n
+	help
+	Enables Itanium 2 (McKinley, Madison, Deerfield) PMU support.
+
+config IA64_PERFMON_MONTECITO
+	tristate "Itanium 2 9000 (Montecito) Performance Monitoring  support"
+	depends on PERFMON
+	default n
+	help
+	Enables support for Itanium 2 9000 (Montecito) PMU.
+endmenu
Index: linux-2.6.31-master/arch/ia64/perfmon/Makefile
===================================================================
--- /dev/null
+++ linux-2.6.31-master/arch/ia64/perfmon/Makefile
@@ -0,0 +1,11 @@
+#
+# Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+# Contributed by Stephane Eranian <eranian@hpl.hp.com>
+#
+obj-$(CONFIG_PERFMON)			+= perfmon.o
+obj-$(CONFIG_IA64_PERFMON_COMPAT)	+= perfmon_default_smpl.o \
+					   perfmon_compat.o
+obj-$(CONFIG_IA64_PERFMON_GENERIC)	+= perfmon_generic.o
+obj-$(CONFIG_IA64_PERFMON_ITANIUM)	+= perfmon_itanium.o
+obj-$(CONFIG_IA64_PERFMON_MCKINLEY)	+= perfmon_mckinley.o
+obj-$(CONFIG_IA64_PERFMON_MONTECITO)	+= perfmon_montecito.o
Index: linux-2.6.31-master/arch/ia64/perfmon/perfmon.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/arch/ia64/perfmon/perfmon.c
@@ -0,0 +1,937 @@
+/*
+ * This file implements the IA-64 specific
+ * support for the perfmon2 interface
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/perfmon_kern.h>
+
+struct pfm_arch_session {
+	u32	pfs_sys_use_dbr;    /* syswide session uses dbr */
+	u32	pfs_ptrace_use_dbr; /* a thread uses dbr via ptrace()*/
+};
+
+DEFINE_PER_CPU(u32, pfm_syst_info);
+
+static struct pfm_arch_session pfm_arch_sessions;
+static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_arch_sessions_lock);
+
+static inline void pfm_clear_psr_pp(void)
+{
+	ia64_rsm(IA64_PSR_PP);
+}
+
+static inline void pfm_set_psr_pp(void)
+{
+	ia64_ssm(IA64_PSR_PP);
+}
+
+static inline void pfm_clear_psr_up(void)
+{
+	ia64_rsm(IA64_PSR_UP);
+}
+
+static inline void pfm_set_psr_up(void)
+{
+	ia64_ssm(IA64_PSR_UP);
+}
+
+static inline void pfm_set_psr_l(u64 val)
+{
+	ia64_setreg(_IA64_REG_PSR_L, val);
+}
+
+static inline void pfm_restore_ibrs(u64 *ibrs, unsigned int nibrs)
+{
+	unsigned int i;
+
+	for (i = 0; i < nibrs; i++) {
+		ia64_set_ibr(i, ibrs[i]);
+		ia64_dv_serialize_instruction();
+	}
+	ia64_srlz_i();
+}
+
+static inline void pfm_restore_dbrs(u64 *dbrs, unsigned int ndbrs)
+{
+	unsigned int i;
+
+	for (i = 0; i < ndbrs; i++) {
+		ia64_set_dbr(i, dbrs[i]);
+		ia64_dv_serialize_data();
+	}
+	ia64_srlz_d();
+}
+
+irqreturn_t pmu_interrupt_handler(int irq, void *arg)
+{
+	struct pt_regs *regs;
+	regs = get_irq_regs();
+	irq_enter();
+	pfm_interrupt_handler(instruction_pointer(regs), regs);
+	irq_exit();
+	return IRQ_HANDLED;
+}
+static struct irqaction perfmon_irqaction = {
+	.handler = pmu_interrupt_handler,
+	.flags = IRQF_DISABLED, /* means keep interrupts masked */
+	.name = "perfmon"
+};
+
+void pfm_arch_quiesce_pmu_percpu(void)
+{
+	u64 dcr;
+	/*
+	 * make sure no measurement is active
+	 * (may inherit programmed PMCs from EFI).
+	 */
+	pfm_clear_psr_pp();
+	pfm_clear_psr_up();
+
+	/*
+	 * ensure dcr.pp is cleared
+	 */
+	dcr = ia64_getreg(_IA64_REG_CR_DCR);
+	ia64_setreg(_IA64_REG_CR_DCR, dcr & ~IA64_DCR_PP);
+
+	/*
+	 * we run with the PMU not frozen at all times
+	 */
+	ia64_set_pmc(0, 0);
+	ia64_srlz_d();
+}
+
+void pfm_arch_init_percpu(void)
+{
+	pfm_arch_quiesce_pmu_percpu();
+	/*
+	 * program PMU interrupt vector
+	 */
+	ia64_setreg(_IA64_REG_CR_PMV, IA64_PERFMON_VECTOR);
+	ia64_srlz_d();
+}
+
+int pfm_arch_context_create(struct pfm_context *ctx, u32 ctx_flags)
+{
+	struct pfm_arch_context *ctx_arch;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	ctx_arch->flags.use_dbr = 0;
+	ctx_arch->flags.insecure = (ctx_flags & PFM_ITA_FL_INSECURE) ? 1: 0;
+
+	PFM_DBG("insecure=%d", ctx_arch->flags.insecure);
+
+	return 0;
+}
+
+/*
+ * Called from pfm_ctxsw(). Task is guaranteed to be current.
+ * Context is locked. Interrupts are masked. Monitoring may be active.
+ * PMU access is guaranteed. PMC and PMD registers are live in PMU.
+ *
+ * Return:
+ * 	non-zero : did not save PMDs (as part of stopping the PMU)
+ * 	       0 : saved PMDs (no need to save them in caller)
+ */
+int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pfm_event_set *set;
+	u64 psr, tmp;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	set = ctx->active_set;
+
+	/*
+	 * save current PSR: needed because we modify it
+	 */
+	ia64_srlz_d();
+	psr = ia64_getreg(_IA64_REG_PSR);
+
+	/*
+	 * stop monitoring:
+	 * This is the last instruction which may generate an overflow
+	 *
+	 * we do not clear ipsr.up
+	 */
+	pfm_clear_psr_up();
+	ia64_srlz_d();
+
+	/*
+	 * extract overflow status bits
+	 */
+	tmp =  ia64_get_pmc(0) & ~0xf;
+
+	/*
+	 * keep a copy of psr.up (for reload)
+	 */
+	ctx_arch->ctx_saved_psr_up = psr & IA64_PSR_UP;
+
+	/*
+	 * save overflow status bits
+	 */
+	set->povfl_pmds[0] = tmp;
+
+	/*
+	 * record how many pending overflows
+	 * XXX: assume identity mapping for counters
+	 */
+	set->npend_ovfls = ia64_popcnt(tmp);
+
+	/*
+	 * make sure the PMU is unfrozen for the next task
+	 */
+	if (set->npend_ovfls) {
+		ia64_set_pmc(0, 0);
+		ia64_srlz_d();
+	}
+	return 1;
+}
+
+/*
+ * Called from pfm_ctxsw(). Task is guaranteed to be current.
+ * set cannot be NULL. Context is locked. Interrupts are masked.
+ * Caller has already restored all PMD and PMC registers.
+ *
+ * must reactivate monitoring
+ */
+void pfm_arch_ctxswin_thread(struct task_struct *task, struct pfm_context *ctx)
+{
+	struct pfm_arch_context *ctx_arch;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	/*
+	 * when monitoring is not explicitly started
+	 * then psr_up = 0, in which case we do not
+	 * need to restore
+	 */
+	if (likely(ctx_arch->ctx_saved_psr_up)) {
+		pfm_set_psr_up();
+		ia64_srlz_d();
+	}
+}
+
+int pfm_arch_reserve_session(struct pfm_context *ctx, u32 cpu)
+{
+	struct pfm_arch_context *ctx_arch;
+	int is_system;
+	int ret = 0;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	is_system = ctx->flags.system;
+
+	spin_lock(&pfm_arch_sessions_lock);
+
+	if (is_system && ctx_arch->flags.use_dbr) {
+		PFM_DBG("syswide context uses dbregs");
+
+		if (pfm_arch_sessions.pfs_ptrace_use_dbr) {
+			PFM_DBG("cannot reserve syswide context: "
+				  "dbregs in use by ptrace");
+			ret = -EBUSY;
+		} else {
+			pfm_arch_sessions.pfs_sys_use_dbr++;
+		}
+	}
+	spin_unlock(&pfm_arch_sessions_lock);
+
+	return ret;
+}
+
+void pfm_arch_release_session(struct pfm_context *ctx, u32 cpu)
+{
+	struct pfm_arch_context *ctx_arch;
+	int is_system;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	is_system = ctx->flags.system;
+
+	spin_lock(&pfm_arch_sessions_lock);
+
+	if (is_system && ctx_arch->flags.use_dbr)
+		pfm_arch_sessions.pfs_sys_use_dbr--;
+	spin_unlock(&pfm_arch_sessions_lock);
+}
+
+/*
+ * function called from pfm_load_context_*(). Task is not guaranteed to be
+ * current task. If not then other task is guaranteed stopped and off any CPU.
+ * context is locked and interrupts are masked.
+ *
+ * On PFM_LOAD_CONTEXT, the interface guarantees monitoring is stopped.
+ *
+ * For system-wide task is NULL
+ */
+int pfm_arch_load_context(struct pfm_context *ctx)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pt_regs *regs;
+	int ret = 0;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	/*
+	 * cannot load a context which is using range restrictions,
+	 * into a thread that is being debugged.
+	 *
+	 * if one set out of several is using the debug registers, then
+	 * we assume the context as whole is using them.
+	 */
+	if (ctx_arch->flags.use_dbr) {
+		if (ctx->flags.system) {
+			spin_lock(&pfm_arch_sessions_lock);
+
+			if (pfm_arch_sessions.pfs_ptrace_use_dbr) {
+				PFM_DBG("cannot reserve syswide context: "
+					"dbregs in use by ptrace");
+				ret = -EBUSY;
+			} else {
+				pfm_arch_sessions.pfs_sys_use_dbr++;
+				PFM_DBG("pfs_sys_use_dbr=%u",
+					pfm_arch_sessions.pfs_sys_use_dbr);
+			}
+			spin_unlock(&pfm_arch_sessions_lock);
+
+		} else if (ctx->task->thread.flags & IA64_THREAD_DBG_VALID) {
+			PFM_DBG("load_pid [%d] thread is debugged, cannot "
+				  "use range restrictions", ctx->task->pid);
+			ret = -EBUSY;
+		}
+		if (ret)
+			return ret;
+	}
+
+	/*
+	 * We need to intervene on context switch to toggle the
+	 * psr.pp bit in system-wide. As such, we set the TIF
+	 * flag so that pfm_arch_ctxswout_sys() and the
+	 * pfm_arch_ctxswin_sys() functions get called
+	 * from pfm_ctxsw_sys();
+	 */
+	if (ctx->flags.system) {
+		set_thread_flag(TIF_PERFMON_CTXSW);
+		PFM_DBG("[%d] set TIF", current->pid);
+		return 0;
+	}
+
+	regs = task_pt_regs(ctx->task);
+
+	/*
+	 * self-monitoring systematically allows user level control
+	 */
+	if (ctx->task != current) {
+		/*
+		 * when not current, task is stopped, so this is safe
+		 */
+		ctx_arch->ctx_saved_psr_up = 0;
+		ia64_psr(regs)->up = ia64_psr(regs)->pp = 0;
+	} else
+		ctx_arch->flags.insecure = 1;
+
+	/*
+	 * allow user level control (start/stop/read pmd) if:
+	 * 	- self-monitoring
+	 * 	- requested at context creation (PFM_IA64_FL_INSECURE)
+	 *
+	 * There is not security hole with PFM_IA64_FL_INSECURE because
+	 * when not self-monitored, the caller must have permissions to
+	 * attached to the task.
+	 */
+	if (ctx_arch->flags.insecure) {
+		ia64_psr(regs)->sp = 0;
+		PFM_DBG("clearing psr.sp for [%d]", ctx->task->pid);
+	}
+	return 0;
+}
+
+int pfm_arch_setfl_sane(struct pfm_context *ctx, u32 flags)
+{
+#define PFM_SETFL_BOTH_SWITCH	(PFM_SETFL_OVFL_SWITCH|PFM_SETFL_TIME_SWITCH)
+#define PFM_ITA_SETFL_BOTH_INTR	(PFM_ITA_SETFL_INTR_ONLY|\
+				 PFM_ITA_SETFL_EXCL_INTR)
+
+/* exclude return value field */
+#define PFM_SETFL_ALL_MASK	(PFM_ITA_SETFL_BOTH_INTR \
+				 | PFM_SETFL_BOTH_SWITCH	\
+				 | PFM_ITA_SETFL_IDLE_EXCL)
+
+	if ((flags & ~PFM_SETFL_ALL_MASK)) {
+		PFM_DBG("invalid flags=0x%x", flags);
+		return -EINVAL;
+	}
+
+	if ((flags & PFM_ITA_SETFL_BOTH_INTR) == PFM_ITA_SETFL_BOTH_INTR) {
+		PFM_DBG("both excl intr and ontr only are set");
+		return -EINVAL;
+	}
+
+	if ((flags & PFM_ITA_SETFL_IDLE_EXCL) && !ctx->flags.system) {
+		PFM_DBG("idle exclude flag only for system-wide context");
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * function called from pfm_unload_context_*(). Context is locked.
+ * interrupts are masked. task is not guaranteed to be current task.
+ * Access to PMU is not guaranteed.
+ *
+ * function must do whatever arch-specific action is required on unload
+ * of a context.
+ *
+ * called for both system-wide and per-thread. task is NULL for ssytem-wide
+ */
+void pfm_arch_unload_context(struct pfm_context *ctx)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pt_regs *regs;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	if (ctx->flags.system) {
+		/*
+		 * disable context switch hook
+		 */
+		clear_thread_flag(TIF_PERFMON_CTXSW);
+
+		if (ctx_arch->flags.use_dbr) {
+			spin_lock(&pfm_arch_sessions_lock);
+			pfm_arch_sessions.pfs_sys_use_dbr--;
+			PFM_DBG("sys_use_dbr=%u", pfm_arch_sessions.pfs_sys_use_dbr);
+			spin_unlock(&pfm_arch_sessions_lock);
+		}
+	} else {
+		regs = task_pt_regs(ctx->task);
+
+		/*
+		 * cancel user level control for per-task context
+		 */
+		ia64_psr(regs)->sp = 1;
+		PFM_DBG("setting psr.sp for [%d]", ctx->task->pid);
+	}
+}
+
+/*
+ * mask monitoring by setting the privilege level to 0
+ * we cannot use psr.pp/psr.up for this, it is controlled by
+ * the user
+ */
+void pfm_arch_mask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info;
+	unsigned long mask;
+	unsigned int i;
+
+	arch_info = pfm_pmu_info();
+	/*
+	 * as an optimization we look at the first 64 PMC
+	 * registers only starting at PMC4.
+	 */
+	mask = arch_info->mask_pmcs[0] >> PFM_ITA_FCNTR;
+	for (i = PFM_ITA_FCNTR; mask; i++, mask >>= 1) {
+		if (likely(mask & 0x1))
+			ia64_set_pmc(i, set->pmcs[i] & ~0xfUL);
+	}
+	/*
+	 * make changes visisble
+	 */
+	ia64_srlz_d();
+}
+
+/*
+ * function called from pfm_switch_sets(), pfm_context_load_thread(),
+ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
+ * context is locked. Interrupts are masked. set cannot be NULL.
+ * Access to the PMU is guaranteed.
+ *
+ * function must restore all PMD registers from set.
+ */
+void pfm_arch_restore_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_context *ctx_arch;
+	u64 *mask;
+	int i;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	mask = ctx_arch->flags.insecure ? ctx->regs.rw_pmds : set->used_pmds;
+	/*
+	 * must restore all implemented read-write PMDS to avoid leaking
+	 * information especially when PFM_IA64_FL_INSECURE is set.
+	 *
+	 * XXX: should check PFM_IA64_FL_INSECURE==0 and use used_pmd instead
+	 */
+	for_each_bit(i, cast_ulp(mask), ctx->regs.max_pmd)
+		pfm_arch_write_pmd(ctx, i, set->pmds[i].value);
+
+	ia64_srlz_d();
+}
+
+/*
+ * function called from pfm_switch_sets(), pfm_context_load_thread(),
+ * pfm_context_load_sys(), pfm_ctxsw(), pfm_switch_sets()
+ * context is locked. Interrupts are masked. set cannot be NULL.
+ * Access to the PMU is guaranteed.
+ *
+ * function must restore all PMC registers from set if needed
+ */
+void pfm_arch_restore_pmcs(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct pfm_arch_pmu_info *arch_info;
+	u64 mask2 = 0, val, plm;
+	unsigned long impl_mask, mask_pmcs;
+	unsigned int i;
+
+	arch_info = pfm_pmu_info();
+	/*
+	 * as an optimization we only look at the first 64
+	 * PMC registers. In fact, we should never scan the
+	 * entire impl_pmcs because ibr/dbr are implemented
+	 * separately.
+	 *
+	 * always skip PMC0-PMC3. PMC0 taken care of when saving
+	 * state. PMC1-PMC3 not used until we get counters in
+	 * the 60 and above index range.
+	 */
+	impl_mask = ctx->regs.pmcs[0] >> PFM_ITA_FCNTR;
+	mask_pmcs = arch_info->mask_pmcs[0] >> PFM_ITA_FCNTR;
+	plm = ctx->state == PFM_CTX_MASKED ? ~0xf : ~0x0;
+
+	for (i = PFM_ITA_FCNTR;
+	     impl_mask;
+	     i++, impl_mask >>= 1, mask_pmcs >>= 1) {
+		if (likely(impl_mask & 0x1)) {
+			mask2 = mask_pmcs & 0x1 ? plm : ~0;
+			val = set->pmcs[i] & mask2;
+			ia64_set_pmc(i, val);
+			PFM_DBG_ovfl("pmc%u=0x%lx", i, val);
+		}
+	}
+	/*
+	 * restore DBR/IBR
+	 */
+	if (set->priv_flags & PFM_ITA_SETFL_USE_DBR) {
+		pfm_restore_ibrs(set->pmcs+256, 8);
+		pfm_restore_dbrs(set->pmcs+264, 8);
+	}
+	ia64_srlz_d();
+}
+
+void pfm_arch_unmask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	u64 psr;
+	int is_system;
+
+	is_system = ctx->flags.system;
+
+	psr = ia64_getreg(_IA64_REG_PSR);
+
+	/*
+	 * monitoring is masked via the PMC.plm
+	 *
+	 * As we restore their value, we do not want each counter to
+	 * restart right away. We stop monitoring using the PSR,
+	 * restore the PMC (and PMD) and then re-establish the psr
+	 * as it was. Note that there can be no pending overflow at
+	 * this point, because monitoring is still MASKED.
+	 *
+	 * Because interrupts are masked we can avoid changing
+	 * DCR.pp.
+	 */
+	if (is_system)
+		pfm_clear_psr_pp();
+	else
+		pfm_clear_psr_up();
+
+	ia64_srlz_d();
+
+	pfm_arch_restore_pmcs(ctx, set);
+
+	/*
+	 * restore psr
+	 *
+	 * monitoring may start right now but interrupts
+	 * are still masked
+	 */
+	pfm_set_psr_l(psr);
+	ia64_srlz_d();
+}
+
+/*
+ * Called from pfm_stop()
+ *
+ * For per-thread:
+ *   task is not necessarily current. If not current task, then
+ *   task is guaranteed stopped and off any cpu. Access to PMU
+ *   is not guaranteed. Interrupts are masked. Context is locked.
+ *   Set is the active set.
+ *
+ * must disable active monitoring. ctx cannot be NULL
+ */
+void pfm_arch_stop(struct task_struct *task, struct pfm_context *ctx)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pt_regs *regs;
+	u64 dcr, psr;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	regs = task_pt_regs(task);
+
+	if (!ctx->flags.system) {
+		/*
+		 * in ZOMBIE state we always have task == current due to
+		 * pfm_exit_thread()
+		 */
+		ia64_psr(regs)->up = 0;
+		ctx_arch->ctx_saved_psr_up = 0;
+
+		/*
+		 * in case of ZOMBIE state, there is no unload to clear
+		 * insecure monitoring, so we do it in stop instead.
+		 */
+		if (ctx->state == PFM_CTX_ZOMBIE)
+			ia64_psr(regs)->sp = 1;
+
+		if (task == current) {
+			pfm_clear_psr_up();
+			ia64_srlz_d();
+		}
+	} else if (ctx->flags.started) { /* do not stop twice */
+		dcr = ia64_getreg(_IA64_REG_CR_DCR);
+		psr = ia64_getreg(_IA64_REG_PSR);
+
+		ia64_psr(regs)->pp = 0;
+		ia64_setreg(_IA64_REG_CR_DCR, dcr & ~IA64_DCR_PP);
+		pfm_clear_psr_pp();
+		ia64_srlz_d();
+
+		if (ctx->active_set->flags & PFM_ITA_SETFL_IDLE_EXCL) {
+			PFM_DBG("disabling idle exclude");
+			__get_cpu_var(pfm_syst_info) &= ~PFM_ITA_CPUINFO_IDLE_EXCL;
+		}
+	}
+}
+
+/*
+ * called from pfm_start()
+ *
+ * Interrupts are masked. Context is locked. Set is the active set.
+ *
+ * For per-thread:
+ * 	Task is not necessarily current. If not current task, then task
+ * 	is guaranteed stopped and off any cpu. No access to PMU is task
+ *	is not current.
+ *
+ * For system-wide:
+ * 	task is always current
+ *
+ * must enable active monitoring.
+ */
+void pfm_arch_start(struct task_struct *task, struct pfm_context *ctx)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pt_regs *regs;
+	u64 dcr, dcr_pp, psr_pp;
+	u32 flags;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	regs = task_pt_regs(task);
+	flags = ctx->active_set->flags;
+
+	/*
+	 * per-thread mode
+	 */
+	if (!ctx->flags.system) {
+
+		ia64_psr(regs)->up = 1;
+
+		if (task == current) {
+			pfm_set_psr_up();
+			ia64_srlz_d();
+		} else {
+			/*
+			 * activate monitoring at next ctxswin
+			 */
+			ctx_arch->ctx_saved_psr_up = IA64_PSR_UP;
+		}
+		return;
+	}
+
+	/*
+	 * system-wide mode
+	 */
+	dcr = ia64_getreg(_IA64_REG_CR_DCR);
+	if (flags & PFM_ITA_SETFL_INTR_ONLY) {
+		dcr_pp = 1;
+		psr_pp = 0;
+	} else if (flags & PFM_ITA_SETFL_EXCL_INTR) {
+		dcr_pp = 0;
+		psr_pp = 1;
+	} else {
+		dcr_pp = psr_pp = 1;
+	}
+	PFM_DBG("dcr_pp=%lu psr_pp=%lu", dcr_pp, psr_pp);
+
+	/*
+	 * update dcr_pp and psr_pp
+	 */
+	if (dcr_pp)
+		ia64_setreg(_IA64_REG_CR_DCR, dcr | IA64_DCR_PP);
+	else
+		ia64_setreg(_IA64_REG_CR_DCR, dcr & ~IA64_DCR_PP);
+
+	if (psr_pp) {
+		pfm_set_psr_pp();
+		ia64_psr(regs)->pp = 1;
+	} else {
+		pfm_clear_psr_pp();
+		ia64_psr(regs)->pp = 0;
+	}
+	ia64_srlz_d();
+
+	if (ctx->active_set->flags & PFM_ITA_SETFL_IDLE_EXCL) {
+		PFM_DBG("enable idle exclude");
+		__get_cpu_var(pfm_syst_info) |= PFM_ITA_CPUINFO_IDLE_EXCL;
+	}
+}
+
+/*
+ * Only call this function when a process is trying to
+ * write the debug registers (reading is always allowed)
+ * called from arch/ia64/kernel/ptrace.c:access_uarea()
+ */
+int __pfm_use_dbregs(struct task_struct *task)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct pfm_context *ctx;
+	unsigned long flags;
+	int ret = 0;
+
+	PFM_DBG("called for [%d]", task->pid);
+
+	ctx = task->pfm_context;
+
+	/*
+	 * do it only once
+	 */
+	if (task->thread.flags & IA64_THREAD_DBG_VALID) {
+		PFM_DBG("IA64_THREAD_DBG_VALID already set");
+		return 0;
+	}
+	if (ctx) {
+		spin_lock_irqsave(&ctx->lock, flags);
+		ctx_arch = pfm_ctx_arch(ctx);
+
+		if (ctx_arch->flags.use_dbr == 1) {
+			PFM_DBG("PMU using dbregs already, no ptrace access");
+			ret = -1;
+		}
+		spin_unlock_irqrestore(&ctx->lock, flags);
+		if (ret)
+			return ret;
+	}
+
+	spin_lock(&pfm_arch_sessions_lock);
+
+	/*
+	 * We cannot allow setting breakpoints when system wide monitoring
+	 * sessions are using the debug registers.
+	 */
+	if (!pfm_arch_sessions.pfs_sys_use_dbr)
+		pfm_arch_sessions.pfs_ptrace_use_dbr++;
+	else
+		ret = -1;
+
+	PFM_DBG("ptrace_use_dbr=%u  sys_use_dbr=%u by [%d] ret = %d",
+		  pfm_arch_sessions.pfs_ptrace_use_dbr,
+		  pfm_arch_sessions.pfs_sys_use_dbr,
+		  task->pid, ret);
+
+	spin_unlock(&pfm_arch_sessions_lock);
+	if (ret)
+		return ret;
+#ifndef CONFIG_SMP
+	/*
+	 * in UP, we need to check whether the current
+	 * owner of the PMU is not using the debug registers
+	 * for monitoring. Because we are using a lazy
+	 * save on ctxswout, we must force a save in this
+	 * case because the debug registers are being
+	 * modified by another task. We save the current
+	 * PMD registers, and clear ownership. In ctxswin,
+	 * full state will be reloaded.
+	 *
+	 * Note: we overwrite task.
+	 */
+	task = __get_cpu_var(pmu_owner);
+	ctx = __get_cpu_var(pmu_ctx);
+
+	if (task == NULL)
+		return 0;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	if (ctx_arch->flags.use_dbr)
+		pfm_save_pmds_release(ctx);
+#endif
+	return 0;
+}
+
+/*
+ * This function is called for every task that exits with the
+ * IA64_THREAD_DBG_VALID set. This indicates a task which was
+ * able to use the debug registers for debugging purposes via
+ * ptrace(). Therefore we know it was not using them for
+ * perfmormance monitoring, so we only decrement the number
+ * of "ptraced" debug register users to keep the count up to date
+ */
+int __pfm_release_dbregs(struct task_struct *task)
+{
+	int ret;
+
+	spin_lock(&pfm_arch_sessions_lock);
+
+	if (pfm_arch_sessions.pfs_ptrace_use_dbr == 0) {
+		PFM_ERR("invalid release for [%d] ptrace_use_dbr=0", task->pid);
+		ret = -1;
+	}  else {
+		pfm_arch_sessions.pfs_ptrace_use_dbr--;
+		ret = 0;
+	}
+	spin_unlock(&pfm_arch_sessions_lock);
+
+	return ret;
+}
+
+int pfm_ia64_mark_dbregs_used(struct pfm_context *ctx,
+			      struct pfm_event_set *set)
+{
+	struct pfm_arch_context *ctx_arch;
+	struct task_struct *task;
+	struct thread_struct *thread;
+	int ret = 0, state;
+	int i, can_access_pmu = 0;
+	int is_loaded, is_system;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+	state = ctx->state;
+	task = ctx->task;
+	is_loaded = state == PFM_CTX_LOADED || state == PFM_CTX_MASKED;
+	is_system = ctx->flags.system;
+	can_access_pmu = __get_cpu_var(pmu_owner) == task || is_system;
+
+	if (is_loaded == 0)
+		goto done;
+
+	if (is_system == 0) {
+		thread = &(task->thread);
+
+		/*
+		 * cannot use debug registers for montioring if they are
+		 * already used for debugging
+		 */
+		if (thread->flags & IA64_THREAD_DBG_VALID) {
+			PFM_DBG("debug registers already in use for [%d]",
+				  task->pid);
+			return -EBUSY;
+		}
+	}
+
+	/*
+	 * check for debug registers in system wide mode
+	 */
+	spin_lock(&pfm_arch_sessions_lock);
+
+	if (is_system) {
+		if (pfm_arch_sessions.pfs_ptrace_use_dbr)
+			ret = -EBUSY;
+		else
+			pfm_arch_sessions.pfs_sys_use_dbr++;
+	}
+
+	spin_unlock(&pfm_arch_sessions_lock);
+
+	if (ret != 0)
+		return ret;
+
+	/*
+	 * clear hardware registers to make sure we don't
+	 * pick up stale state.
+	 */
+	if (can_access_pmu) {
+		PFM_DBG("clearing ibrs, dbrs");
+		for (i = 0; i < 8; i++) {
+			ia64_set_ibr(i, 0);
+			ia64_dv_serialize_instruction();
+		}
+		ia64_srlz_i();
+		for (i = 0; i < 8; i++) {
+			ia64_set_dbr(i, 0);
+			ia64_dv_serialize_data();
+		}
+		ia64_srlz_d();
+	}
+done:
+	/*
+	 * debug registers are now in use
+	 */
+	ctx_arch->flags.use_dbr = 1;
+	set->priv_flags |= PFM_ITA_SETFL_USE_DBR;
+	PFM_DBG("set%u use_dbr=1", set->id);
+	return 0;
+}
+EXPORT_SYMBOL(pfm_ia64_mark_dbregs_used);
+
+char *pfm_arch_get_pmu_module_name(void)
+{
+	switch (local_cpu_data->family) {
+	case 0x07:
+		return "perfmon_itanium";
+	case 0x1f:
+		return "perfmon_mckinley";
+	case 0x20:
+		return "perfmon_montecito";
+	default:
+		return "perfmon_generic";
+	}
+	return NULL;
+}
+
+/*
+ * global arch-specific intialization, called only once
+ */
+int __init pfm_arch_init(void)
+{
+	int ret;
+
+	spin_lock_init(&pfm_arch_sessions_lock);
+
+#ifdef CONFIG_IA64_PERFMON_COMPAT
+	ret = pfm_ia64_compat_init();
+	if (ret)
+		return ret;
+#endif
+	register_percpu_irq(IA64_PERFMON_VECTOR, &perfmon_irqaction);
+
+
+	return 0;
+}
Index: linux-2.6.31-master/arch/ia64/perfmon/perfmon_compat.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/arch/ia64/perfmon/perfmon_compat.c
@@ -0,0 +1,1221 @@
+/*
+ * This file implements the IA-64 specific
+ * support for the perfmon2 interface
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/file.h>
+#include <linux/fdtable.h>
+#include <linux/seq_file.h>
+#include <linux/vmalloc.h>
+#include <linux/proc_fs.h>
+#include <linux/perfmon_kern.h>
+#include <linux/uaccess.h>
+
+asmlinkage long sys_pfm_stop(int fd);
+asmlinkage long sys_pfm_start(int fd, struct pfarg_start __user *st);
+asmlinkage long sys_pfm_unload_context(int fd);
+asmlinkage long sys_pfm_restart(int fd);
+asmlinkage long sys_pfm_load_context(int fd, struct pfarg_load __user *ld);
+
+ssize_t pfm_sysfs_res_show(char *buf, size_t sz, int what);
+
+extern ssize_t __pfm_read(struct pfm_context *ctx,
+			  union pfarg_msg *msg_buf,
+			  int non_block);
+/*
+ * function providing some help for backward compatiblity with old IA-64
+ * applications. In the old model, certain attributes of a counter were
+ * passed via the PMC, now they are passed via the PMD.
+ */
+static int pfm_compat_update_pmd(struct pfm_context *ctx, u16 set_id, u16 cnum,
+				 u32 rflags,
+				 unsigned long *smpl_pmds,
+				 unsigned long *reset_pmds,
+				 u64 eventid)
+{
+	struct pfm_event_set *set;
+	int is_counting;
+	unsigned long *impl_pmds;
+	u32 flags = 0;
+	u16 max_pmd;
+
+	impl_pmds = ctx->regs.pmds;
+	max_pmd	= ctx->regs.max_pmd;
+
+	/*
+	 * given that we do not maintain PMC ->PMD dependencies
+	 * we cannot figure out what to do in case PMCxx != PMDxx
+	 */
+	if (cnum > max_pmd)
+		return 0;
+
+	/*
+	 * assumes PMCxx controls PMDxx which is always true for counters
+	 * on Itanium PMUs.
+	 */
+	is_counting = pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_C64;
+	set = pfm_find_set(ctx, set_id, 0);
+
+	/*
+	 * for v2.0, we only allowed counting PMD to generate
+	 * user-level notifications. Same thing with randomization.
+	 */
+	if (is_counting) {
+		if (rflags & PFM_REGFL_OVFL_NOTIFY)
+			flags |= PFM_REGFL_OVFL_NOTIFY;
+		if (rflags & PFM_REGFL_RANDOM)
+			flags |= PFM_REGFL_RANDOM;
+		/*
+		 * verify validity of smpl_pmds
+		 */
+		if (unlikely(bitmap_subset(smpl_pmds,
+					   impl_pmds, max_pmd) == 0)) {
+			PFM_DBG("invalid smpl_pmds=0x%llx for pmd%u",
+				  (unsigned long long)smpl_pmds[0], cnum);
+			return -EINVAL;
+		}
+		/*
+		 * verify validity of reset_pmds
+		 */
+		if (unlikely(bitmap_subset(reset_pmds,
+					   impl_pmds, max_pmd) == 0)) {
+			PFM_DBG("invalid reset_pmds=0x%lx for pmd%u",
+				  reset_pmds[0], cnum);
+			return -EINVAL;
+		}
+		/*
+		 * ensures that a PFM_READ_PMDS succeeds with a
+		 * corresponding PFM_WRITE_PMDS
+		 */
+		__set_bit(cnum, set->used_pmds);
+
+	} else if (rflags & (PFM_REGFL_OVFL_NOTIFY|PFM_REGFL_RANDOM)) {
+		PFM_DBG("cannot set ovfl_notify or random on pmd%u", cnum);
+		return -EINVAL;
+	}
+
+	set->pmds[cnum].flags = flags;
+
+	if (is_counting) {
+		bitmap_copy(set->pmds[cnum].reset_pmds,
+			    reset_pmds,
+			    max_pmd);
+
+		bitmap_copy(set->pmds[cnum].smpl_pmds,
+			    smpl_pmds,
+			    max_pmd);
+
+		set->pmds[cnum].eventid = eventid;
+
+		/*
+		 * update ovfl_notify
+		 */
+		if (rflags & PFM_REGFL_OVFL_NOTIFY)
+			__set_bit(cnum, set->ovfl_notify);
+		else
+			__clear_bit(cnum, set->ovfl_notify);
+
+	}
+	PFM_DBG("pmd%u flags=0x%x eventid=0x%lx r_pmds=0x%lx s_pmds=0x%lx",
+		  cnum, flags,
+		  eventid,
+		  reset_pmds[0],
+		  smpl_pmds[0]);
+
+	return 0;
+}
+
+
+int __pfm_write_ibrs_old(struct pfm_context *ctx, void *arg, int count)
+{
+	struct pfarg_dbreg *req = arg;
+	struct pfarg_pmc pmc;
+	int i, ret = 0;
+
+	memset(&pmc, 0, sizeof(pmc));
+
+	for (i = 0; i < count; i++, req++) {
+		pmc.reg_num   = 256+req->dbreg_num;
+		pmc.reg_value = req->dbreg_value;
+		pmc.reg_flags = 0;
+		pmc.reg_set   = req->dbreg_set;
+
+		ret = __pfm_write_pmcs(ctx, &pmc, 1);
+
+		req->dbreg_flags &= ~PFM_REG_RETFL_MASK;
+		req->dbreg_flags |= pmc.reg_flags;
+
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
+static long pfm_write_ibrs_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct file *filp;
+	struct pfarg_dbreg *req = NULL;
+	void *fptr, *resume;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
+		return -EINVAL;
+
+	sz = count*sizeof(*req);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags, &resume);
+	if (ret == 0)
+		ret = __pfm_write_ibrs_old(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+int __pfm_write_dbrs_old(struct pfm_context *ctx, void *arg, int count)
+{
+	struct pfarg_dbreg *req = arg;
+	struct pfarg_pmc pmc;
+	int i, ret = 0;
+
+	memset(&pmc, 0, sizeof(pmc));
+
+	for (i = 0; i < count; i++, req++) {
+		pmc.reg_num   = 264+req->dbreg_num;
+		pmc.reg_value = req->dbreg_value;
+		pmc.reg_flags = 0;
+		pmc.reg_set   = req->dbreg_set;
+
+		ret = __pfm_write_pmcs(ctx, &pmc, 1);
+
+		req->dbreg_flags &= ~PFM_REG_RETFL_MASK;
+		req->dbreg_flags |= pmc.reg_flags;
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
+static long pfm_write_dbrs_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct file *filp;
+	struct pfarg_dbreg *req = NULL;
+	void *fptr, *resume;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
+		return -EINVAL;
+
+	sz = count*sizeof(*req);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags, &resume);
+	if (ret == 0)
+		ret = __pfm_write_dbrs_old(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+int __pfm_write_pmcs_old(struct pfm_context *ctx, struct pfarg_reg *req_old,
+			 int count)
+{
+	struct pfarg_pmc req;
+	unsigned int i;
+	int ret, error_code;
+
+	memset(&req, 0, sizeof(req));
+
+	for (i = 0; i < count; i++, req_old++) {
+		req.reg_num   = req_old->reg_num;
+		req.reg_set   = req_old->reg_set;
+		req.reg_flags = 0;
+		req.reg_value = req_old->reg_value;
+
+		ret = __pfm_write_pmcs(ctx, (void *)&req, 1);
+		req_old->reg_flags &= ~PFM_REG_RETFL_MASK;
+		req_old->reg_flags |= req.reg_flags;
+
+		if (ret)
+			return ret;
+
+		ret = pfm_compat_update_pmd(ctx, req_old->reg_set,
+				      req_old->reg_num,
+				      (u32)req_old->reg_flags,
+				      req_old->reg_smpl_pmds,
+				      req_old->reg_reset_pmds,
+				      req_old->reg_smpl_eventid);
+
+		error_code = ret ? PFM_REG_RETFL_EINVAL : 0;
+		req_old->reg_flags &= ~PFM_REG_RETFL_MASK;
+		req_old->reg_flags |= error_code;
+
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
+static long pfm_write_pmcs_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct file *filp;
+	struct pfarg_reg *req = NULL;
+	void *fptr, *resume;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
+		return -EINVAL;
+
+	sz = count*sizeof(*req);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags, &resume);
+	if (ret == 0)
+		ret = __pfm_write_pmcs_old(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+int __pfm_write_pmds_old(struct pfm_context *ctx, struct pfarg_reg *req_old,
+			 int count)
+{
+	struct pfarg_pmd req;
+	int i, ret;
+
+	memset(&req, 0, sizeof(req));
+
+	for (i = 0; i < count; i++, req_old++) {
+		req.reg_num   = req_old->reg_num;
+		req.reg_set   = req_old->reg_set;
+		req.reg_value = req_old->reg_value;
+		/* flags passed with pmcs in v2.0 */
+
+		req.reg_long_reset  = req_old->reg_long_reset;
+		req.reg_short_reset = req_old->reg_short_reset;
+		req.reg_random_mask = req_old->reg_random_mask;
+		/*
+		 * reg_random_seed is ignored since v2.3
+		 */
+
+		/*
+		 * skip last_reset_val not used for writing
+		 * skip smpl_pmds, reset_pmds, eventid, ovfl_swtch_cnt
+		 * as set in pfm_write_pmcs_old.
+		 *
+		 * ovfl_switch_cnt ignored, not implemented in v2.0
+		 */
+		ret = __pfm_write_pmds(ctx, (void *)&req, 1, 1);
+
+		req_old->reg_flags &= ~PFM_REG_RETFL_MASK;
+		req_old->reg_flags |= req.reg_flags;
+
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
+static long pfm_write_pmds_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct file *filp;
+	struct pfarg_reg *req = NULL;
+	void *fptr, *resume;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
+		return -EINVAL;
+
+	sz = count*sizeof(*req);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags, &resume);
+	if (ret == 0)
+		ret = __pfm_write_pmds_old(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	kfree(fptr);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+int __pfm_read_pmds_old(struct pfm_context *ctx, struct pfarg_reg *req_old,
+			int count)
+{
+	struct pfarg_pmd req;
+	int i, ret;
+
+	memset(&req, 0, sizeof(req));
+
+	for (i = 0; i < count; i++, req_old++) {
+		req.reg_num   = req_old->reg_num;
+		req.reg_set   = req_old->reg_set;
+
+		/* skip value not used for reading */
+		req.reg_flags = req_old->reg_flags;
+
+		/* skip short/long_reset not used for reading */
+		/* skip last_reset_val not used for reading */
+		/* skip ovfl_switch_cnt not used for reading */
+
+		ret = __pfm_read_pmds(ctx, (void *)&req, 1);
+
+		req_old->reg_flags &= ~PFM_REG_RETFL_MASK;
+		req_old->reg_flags |= req.reg_flags;
+		if (ret)
+			return ret;
+
+		/* update fields */
+		req_old->reg_value = req.reg_value;
+
+		req_old->reg_last_reset_val  = req.reg_last_reset_val;
+		req_old->reg_ovfl_switch_cnt = req.reg_ovfl_switch_cnt;
+	}
+	return 0;
+}
+
+static long pfm_read_pmds_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct file *filp;
+	struct pfarg_reg *req = NULL;
+	void *fptr, *resume;
+	unsigned long flags;
+	size_t sz;
+	int ret, fput_needed;
+
+	if (count < 1 || count >= PFM_MAX_ARG_COUNT(req))
+		return -EINVAL;
+
+	sz = count*sizeof(*req);
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	ctx = filp->private_data;
+	ret = -EBADF;
+
+	if (unlikely(!ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		goto error;
+	}
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags, &resume);
+	if (ret == 0)
+		ret = __pfm_read_pmds_old(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+error:
+	fput_light(filp, fput_needed);
+	return ret;
+}
+
+/*
+ * OBSOLETE: use /proc/perfmon_map instead
+ */
+static long pfm_get_default_pmcs_old(int fd, void __user *ureq, int count)
+{
+	struct pfarg_reg *req = NULL;
+	void *fptr;
+	size_t sz;
+	int ret, i;
+	unsigned int cnum;
+
+	if (count < 1)
+		return -EINVAL;
+
+	/*
+	 * ensure the pfm_pmu_conf does not disappear while
+	 * we use it
+	 */
+	ret = pfm_pmu_conf_get(1);
+	if (ret)
+		return ret;
+
+	sz = count*sizeof(*ureq);
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+
+	for (i = 0; i < count; i++, req++) {
+		cnum   = req->reg_num;
+
+		if (i >= PFM_MAX_PMCS ||
+		    (pfm_pmu_conf->pmc_desc[cnum].type & PFM_REG_I) == 0) {
+			req->reg_flags = PFM_REG_RETFL_EINVAL;
+			break;
+		}
+		req->reg_value = pfm_pmu_conf->pmc_desc[cnum].dfl_val;
+		req->reg_flags = 0;
+
+		PFM_DBG("pmc[%u]=0x%lx", cnum, req->reg_value);
+	}
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+error:
+	pfm_pmu_conf_put();
+
+	return ret;
+}
+
+/*
+ * allocate a sampling buffer and remaps it into the user address space of
+ * the task. This is only in compatibility mode
+ *
+ * function called ONLY on current task
+ */
+int pfm_smpl_buf_alloc_compat(struct pfm_context *ctx, size_t rsize, int fd)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma = NULL;
+	struct pfm_arch_context *ctx_arch;
+	struct file *filp;
+	size_t size;
+	int ret;
+	extern struct vm_operations_struct pfm_buf_map_vm_ops;
+
+	ctx_arch = pfm_ctx_arch(ctx);
+
+	/*
+	 * allocate buffer + map desc
+	 */
+	ret = pfm_smpl_buf_alloc(ctx, rsize);
+	if (ret)
+		return ret;
+
+	size = ctx->smpl_size;
+
+
+	/* allocate vma */
+	vma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+	if (!vma) {
+		PFM_DBG("Cannot allocate vma");
+		goto error_kmem;
+	}
+	memset(vma, 0, sizeof(*vma));
+
+	/*
+	 * simulate effect of mmap()
+	 */
+	filp = fget(fd);
+
+	/*
+	 * partially initialize the vma for the sampling buffer
+	 */
+	vma->vm_mm	     = mm;
+	vma->vm_flags	     = VM_READ | VM_MAYREAD | VM_RESERVED;
+	vma->vm_page_prot    = PAGE_READONLY;
+	vma->vm_ops	     = &pfm_buf_map_vm_ops;
+	vma->vm_file	     = filp;
+	vma->vm_private_data = ctx;
+	vma->vm_pgoff        = 0;
+
+	/*
+	 * Let's do the difficult operations next.
+	 *
+	 * now we atomically find some area in the address space and
+	 * remap the buffer into it.
+	 */
+	down_write(&current->mm->mmap_sem);
+
+	/* find some free area in address space, must have mmap sem held */
+	vma->vm_start = get_unmapped_area(NULL, 0, size, 0,
+					  MAP_PRIVATE|MAP_ANONYMOUS);
+	if (vma->vm_start == 0) {
+		PFM_DBG("cannot find unmapped area of size %zu", size);
+		up_write(&current->mm->mmap_sem);
+		goto error;
+	}
+	vma->vm_end = vma->vm_start + size;
+
+	PFM_DBG("aligned_size=%zu mapped @0x%lx", size, vma->vm_start);
+	/*
+	 * now insert the vma in the vm list for the process, must be
+	 * done with mmap lock held
+	 */
+	insert_vm_struct(mm, vma);
+
+	mm->total_vm  += size >> PAGE_SHIFT;
+
+	up_write(&current->mm->mmap_sem);
+
+	/*
+	 * IMPORTANT: we do not issue the fput()
+	 * because we want to increase the ref count
+	 * on the descriptor to simulate what mmap()
+	 * would do
+	 */
+
+	/*
+	 * used to propagate vaddr to syscall stub
+	 */
+	ctx_arch->ctx_smpl_vaddr = (void *)vma->vm_start;
+
+	return 0;
+error:
+	fput(filp);
+	kmem_cache_free(vm_area_cachep, vma);
+error_kmem:
+	pfm_smpl_buf_space_release(ctx, ctx->smpl_size);
+	vfree(ctx->smpl_addr);
+	return -ENOMEM;
+}
+
+#define PFM_DEFAULT_SMPL_UUID { \
+		0x4d, 0x72, 0xbe, 0xc0, 0x06, 0x64, 0x41, 0x43, 0x82,\
+		0xb4, 0xd3, 0xfd, 0x27, 0x24, 0x3c, 0x97}
+
+#define OPROFILE_FMT_UUID { \
+	0x77, 0x7a, 0x6e, 0x61, 0x20, 0x65, 0x73, 0x69, 0x74, 0x6e, 0x72, 0x20, 0x61, 0x65, 0x0a, 0x6c }
+
+static pfm_uuid_t old_default_uuid = PFM_DEFAULT_SMPL_UUID;
+static pfm_uuid_t oprofile_uuid = OPROFILE_FMT_UUID;
+static pfm_uuid_t null_uuid;
+
+/*
+ * function invoked in case, pfm_context_create fails
+ * at the last operation, copy_to_user. It needs to
+ * undo memory allocations and free the file descriptor
+ */
+static void pfm_undo_create_context_fd(int fd, struct pfm_context *ctx)
+{
+	struct files_struct *files = current->files;
+	struct file *file;
+	int fput_needed;
+
+	file = fget_light(fd, &fput_needed);
+	/*
+	 * there is no fd_uninstall(), so we do it
+	 * here. put_unused_fd() does not remove the
+	 * effect of fd_install().
+	 */
+
+	spin_lock(&files->file_lock);
+	files->fd_array[fd] = NULL;
+	spin_unlock(&files->file_lock);
+
+	fput_light(file, fput_needed);
+
+	/*
+	 * decrement ref count and kill file
+	 */
+	put_filp(file);
+
+	put_unused_fd(fd);
+
+	pfm_free_context(ctx);
+}
+
+static int pfm_get_smpl_arg_old(pfm_uuid_t uuid, void __user *fmt_uarg,
+				size_t usize, void **arg,
+				struct pfm_smpl_fmt **fmt)
+{
+	struct pfm_smpl_fmt *f;
+	void *addr = NULL;
+	char *fmt_name;
+	size_t sz;
+	int ret;
+
+	if (!memcmp(uuid, null_uuid, sizeof(pfm_uuid_t)))
+		return 0;
+
+	if (!memcmp(uuid, old_default_uuid, sizeof(pfm_uuid_t))) {
+		fmt_name = "default-old";
+	} else if (!memcmp(uuid, oprofile_uuid, sizeof(pfm_uuid_t))) {
+		fmt_name = "OProfile";
+	} else {
+		PFM_DBG("compatibility mode supports only default and oprofile sampling formats");
+		return -EINVAL;
+	}
+
+	/*
+	 * find fmt and increase refcount
+	 */
+	f = pfm_smpl_fmt_get(fmt_name);
+	if (f == NULL) {
+		PFM_DBG("%s buffer format not found", fmt_name);
+		return -EINVAL;
+	}
+
+	/*
+	 * expected format argument size
+	 */
+	sz = f->fmt_arg_size;
+
+	/*
+	 * check user size matches expected size
+	 * usize = -1 is for IA-64 backward compatibility
+	 */
+	ret = -EINVAL;
+	if (sz != usize && usize != -1) {
+		PFM_DBG("invalid arg size %zu, format expects %zu",
+			usize, sz);
+		goto error;
+	}
+
+	ret = -ENOMEM;
+	addr = kmalloc(sz, GFP_KERNEL);
+	if (addr == NULL)
+		goto error;
+
+	ret = -EFAULT;
+	if (copy_from_user(addr, fmt_uarg, sz))
+		goto error;
+
+	*arg = addr;
+	*fmt = f;
+	return 0;
+
+error:
+	kfree(addr);
+	pfm_smpl_fmt_put(f);
+	return ret;
+}
+
+static long pfm_create_context_old(int fd, void __user *ureq, int count)
+{
+	struct pfm_context *new_ctx;
+	struct pfm_arch_context *ctx_arch;
+	struct pfm_smpl_fmt *fmt = NULL;
+	struct pfarg_context req_old;
+	void __user *usmpl_arg;
+	void *smpl_arg = NULL;
+	struct pfarg_ctx req;
+	int ret;
+
+	if (count != 1)
+		return -EINVAL;
+
+	if (copy_from_user(&req_old, ureq, sizeof(req_old)))
+		return -EFAULT;
+
+	memset(&req, 0, sizeof(req));
+
+	/*
+	 * sampling format args are following pfarg_context
+	 */
+	usmpl_arg = ureq+sizeof(req_old);
+
+	ret = pfm_get_smpl_arg_old(req_old.ctx_smpl_buf_id, usmpl_arg, -1,
+				   &smpl_arg, &fmt);
+	if (ret)
+		return ret;
+
+	req.ctx_flags = req_old.ctx_flags;
+
+	/*
+	 * returns file descriptor if >=0, or error code */
+	ret = __pfm_create_context(&req, fmt, smpl_arg, PFM_COMPAT, &new_ctx);
+	if (ret >= 0) {
+		ctx_arch = pfm_ctx_arch(new_ctx);
+		req_old.ctx_fd = ret;
+		req_old.ctx_smpl_vaddr = ctx_arch->ctx_smpl_vaddr;
+	}
+
+	if (copy_to_user(ureq, &req_old, sizeof(req_old))) {
+		pfm_undo_create_context_fd(req_old.ctx_fd, new_ctx);
+		ret = -EFAULT;
+	}
+
+	kfree(smpl_arg);
+
+	return ret;
+}
+
+/*
+ * obsolete call: use /proc/perfmon
+ */
+static long pfm_get_features_old(int fd, void __user *arg, int count)
+{
+	struct pfarg_features req;
+	int ret = 0;
+
+	if (count != 1)
+		return -EINVAL;
+
+	memset(&req, 0, sizeof(req));
+
+	req.ft_version = PFM_VERSION;
+
+	if (copy_to_user(arg, &req, sizeof(req)))
+		ret = -EFAULT;
+
+	return ret;
+}
+
+static long pfm_debug_old(int fd, void __user *arg, int count)
+{
+	int m;
+
+	if (count != 1)
+		return -EINVAL;
+
+	if (get_user(m, (int __user *)arg))
+		return -EFAULT;
+
+
+	pfm_controls.debug = m == 0 ? 0 : 1;
+
+	PFM_INFO("debugging %s (timing reset)",
+		 pfm_controls.debug ? "on" : "off");
+
+	if (m == 0)
+		for_each_online_cpu(m) {
+			memset(&per_cpu(pfm_stats, m), 0,
+			       sizeof(struct pfm_stats));
+		}
+	return 0;
+}
+
+static long pfm_unload_context_old(int fd, void __user *arg, int count)
+{
+	if (count)
+		return -EINVAL;
+
+	return sys_pfm_unload_context(fd);
+}
+
+static long pfm_restart_old(int fd, void __user *arg, int count)
+{
+	if (count)
+		return -EINVAL;
+
+	return sys_pfm_restart(fd);
+}
+
+static long pfm_stop_old(int fd, void __user *arg, int count)
+{
+	if (count)
+		return -EINVAL;
+
+	return sys_pfm_stop(fd);
+}
+
+static long pfm_start_old(int fd, void __user *arg, int count)
+{
+	if (count > 1)
+		return -EINVAL;
+
+	return sys_pfm_start(fd, arg);
+}
+
+static long pfm_load_context_old(int fd, void __user *ureq, int count)
+{
+	if (count != 1)
+		return -EINVAL;
+
+	return sys_pfm_load_context(fd, ureq);
+}
+
+/*
+ * perfmon command descriptions
+ */
+struct pfm_cmd_desc {
+	long (*cmd_func)(int fd, void __user *arg, int count);
+};
+
+/*
+ * functions MUST be listed in the increasing order of
+ * their index (see permfon.h)
+ */
+#define PFM_CMD(name)  \
+	{ .cmd_func = name,  \
+	}
+#define PFM_CMD_NONE		\
+	{ .cmd_func = NULL   \
+	}
+
+static struct pfm_cmd_desc pfm_cmd_tab[] = {
+/* 0  */PFM_CMD_NONE,
+/* 1  */PFM_CMD(pfm_write_pmcs_old),
+/* 2  */PFM_CMD(pfm_write_pmds_old),
+/* 3  */PFM_CMD(pfm_read_pmds_old),
+/* 4  */PFM_CMD(pfm_stop_old),
+/* 5  */PFM_CMD(pfm_start_old),
+/* 6  */PFM_CMD_NONE,
+/* 7  */PFM_CMD_NONE,
+/* 8  */PFM_CMD(pfm_create_context_old),
+/* 9  */PFM_CMD_NONE,
+/* 10 */PFM_CMD(pfm_restart_old),
+/* 11 */PFM_CMD_NONE,
+/* 12 */PFM_CMD(pfm_get_features_old),
+/* 13 */PFM_CMD(pfm_debug_old),
+/* 14 */PFM_CMD_NONE,
+/* 15 */PFM_CMD(pfm_get_default_pmcs_old),
+/* 16 */PFM_CMD(pfm_load_context_old),
+/* 17 */PFM_CMD(pfm_unload_context_old),
+/* 18 */PFM_CMD_NONE,
+/* 19 */PFM_CMD_NONE,
+/* 20 */PFM_CMD_NONE,
+/* 21 */PFM_CMD_NONE,
+/* 22 */PFM_CMD_NONE,
+/* 23 */PFM_CMD_NONE,
+/* 24 */PFM_CMD_NONE,
+/* 25 */PFM_CMD_NONE,
+/* 26 */PFM_CMD_NONE,
+/* 27 */PFM_CMD_NONE,
+/* 28 */PFM_CMD_NONE,
+/* 29 */PFM_CMD_NONE,
+/* 30 */PFM_CMD_NONE,
+/* 31 */PFM_CMD_NONE,
+/* 32 */PFM_CMD(pfm_write_ibrs_old),
+/* 33 */PFM_CMD(pfm_write_dbrs_old),
+};
+#define PFM_CMD_COUNT ARRAY_SIZE(pfm_cmd_tab)
+
+/*
+ * system-call entry point (must return long)
+ */
+asmlinkage long sys_perfmonctl(int fd, int cmd, void __user *arg, int count)
+{
+	if (perfmon_disabled)
+		return -ENOSYS;
+
+	if (unlikely(cmd < 0 || cmd >= PFM_CMD_COUNT
+		     || pfm_cmd_tab[cmd].cmd_func == NULL)) {
+		PFM_DBG("invalid cmd=%d", cmd);
+		return -EINVAL;
+	}
+	return (long)pfm_cmd_tab[cmd].cmd_func(fd, arg, count);
+}
+
+/*
+ * Called from pfm_read() for a perfmon v2.0 context.
+ *
+ * compatibility mode pfm_read() routine. We need a separate
+ * routine because the definition of the message has changed.
+ * The pfm_msg and pfarg_msg structures are different.
+ *
+ * return: sizeof(pfm_msg_t) on success, -errno otherwise
+ */
+ssize_t pfm_arch_compat_read(struct pfm_context *ctx,
+			     char __user *buf,
+			     int non_block,
+			     size_t size)
+{
+	union pfarg_msg msg_buf;
+	pfm_msg_t old_msg_buf;
+	pfm_ovfl_msg_t *o_msg;
+	struct pfarg_ovfl_msg *n_msg;
+	int ret;
+
+	PFM_DBG("msg=%p size=%zu", buf, size);
+
+	/*
+	 * cannot extract partial messages.
+	 * check even when there is no message
+	 *
+	 * cannot extract more than one message per call. Bytes
+	 * above sizeof(msg) are ignored.
+	 */
+	if (size < sizeof(old_msg_buf)) {
+		PFM_DBG("message is too small size=%zu must be >=%zu)",
+			size,
+			sizeof(old_msg_buf));
+		return -EINVAL;
+	}
+
+	ret =  __pfm_read(ctx, &msg_buf, non_block);
+	if (ret < 1)
+		return ret;
+
+	/*
+	 * force return value to old message size
+	 */
+	ret = sizeof(old_msg_buf);
+
+	o_msg = &old_msg_buf.pfm_ovfl_msg;
+	n_msg = &msg_buf.pfm_ovfl_msg;
+
+	switch (msg_buf.type) {
+	case PFM_MSG_OVFL:
+		o_msg->msg_type   = PFM_MSG_OVFL;
+		o_msg->msg_ctx_fd = 0;
+		o_msg->msg_active_set = n_msg->msg_active_set;
+		o_msg->msg_tstamp = 0;
+
+		o_msg->msg_ovfl_pmds[0] = n_msg->msg_ovfl_pmds[0];
+		o_msg->msg_ovfl_pmds[1] = n_msg->msg_ovfl_pmds[1];
+		o_msg->msg_ovfl_pmds[2] = n_msg->msg_ovfl_pmds[2];
+		o_msg->msg_ovfl_pmds[3] = n_msg->msg_ovfl_pmds[3];
+		break;
+	case PFM_MSG_END:
+		o_msg->msg_type = PFM_MSG_END;
+		o_msg->msg_ctx_fd = 0;
+		o_msg->msg_tstamp = 0;
+		break;
+	default:
+		PFM_DBG("unknown msg type=%d", msg_buf.type);
+	}
+	if (copy_to_user(buf, &old_msg_buf, sizeof(old_msg_buf)))
+		ret = -EFAULT;
+	PFM_DBG_ovfl("ret=%d", ret);
+	return ret;
+}
+
+/*
+ * legacy /proc/perfmon simplified interface (we only maintain the
+ * global information (no more per-cpu stats, use
+ * /sys/devices/system/cpu/cpuXX/perfmon
+ */
+static struct proc_dir_entry 	*perfmon_proc;
+
+static void *pfm_proc_start(struct seq_file *m, loff_t *pos)
+{
+	if (*pos == 0)
+		return (void *)1;
+
+	return NULL;
+}
+
+static void *pfm_proc_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	++*pos;
+	return pfm_proc_start(m, pos);
+}
+
+static void pfm_proc_stop(struct seq_file *m, void *v)
+{
+}
+
+/*
+ * this is a simplified version of the legacy /proc/perfmon.
+ * We have retained ONLY the key information that tools are actually
+ * using
+ */
+static void pfm_proc_show_header(struct seq_file *m)
+{
+	char buf[128];
+
+	pfm_sysfs_res_show(buf, sizeof(buf), 3);
+
+	seq_printf(m, "perfmon version            : %u.%u\n",
+		PFM_VERSION_MAJ, PFM_VERSION_MIN);
+
+	seq_printf(m, "model                      : %s", buf);
+}
+
+static int pfm_proc_show(struct seq_file *m, void *v)
+{
+	pfm_proc_show_header(m);
+	return 0;
+}
+
+struct seq_operations pfm_proc_seq_ops = {
+	.start = pfm_proc_start,
+	.next =	pfm_proc_next,
+	.stop =	pfm_proc_stop,
+	.show =	pfm_proc_show
+};
+
+static int pfm_proc_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &pfm_proc_seq_ops);
+}
+
+
+static struct file_operations pfm_proc_fops = {
+	.open = pfm_proc_open,
+	.read = seq_read,
+	.llseek	= seq_lseek,
+	.release = seq_release,
+};
+
+/*
+ * called from pfm_arch_init(), global initialization, called once
+ */
+int __init pfm_ia64_compat_init(void)
+{
+	/*
+	 * create /proc/perfmon
+	 */
+	perfmon_proc = create_proc_entry("perfmon", S_IRUGO, NULL);
+	if (perfmon_proc == NULL) {
+		PFM_ERR("cannot create /proc entry, perfmon disabled");
+		return -1;
+	}
+	perfmon_proc->proc_fops = &pfm_proc_fops;
+	return 0;
+}
Index: linux-2.6.31-master/arch/ia64/perfmon/perfmon_default_smpl.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/arch/ia64/perfmon/perfmon_default_smpl.c
@@ -0,0 +1,273 @@
+/*
+ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file implements the old default sampling buffer format
+ * for the Linux/ia64 perfmon-2 subsystem. This is for backward
+ * compatibility only. use the new default format in perfmon/
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/smp.h>
+#include <linux/sysctl.h>
+
+#ifdef MODULE
+#define FMT_FLAGS	0
+#else
+#define FMT_FLAGS	PFM_FMTFL_IS_BUILTIN
+#endif
+
+#include <linux/perfmon_kern.h>
+#include <asm/perfmon_default_smpl.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("perfmon old default sampling format");
+MODULE_LICENSE("GPL");
+
+static int pfm_default_fmt_validate(u32 flags, u16 npmds, void *data)
+{
+	struct pfm_default_smpl_arg *arg = data;
+	size_t min_buf_size;
+
+	if (data == NULL) {
+		PFM_DBG("no argument passed");
+		return -EINVAL;
+	}
+
+	/*
+	 * compute min buf size. All PMD are manipulated as 64bit entities
+	 */
+	min_buf_size = sizeof(struct pfm_default_smpl_hdr)
+	     + (sizeof(struct pfm_default_smpl_entry) + (npmds*sizeof(u64)));
+
+	PFM_DBG("validate flags=0x%x npmds=%u min_buf_size=%lu "
+		  "buf_size=%lu CPU%d", flags, npmds, min_buf_size,
+		  arg->buf_size, smp_processor_id());
+
+	/*
+	 * must hold at least the buffer header + one minimally sized entry
+	 */
+	if (arg->buf_size < min_buf_size)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int pfm_default_fmt_get_size(unsigned int flags, void *data,
+				    size_t *size)
+{
+	struct pfm_default_smpl_arg *arg = data;
+
+	/*
+	 * size has been validated in default_validate
+	 */
+	*size = arg->buf_size;
+
+	return 0;
+}
+
+static int pfm_default_fmt_init(struct pfm_context *ctx, void *buf,
+				u32 flags, u16 npmds, void *data)
+{
+	struct pfm_default_smpl_hdr *hdr;
+	struct pfm_default_smpl_arg *arg = data;
+
+	hdr = buf;
+
+	hdr->hdr_version      = PFM_DEFAULT_SMPL_VERSION;
+	hdr->hdr_buf_size     = arg->buf_size;
+	hdr->hdr_cur_offs     = sizeof(*hdr);
+	hdr->hdr_overflows    = 0;
+	hdr->hdr_count        = 0;
+
+	PFM_DBG("buffer=%p buf_size=%lu hdr_size=%lu "
+		  "hdr_version=%u cur_offs=%lu",
+		  buf,
+		  hdr->hdr_buf_size,
+		  sizeof(*hdr),
+		  hdr->hdr_version,
+		  hdr->hdr_cur_offs);
+
+	return 0;
+}
+
+static int pfm_default_fmt_handler(struct pfm_context *ctx,
+				   unsigned long ip, u64 tstamp, void *data)
+{
+	struct pfm_default_smpl_hdr *hdr;
+	struct pfm_default_smpl_entry *ent;
+	void *cur, *last, *buf;
+	u64 *e;
+	size_t entry_size;
+	u16 npmds, i, ovfl_pmd;
+	struct pfm_ovfl_arg *arg;
+
+	hdr = ctx->smpl_addr;
+	arg = &ctx->ovfl_arg;
+
+	buf = hdr;
+	cur = buf+hdr->hdr_cur_offs;
+	last = buf+hdr->hdr_buf_size;
+	ovfl_pmd = arg->ovfl_pmd;
+
+	/*
+	 * precheck for sanity
+	 */
+	if ((last - cur) < PFM_DEFAULT_MAX_ENTRY_SIZE)
+		goto full;
+
+	npmds = arg->num_smpl_pmds;
+
+	ent = cur;
+
+	prefetch(arg->smpl_pmds_values);
+
+	entry_size = sizeof(*ent) + (npmds << 3);
+
+	/* position for first pmd */
+	e = (unsigned long *)(ent+1);
+
+	hdr->hdr_count++;
+
+	PFM_DBG_ovfl("count=%lu cur=%p last=%p free_bytes=%lu "
+		       "ovfl_pmd=%d npmds=%u",
+		       hdr->hdr_count,
+		       cur, last,
+		       last-cur,
+		       ovfl_pmd,
+		       npmds);
+
+	/*
+	 * current = task running at the time of the overflow.
+	 *
+	 * per-task mode:
+	 * 	- this is ususally the task being monitored.
+	 * 	  Under certain conditions, it might be a different task
+	 *
+	 * system-wide:
+	 * 	- this is not necessarily the task controlling the session
+	 */
+	ent->pid            = current->pid;
+	ent->ovfl_pmd  	    = ovfl_pmd;
+	ent->last_reset_val = arg->pmd_last_reset;
+
+	/*
+	 * where did the fault happen (includes slot number)
+	 */
+	ent->ip = ip;
+
+	ent->tstamp    = tstamp;
+	ent->cpu       = smp_processor_id();
+	ent->set       = arg->active_set;
+	ent->tgid      = current->tgid;
+
+	/*
+	 * selectively store PMDs in increasing index number
+	 */
+	if (npmds) {
+		u64 *val = arg->smpl_pmds_values;
+		for (i = 0; i < npmds; i++)
+			*e++ = *val++;
+	}
+
+	/*
+	 * update position for next entry
+	 */
+	hdr->hdr_cur_offs += entry_size;
+	cur               += entry_size;
+
+	/*
+	 * post check to avoid losing the last sample
+	 */
+	if ((last - cur) < PFM_DEFAULT_MAX_ENTRY_SIZE)
+		goto full;
+
+	/*
+	 * reset before returning from interrupt handler
+	 */
+	arg->ovfl_ctrl = PFM_OVFL_CTRL_RESET;
+	return 0;
+full:
+	PFM_DBG_ovfl("smpl buffer full free=%lu, count=%lu",
+		       last-cur, hdr->hdr_count);
+
+	/*
+	 * increment number of buffer overflow.
+	 * important to detect duplicate set of samples.
+	 */
+	hdr->hdr_overflows++;
+
+	/*
+	 * request notification and masking of monitoring.
+	 * Notification is still subject to the overflowed
+	 */
+	arg->ovfl_ctrl = PFM_OVFL_CTRL_NOTIFY | PFM_OVFL_CTRL_MASK;
+
+	return -ENOBUFS; /* we are full, sorry */
+}
+
+static int pfm_default_fmt_restart(struct pfm_context *ctx, u32 *ovfl_ctrl)
+{
+	struct pfm_default_smpl_hdr *hdr;
+
+	hdr = ctx->smpl_addr;
+
+	hdr->hdr_count    = 0;
+	hdr->hdr_cur_offs = sizeof(*hdr);
+
+	*ovfl_ctrl = PFM_OVFL_CTRL_RESET;
+
+	return 0;
+}
+
+static int pfm_default_fmt_exit(void *buf)
+{
+	return 0;
+}
+
+static struct pfm_smpl_fmt default_fmt = {
+	.fmt_name = "default-old",
+	.fmt_version = 0x10000,
+	.fmt_arg_size = sizeof(struct pfm_default_smpl_arg),
+	.fmt_validate = pfm_default_fmt_validate,
+	.fmt_getsize = pfm_default_fmt_get_size,
+	.fmt_init = pfm_default_fmt_init,
+	.fmt_handler = pfm_default_fmt_handler,
+	.fmt_restart = pfm_default_fmt_restart,
+	.fmt_exit = pfm_default_fmt_exit,
+	.fmt_flags = FMT_FLAGS,
+	.owner = THIS_MODULE
+};
+
+static int pfm_default_fmt_init_module(void)
+{
+	int ret;
+
+	return pfm_fmt_register(&default_fmt);
+	return ret;
+}
+
+static void pfm_default_fmt_cleanup_module(void)
+{
+	pfm_fmt_unregister(&default_fmt);
+}
+
+module_init(pfm_default_fmt_init_module);
+module_exit(pfm_default_fmt_cleanup_module);
Index: linux-2.6.31-master/arch/ia64/perfmon/perfmon_generic.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/arch/ia64/perfmon/perfmon_generic.c
@@ -0,0 +1,148 @@
+/*
+ * This file contains the generic PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
+ * contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/perfmon_kern.h>
+#include <asm/pal.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("Generic IA-64 PMU description tables");
+MODULE_LICENSE("GPL");
+
+#define RDEP(x)	(1UL << (x))
+
+#define PFM_IA64GEN_MASK_PMCS	(RDEP(4)|RDEP(5)|RDEP(6)|RDEP(7))
+#define PFM_IA64GEN_RSVD	(0xffffffffffff0080UL)
+#define PFM_IA64GEN_NO64	(1UL<<5)
+
+/* forward declaration */
+static struct pfm_pmu_config pfm_ia64gen_pmu_conf;
+
+static struct pfm_arch_pmu_info pfm_ia64gen_pmu_info = {
+	.mask_pmcs = {PFM_IA64GEN_MASK_PMCS,},
+};
+
+static struct pfm_regmap_desc pfm_ia64gen_pmc_desc[] = {
+/* pmc0  */ PMX_NA,
+/* pmc1  */ PMX_NA,
+/* pmc2  */ PMX_NA,
+/* pmc3  */ PMX_NA,
+/* pmc4  */ PMC_D(PFM_REG_W64, "PMC4", 0x0, PFM_IA64GEN_RSVD, PFM_IA64GEN_NO64, 4),
+/* pmc5  */ PMC_D(PFM_REG_W64, "PMC5", 0x0, PFM_IA64GEN_RSVD, PFM_IA64GEN_NO64, 5),
+/* pmc6  */ PMC_D(PFM_REG_W64, "PMC6", 0x0, PFM_IA64GEN_RSVD, PFM_IA64GEN_NO64, 6),
+/* pmc7  */ PMC_D(PFM_REG_W64, "PMC7", 0x0, PFM_IA64GEN_RSVD, PFM_IA64GEN_NO64, 7)
+};
+#define PFM_IA64GEN_NUM_PMCS ARRAY_SIZE(pfm_ia64gen_pmc_desc)
+
+static struct pfm_regmap_desc pfm_ia64gen_pmd_desc[] = {
+/* pmd0  */ PMX_NA,
+/* pmd1  */ PMX_NA,
+/* pmd2  */ PMX_NA,
+/* pmd3  */ PMX_NA,
+/* pmd4  */ PMD_DP(PFM_REG_C, "PMD4", 4, 1ull << 4),
+/* pmd5  */ PMD_DP(PFM_REG_C, "PMD5", 5, 1ull << 5),
+/* pmd6  */ PMD_DP(PFM_REG_C, "PMD6", 6, 1ull << 6),
+/* pmd7  */ PMD_DP(PFM_REG_C, "PMD7", 7, 1ull << 7)
+};
+#define PFM_IA64GEN_NUM_PMDS ARRAY_SIZE(pfm_ia64gen_pmd_desc)
+
+static int pfm_ia64gen_pmc_check(struct pfm_context *ctx,
+				 struct pfm_event_set *set,
+				 struct pfarg_pmc *req)
+{
+#define PFM_IA64GEN_PMC_PM_POS6	(1UL<<6)
+	u64 tmpval;
+	int is_system;
+
+	is_system = ctx->flags.system;
+	tmpval = req->reg_value;
+
+	switch (req->reg_num) {
+	case  4:
+	case  5:
+	case  6:
+	case  7:
+		/* set pmc.oi for 64-bit emulation */
+		tmpval |= 1UL << 5;
+
+		if (is_system)
+			tmpval |= PFM_IA64GEN_PMC_PM_POS6;
+		else
+			tmpval &= ~PFM_IA64GEN_PMC_PM_POS6;
+		break;
+
+	}
+	req->reg_value = tmpval;
+
+	return 0;
+}
+
+/*
+ * matches anything
+ */
+static int pfm_ia64gen_probe_pmu(void)
+{
+	u64 pm_buffer[16];
+	pal_perf_mon_info_u_t pm_info;
+
+	/*
+	 * call PAL_PERFMON_INFO to retrieve counter width which
+	 * is implementation specific
+	 */
+	if (ia64_pal_perf_mon_info(pm_buffer, &pm_info))
+		return -1;
+
+	pfm_ia64gen_pmu_conf.counter_width = pm_info.pal_perf_mon_info_s.width;
+
+	return 0;
+}
+
+/*
+ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+ */
+static struct pfm_pmu_config pfm_ia64gen_pmu_conf = {
+	.pmu_name = "Generic IA-64",
+	.counter_width = 0, /* computed from PAL_PERFMON_INFO */
+	.pmd_desc = pfm_ia64gen_pmd_desc,
+	.pmc_desc = pfm_ia64gen_pmc_desc,
+	.probe_pmu = pfm_ia64gen_probe_pmu,
+	.num_pmc_entries = PFM_IA64GEN_NUM_PMCS,
+	.num_pmd_entries = PFM_IA64GEN_NUM_PMDS,
+	.pmc_write_check = pfm_ia64gen_pmc_check,
+	.version = "1.0",
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+	.pmu_info = &pfm_ia64gen_pmu_info
+	/* no read/write checkers */
+};
+
+static int __init pfm_gen_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_ia64gen_pmu_conf);
+}
+
+static void __exit pfm_gen_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_ia64gen_pmu_conf);
+}
+
+module_init(pfm_gen_pmu_init_module);
+module_exit(pfm_gen_pmu_cleanup_module);
Index: linux-2.6.31-master/arch/ia64/perfmon/perfmon_itanium.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/arch/ia64/perfmon/perfmon_itanium.c
@@ -0,0 +1,232 @@
+/*
+ * This file contains the Itanium PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/perfmon_kern.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("Itanium (Merced) PMU description tables");
+MODULE_LICENSE("GPL");
+
+#define RDEP(x)	(1ULL << (x))
+
+#define PFM_ITA_MASK_PMCS (RDEP(4)|RDEP(5)|RDEP(6)|RDEP(7)|RDEP(10)|RDEP(11)|\
+			   RDEP(12))
+
+#define PFM_ITA_NO64	(1ULL<<5)
+
+static struct pfm_arch_pmu_info pfm_ita_pmu_info = {
+	.mask_pmcs = {PFM_ITA_MASK_PMCS,},
+};
+/* reserved bits are 1 in the mask */
+#define PFM_ITA_RSVD 0xfffffffffc8000a0UL
+/*
+ * For debug registers, writing xBR(y) means we use also xBR(y+1). Hence using
+ * PMC256+y means we use PMC256+y+1.  Yet, we do not have dependency information
+ * but this is fine because they are handled separately in the IA-64 specific
+ * code.
+ */
+static struct pfm_regmap_desc pfm_ita_pmc_desc[] = {
+/* pmc0  */ PMX_NA,
+/* pmc1  */ PMX_NA,
+/* pmc2  */ PMX_NA,
+/* pmc3  */ PMX_NA,
+/* pmc4  */ PMC_D(PFM_REG_W64, "PMC4" , 0x20, PFM_ITA_RSVD, PFM_ITA_NO64, 4),
+/* pmc5  */ PMC_D(PFM_REG_W64, "PMC5" , 0x20, PFM_ITA_RSVD, PFM_ITA_NO64, 5),
+/* pmc6  */ PMC_D(PFM_REG_W64, "PMC6" , 0x20, PFM_ITA_RSVD, PFM_ITA_NO64, 6),
+/* pmc7  */ PMC_D(PFM_REG_W64, "PMC7" , 0x20, PFM_ITA_RSVD, PFM_ITA_NO64, 7),
+/* pmc8  */ PMC_D(PFM_REG_W  , "PMC8" , 0xfffffffe3ffffff8UL, 0xfff00000001c0000UL, 0, 8),
+/* pmc9  */ PMC_D(PFM_REG_W  , "PMC9" , 0xfffffffe3ffffff8UL, 0xfff00000001c0000UL, 0, 9),
+/* pmc10 */ PMC_D(PFM_REG_W  , "PMC10", 0x0, 0xfffffffff3f0ff30UL, 0, 10),
+/* pmc11 */ PMC_D(PFM_REG_W  , "PMC11", 0x10000000UL, 0xffffffffecf0ff30UL, 0, 11),
+/* pmc12 */ PMC_D(PFM_REG_W  , "PMC12", 0x0, 0xffffffffffff0030UL, 0, 12),
+/* pmc13 */ PMC_D(PFM_REG_W  , "PMC13", 0x3ffff00000001UL, 0xfffffffffffffffeUL, 0, 13),
+/* pmc14 */ PMX_NA,
+/* pmc15 */ PMX_NA,
+/* pmc16 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc24 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc32 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc40 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc48 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc56 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc64 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc72 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc80 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc88 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc96 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc104 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc112 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc120 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc128 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc136 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc144 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc152 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc160 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc168 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc176 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc184 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc192 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc200 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc208 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc216 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc224 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc232 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc240 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc248 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc256 */ PMC_D(PFM_REG_W  , "IBR0", 0x0, 0, 0, 0),
+/* pmc257 */ PMC_D(PFM_REG_W  , "IBR1", 0x0, 0x8000000000000000UL, 0, 1),
+/* pmc258 */ PMC_D(PFM_REG_W  , "IBR2", 0x0, 0, 0, 2),
+/* pmc259 */ PMC_D(PFM_REG_W  , "IBR3", 0x0, 0x8000000000000000UL, 0, 3),
+/* pmc260 */ PMC_D(PFM_REG_W  , "IBR4", 0x0, 0, 0, 4),
+/* pmc261 */ PMC_D(PFM_REG_W  , "IBR5", 0x0, 0x8000000000000000UL, 0, 5),
+/* pmc262 */ PMC_D(PFM_REG_W  , "IBR6", 0x0, 0, 0, 6),
+/* pmc263 */ PMC_D(PFM_REG_W  , "IBR7", 0x0, 0x8000000000000000UL, 0, 7),
+/* pmc264 */ PMC_D(PFM_REG_W  , "DBR0", 0x0, 0, 0, 0),
+/* pmc265 */ PMC_D(PFM_REG_W  , "DBR1", 0x0, 0xc000000000000000UL, 0, 1),
+/* pmc266 */ PMC_D(PFM_REG_W  , "DBR2", 0x0, 0, 0, 2),
+/* pmc267 */ PMC_D(PFM_REG_W  , "DBR3", 0x0, 0xc000000000000000UL, 0, 3),
+/* pmc268 */ PMC_D(PFM_REG_W  , "DBR4", 0x0, 0, 0, 4),
+/* pmc269 */ PMC_D(PFM_REG_W  , "DBR5", 0x0, 0xc000000000000000UL, 0, 5),
+/* pmc270 */ PMC_D(PFM_REG_W  , "DBR6", 0x0, 0, 0, 6),
+/* pmc271 */ PMC_D(PFM_REG_W  , "DBR7", 0x0, 0xc000000000000000UL, 0, 7)
+};
+#define PFM_ITA_NUM_PMCS ARRAY_SIZE(pfm_ita_pmc_desc)
+
+static struct pfm_regmap_desc pfm_ita_pmd_desc[] = {
+/* pmd0  */ PMD_DP(PFM_REG_I , "PMD0", 0, 1ull << 10),
+/* pmd1  */ PMD_DP(PFM_REG_I , "PMD1", 1, 1ull << 10),
+/* pmd2  */ PMD_DP(PFM_REG_I , "PMD2", 2, 1ull << 11),
+/* pmd3  */ PMD_DP(PFM_REG_I , "PMD3", 3, 1ull << 11),
+/* pmd4  */ PMD_DP(PFM_REG_C , "PMD4", 4, 1ull << 4),
+/* pmd5  */ PMD_DP(PFM_REG_C , "PMD5", 5, 1ull << 5),
+/* pmd6  */ PMD_DP(PFM_REG_C , "PMD6", 6, 1ull << 6),
+/* pmd7  */ PMD_DP(PFM_REG_C , "PMD7", 7, 1ull << 7),
+/* pmd8  */ PMD_DP(PFM_REG_I , "PMD8", 8, 1ull << 12),
+/* pmd9  */ PMD_DP(PFM_REG_I , "PMD9", 9, 1ull << 12),
+/* pmd10 */ PMD_DP(PFM_REG_I , "PMD10", 10, 1ull << 12),
+/* pmd11 */ PMD_DP(PFM_REG_I , "PMD11", 11, 1ull << 12),
+/* pmd12 */ PMD_DP(PFM_REG_I , "PMD12", 12, 1ull << 12),
+/* pmd13 */ PMD_DP(PFM_REG_I , "PMD13", 13, 1ull << 12),
+/* pmd14 */ PMD_DP(PFM_REG_I , "PMD14", 14, 1ull << 12),
+/* pmd15 */ PMD_DP(PFM_REG_I , "PMD15", 15, 1ull << 12),
+/* pmd16 */ PMD_DP(PFM_REG_I , "PMD16", 16, 1ull << 12),
+/* pmd17 */ PMD_DP(PFM_REG_I , "PMD17", 17, 1ull << 11)
+};
+#define PFM_ITA_NUM_PMDS ARRAY_SIZE(pfm_ita_pmd_desc)
+
+static int pfm_ita_pmc_check(struct pfm_context *ctx,
+			     struct pfm_event_set *set,
+			     struct pfarg_pmc *req)
+{
+#define PFM_ITA_PMC_PM_POS6	(1UL<<6)
+	struct pfm_arch_context *ctx_arch;
+	u64 tmpval;
+	u16 cnum;
+	int ret = 0, is_system;
+
+	tmpval = req->reg_value;
+	cnum   = req->reg_num;
+	ctx_arch = pfm_ctx_arch(ctx);
+	is_system = ctx->flags.system;
+
+	switch (cnum) {
+	case  4:
+	case  5:
+	case  6:
+	case  7:
+	case 10:
+	case 11:
+	case 12:
+		if (is_system)
+			tmpval |= PFM_ITA_PMC_PM_POS6;
+		else
+			tmpval &= ~PFM_ITA_PMC_PM_POS6;
+		break;
+	}
+
+	/*
+	 * we must clear the (instruction) debug registers if pmc13.ta bit is
+	 * cleared before they are written (fl_using_dbreg==0) to avoid
+	 * picking up stale information.
+	 */
+	if (cnum == 13 && ((tmpval & 0x1) == 0)
+		&& ctx_arch->flags.use_dbr == 0) {
+		PFM_DBG("pmc13 has pmc13.ta cleared, clearing ibr");
+		ret = pfm_ia64_mark_dbregs_used(ctx, set);
+		if (ret)
+			return ret;
+	}
+
+	/*
+	 * we must clear the (data) debug registers if pmc11.pt bit is cleared
+	 * before they are written (fl_using_dbreg==0) to avoid picking up
+	 * stale information.
+	 */
+	if (cnum == 11 && ((tmpval >> 28) & 0x1) == 0
+		&& ctx_arch->flags.use_dbr == 0) {
+		PFM_DBG("pmc11 has pmc11.pt cleared, clearing dbr");
+		ret = pfm_ia64_mark_dbregs_used(ctx, set);
+		if (ret)
+			return ret;
+	}
+
+	req->reg_value = tmpval;
+
+	return 0;
+}
+
+static int pfm_ita_probe_pmu(void)
+{
+	return local_cpu_data->family == 0x7 && !ia64_platform_is("hpsim")
+		? 0 : -1;
+}
+
+/*
+ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+ */
+static struct pfm_pmu_config pfm_ita_pmu_conf = {
+	.pmu_name = "Itanium",
+	.counter_width = 32,
+	.pmd_desc = pfm_ita_pmd_desc,
+	.pmc_desc = pfm_ita_pmc_desc,
+	.pmc_write_check = pfm_ita_pmc_check,
+	.num_pmc_entries = PFM_ITA_NUM_PMCS,
+	.num_pmd_entries = PFM_ITA_NUM_PMDS,
+	.probe_pmu = pfm_ita_probe_pmu,
+	.version = "1.0",
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+	.pmu_info = &pfm_ita_pmu_info
+};
+
+static int __init pfm_ita_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_ita_pmu_conf);
+}
+
+static void __exit pfm_ita_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_ita_pmu_conf);
+}
+
+module_init(pfm_ita_pmu_init_module);
+module_exit(pfm_ita_pmu_cleanup_module);
+
Index: linux-2.6.31-master/arch/ia64/perfmon/perfmon_mckinley.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/arch/ia64/perfmon/perfmon_mckinley.c
@@ -0,0 +1,290 @@
+/*
+ * This file contains the McKinley PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Copyright (c) 2002-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/perfmon_kern.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("Itanium 2 (McKinley) PMU description tables");
+MODULE_LICENSE("GPL");
+
+#define RDEP(x)	(1UL << (x))
+
+#define PFM_MCK_MASK_PMCS (RDEP(4)|RDEP(5)|RDEP(6)|RDEP(7)|RDEP(10)|RDEP(11)|\
+			   RDEP(12))
+
+#define PFM_MCK_NO64	(1UL<<5)
+
+static struct pfm_arch_pmu_info pfm_mck_pmu_info = {
+	.mask_pmcs = {PFM_MCK_MASK_PMCS,},
+};
+
+/* reserved bits are 1 in the mask */
+#define PFM_ITA2_RSVD 0xfffffffffc8000a0UL
+
+/*
+ * For debug registers, writing xBR(y) means we use also xBR(y+1). Hence using
+ * PMC256+y means we use PMC256+y+1.  Yet, we do not have dependency information
+ * but this is fine because they are handled separately in the IA-64 specific
+ * code.
+ */
+static struct pfm_regmap_desc pfm_mck_pmc_desc[] = {
+/* pmc0  */ PMX_NA,
+/* pmc1  */ PMX_NA,
+/* pmc2  */ PMX_NA,
+/* pmc3  */ PMX_NA,
+/* pmc4  */ PMC_D(PFM_REG_W64, "PMC4" , 0x800020UL, 0xfffffffffc8000a0, PFM_MCK_NO64, 4),
+/* pmc5  */ PMC_D(PFM_REG_W64, "PMC5" , 0x20UL, PFM_ITA2_RSVD, PFM_MCK_NO64, 5),
+/* pmc6  */ PMC_D(PFM_REG_W64, "PMC6" , 0x20UL, PFM_ITA2_RSVD, PFM_MCK_NO64, 6),
+/* pmc7  */ PMC_D(PFM_REG_W64, "PMC7" , 0x20UL, PFM_ITA2_RSVD, PFM_MCK_NO64, 7),
+/* pmc8  */ PMC_D(PFM_REG_W  , "PMC8" , 0xffffffff3fffffffUL, 0xc0000004UL, 0, 8),
+/* pmc9  */ PMC_D(PFM_REG_W  , "PMC9" , 0xffffffff3ffffffcUL, 0xc0000004UL, 0, 9),
+/* pmc10 */ PMC_D(PFM_REG_W  , "PMC10", 0x0, 0xffffffffffff0000UL, 0, 10),
+/* pmc11 */ PMC_D(PFM_REG_W  , "PMC11", 0x0, 0xfffffffffcf0fe30UL, 0, 11),
+/* pmc12 */ PMC_D(PFM_REG_W  , "PMC12", 0x0, 0xffffffffffff0000UL, 0, 12),
+/* pmc13 */ PMC_D(PFM_REG_W  , "PMC13", 0x2078fefefefeUL, 0xfffe1fffe7e7e7e7UL, 0, 13),
+/* pmc14 */ PMC_D(PFM_REG_W  , "PMC14", 0x0db60db60db60db6UL, 0xffffffffffffdb6dUL, 0, 14),
+/* pmc15 */ PMC_D(PFM_REG_W  , "PMC15", 0xfffffff0UL, 0xfffffffffffffff0UL, 0, 15),
+/* pmc16 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc24 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc32 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc40 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc48 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc56 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc64 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc72 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc80 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc88 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc96 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc104 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc112 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc120 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc128 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc136 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc144 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc152 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc160 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc168 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc176 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc184 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc192 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc200 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc208 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc216 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc224 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc232 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc240 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc248 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc256 */ PMC_D(PFM_REG_W  , "IBR0", 0x0, 0, 0, 0),
+/* pmc257 */ PMC_D(PFM_REG_W  , "IBR1", 0x0, 0x8000000000000000UL, 0, 1),
+/* pmc258 */ PMC_D(PFM_REG_W  , "IBR2", 0x0, 0, 0, 2),
+/* pmc259 */ PMC_D(PFM_REG_W  , "IBR3", 0x0, 0x8000000000000000UL, 0, 3),
+/* pmc260 */ PMC_D(PFM_REG_W  , "IBR4", 0x0, 0, 0, 4),
+/* pmc261 */ PMC_D(PFM_REG_W  , "IBR5", 0x0, 0x8000000000000000UL, 0, 5),
+/* pmc262 */ PMC_D(PFM_REG_W  , "IBR6", 0x0, 0, 0, 6),
+/* pmc263 */ PMC_D(PFM_REG_W  , "IBR7", 0x0, 0x8000000000000000UL, 0, 7),
+/* pmc264 */ PMC_D(PFM_REG_W  , "DBR0", 0x0, 0, 0, 0),
+/* pmc265 */ PMC_D(PFM_REG_W  , "DBR1", 0x0, 0xc000000000000000UL, 0, 1),
+/* pmc266 */ PMC_D(PFM_REG_W  , "DBR2", 0x0, 0, 0, 2),
+/* pmc267 */ PMC_D(PFM_REG_W  , "DBR3", 0x0, 0xc000000000000000UL, 0, 3),
+/* pmc268 */ PMC_D(PFM_REG_W  , "DBR4", 0x0, 0, 0, 4),
+/* pmc269 */ PMC_D(PFM_REG_W  , "DBR5", 0x0, 0xc000000000000000UL, 0, 5),
+/* pmc270 */ PMC_D(PFM_REG_W  , "DBR6", 0x0, 0, 0, 6),
+/* pmc271 */ PMC_D(PFM_REG_W  , "DBR7", 0x0, 0xc000000000000000UL, 0, 7)
+};
+#define PFM_MCK_NUM_PMCS ARRAY_SIZE(pfm_mck_pmc_desc)
+
+static struct pfm_regmap_desc pfm_mck_pmd_desc[] = {
+/* pmd0  */ PMD_DP(PFM_REG_I, "PMD0", 0, 1ull << 10),
+/* pmd1  */ PMD_DP(PFM_REG_I, "PMD1", 1, 1ull << 10),
+/* pmd2  */ PMD_DP(PFM_REG_I, "PMD2", 2, 1ull << 11),
+/* pmd3  */ PMD_DP(PFM_REG_I, "PMD3", 3, 1ull << 11),
+/* pmd4  */ PMD_DP(PFM_REG_C, "PMD4", 4, 1ull << 4),
+/* pmd5  */ PMD_DP(PFM_REG_C, "PMD5", 5, 1ull << 5),
+/* pmd6  */ PMD_DP(PFM_REG_C, "PMD6", 6, 1ull << 6),
+/* pmd7  */ PMD_DP(PFM_REG_C, "PMD7", 7, 1ull << 7),
+/* pmd8  */ PMD_DP(PFM_REG_I, "PMD8", 8, 1ull << 12),
+/* pmd9  */ PMD_DP(PFM_REG_I, "PMD9", 9, 1ull << 12),
+/* pmd10 */ PMD_DP(PFM_REG_I, "PMD10", 10, 1ull << 12),
+/* pmd11 */ PMD_DP(PFM_REG_I, "PMD11", 11, 1ull << 12),
+/* pmd12 */ PMD_DP(PFM_REG_I, "PMD12", 12, 1ull << 12),
+/* pmd13 */ PMD_DP(PFM_REG_I, "PMD13", 13, 1ull << 12),
+/* pmd14 */ PMD_DP(PFM_REG_I, "PMD14", 14, 1ull << 12),
+/* pmd15 */ PMD_DP(PFM_REG_I, "PMD15", 15, 1ull << 12),
+/* pmd16 */ PMD_DP(PFM_REG_I, "PMD16", 16, 1ull << 12),
+/* pmd17 */ PMD_DP(PFM_REG_I, "PMD17", 17, 1ull << 11)
+};
+#define PFM_MCK_NUM_PMDS ARRAY_SIZE(pfm_mck_pmd_desc)
+
+static int pfm_mck_pmc_check(struct pfm_context *ctx,
+			     struct pfm_event_set *set,
+			     struct pfarg_pmc *req)
+{
+	struct pfm_arch_context *ctx_arch;
+	u64 val8 = 0, val14 = 0, val13 = 0;
+	u64 tmpval;
+	u16 cnum;
+	int ret = 0, check_case1 = 0;
+	int is_system;
+
+	tmpval = req->reg_value;
+	cnum = req->reg_num;
+	ctx_arch = pfm_ctx_arch(ctx);
+	is_system = ctx->flags.system;
+
+#define PFM_MCK_PMC_PM_POS6	(1UL<<6)
+#define PFM_MCK_PMC_PM_POS4	(1UL<<4)
+
+	switch (cnum) {
+	case  4:
+	case  5:
+	case  6:
+	case  7:
+	case 11:
+	case 12:
+		if (is_system)
+			tmpval |= PFM_MCK_PMC_PM_POS6;
+		else
+			tmpval &= ~PFM_MCK_PMC_PM_POS6;
+		break;
+
+	case  8:
+		val8 = tmpval;
+		val13 = set->pmcs[13];
+		val14 = set->pmcs[14];
+		check_case1 = 1;
+		break;
+
+	case 10:
+		if (is_system)
+			tmpval |= PFM_MCK_PMC_PM_POS4;
+		else
+			tmpval &= ~PFM_MCK_PMC_PM_POS4;
+		break;
+
+	case 13:
+		val8 = set->pmcs[8];
+		val13 = tmpval;
+		val14 = set->pmcs[14];
+		check_case1 = 1;
+		break;
+
+	case 14:
+		val8 = set->pmcs[8];
+		val13 = set->pmcs[13];
+		val14 = tmpval;
+		check_case1 = 1;
+		break;
+	}
+
+	/*
+	 * check illegal configuration which can produce inconsistencies
+	 * in tagging i-side events in L1D and L2 caches
+	 */
+	if (check_case1) {
+		ret = (((val13 >> 45) & 0xf) == 0 && ((val8 & 0x1) == 0))
+			&& ((((val14>>1) & 0x3) == 0x2 || ((val14>>1) & 0x3) == 0x0)
+			|| (((val14>>4) & 0x3) == 0x2 || ((val14>>4) & 0x3) == 0x0));
+
+		if (ret) {
+			PFM_DBG("perfmon: invalid config pmc8=0x%lx "
+				"pmc13=0x%lx pmc14=0x%lx",
+				val8, val13, val14);
+			return -EINVAL;
+		}
+	}
+
+	/*
+	 * check if configuration implicitely activates the use of
+	 * the debug registers. If true, then we ensure that this is
+	 * possible and that we do not pick up stale value in the HW
+	 * registers.
+	 *
+	 * We postpone the checks of pmc13 and pmc14 to avoid side effects
+	 * in case of errors
+	 */
+
+	/*
+	 * pmc13 is "active" if:
+	 * 	one of the pmc13.cfg_dbrpXX field is different from 0x3
+	 * AND
+	 * 	at the corresponding pmc13.ena_dbrpXX is set.
+	 */
+	if (cnum == 13 && (tmpval & 0x1e00000000000UL)
+	    && (tmpval & 0x18181818UL) != 0x18181818UL
+		&& ctx_arch->flags.use_dbr == 0) {
+		PFM_DBG("pmc13=0x%lx active", tmpval);
+		ret = pfm_ia64_mark_dbregs_used(ctx, set);
+		if (ret)
+			return ret;
+	}
+
+	/*
+	 *  if any pmc14.ibrpX bit is enabled we must clear the ibrs
+	 */
+	if (cnum == 14 && ((tmpval & 0x2222UL) != 0x2222UL)
+		&& ctx_arch->flags.use_dbr == 0) {
+		PFM_DBG("pmc14=0x%lx active", tmpval);
+		ret = pfm_ia64_mark_dbregs_used(ctx, set);
+		if (ret)
+			return ret;
+	}
+
+	req->reg_value = tmpval;
+
+	return 0;
+}
+
+static int pfm_mck_probe_pmu(void)
+{
+	return local_cpu_data->family == 0x1f ? 0 : -1;
+}
+
+/*
+ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+ */
+static struct pfm_pmu_config pfm_mck_pmu_conf = {
+	.pmu_name = "Itanium 2",
+	.counter_width = 47,
+	.pmd_desc = pfm_mck_pmd_desc,
+	.pmc_desc = pfm_mck_pmc_desc,
+	.pmc_write_check = pfm_mck_pmc_check,
+	.num_pmc_entries = PFM_MCK_NUM_PMCS,
+	.num_pmd_entries = PFM_MCK_NUM_PMDS,
+	.probe_pmu = pfm_mck_probe_pmu,
+	.version = "1.0",
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE,
+	.pmu_info = &pfm_mck_pmu_info,
+};
+
+static int __init pfm_mck_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_mck_pmu_conf);
+}
+
+static void __exit pfm_mck_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_mck_pmu_conf);
+}
+
+module_init(pfm_mck_pmu_init_module);
+module_exit(pfm_mck_pmu_cleanup_module);
Index: linux-2.6.31-master/arch/ia64/perfmon/perfmon_montecito.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/arch/ia64/perfmon/perfmon_montecito.c
@@ -0,0 +1,412 @@
+/*
+ * This file contains the McKinley PMU register description tables
+ * and pmc checker used by perfmon.c.
+ *
+ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+  */
+#include <linux/module.h>
+#include <linux/smp.h>
+#include <linux/perfmon_kern.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("Dual-Core Itanium 2 (Montecito) PMU description table");
+MODULE_LICENSE("GPL");
+
+#define RDEP(x)	(1UL << (x))
+
+#define PFM_MONT_MASK_PMCS (RDEP(4)|RDEP(5)|RDEP(6)|RDEP(7)|\
+			    RDEP(8)|RDEP(9)|RDEP(10)|RDEP(11)|\
+			    RDEP(12)|RDEP(13)|RDEP(14)|RDEP(15)|\
+			    RDEP(37)|RDEP(39)|RDEP(40)|RDEP(42))
+
+#define PFM_MONT_NO64	(1UL<<5)
+
+static struct pfm_arch_pmu_info pfm_mont_pmu_info = {
+	.mask_pmcs = {PFM_MONT_MASK_PMCS,},
+};
+
+#define PFM_MONT_RSVD 0xffffffff838000a0UL
+/*
+ *
+ * For debug registers, writing xBR(y) means we use also xBR(y+1). Hence using
+ * PMC256+y means we use PMC256+y+1.  Yet, we do not have dependency information
+ * but this is fine because they are handled separately in the IA-64 specific
+ * code.
+ *
+ * For PMC4-PMC15, PMC40: we force pmc.ism=2 (IA-64 mode only)
+ */
+static struct pfm_regmap_desc pfm_mont_pmc_desc[] = {
+/* pmc0  */ PMX_NA,
+/* pmc1  */ PMX_NA,
+/* pmc2  */ PMX_NA,
+/* pmc3  */ PMX_NA,
+/* pmc4  */ PMC_D(PFM_REG_W64, "PMC4" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 4),
+/* pmc5  */ PMC_D(PFM_REG_W64, "PMC5" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 5),
+/* pmc6  */ PMC_D(PFM_REG_W64, "PMC6" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 6),
+/* pmc7  */ PMC_D(PFM_REG_W64, "PMC7" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 7),
+/* pmc8  */ PMC_D(PFM_REG_W64, "PMC8" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 8),
+/* pmc9  */ PMC_D(PFM_REG_W64, "PMC9" , 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 9),
+/* pmc10 */ PMC_D(PFM_REG_W64, "PMC10", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 10),
+/* pmc11 */ PMC_D(PFM_REG_W64, "PMC11", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 11),
+/* pmc12 */ PMC_D(PFM_REG_W64, "PMC12", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 12),
+/* pmc13 */ PMC_D(PFM_REG_W64, "PMC13", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 13),
+/* pmc14 */ PMC_D(PFM_REG_W64, "PMC14", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 14),
+/* pmc15 */ PMC_D(PFM_REG_W64, "PMC15", 0x2000020UL, PFM_MONT_RSVD, PFM_MONT_NO64, 15),
+/* pmc16 */ PMX_NA,
+/* pmc17 */ PMX_NA,
+/* pmc18 */ PMX_NA,
+/* pmc19 */ PMX_NA,
+/* pmc20 */ PMX_NA,
+/* pmc21 */ PMX_NA,
+/* pmc22 */ PMX_NA,
+/* pmc23 */ PMX_NA,
+/* pmc24 */ PMX_NA,
+/* pmc25 */ PMX_NA,
+/* pmc26 */ PMX_NA,
+/* pmc27 */ PMX_NA,
+/* pmc28 */ PMX_NA,
+/* pmc29 */ PMX_NA,
+/* pmc30 */ PMX_NA,
+/* pmc31 */ PMX_NA,
+/* pmc32 */ PMC_D(PFM_REG_W , "PMC32", 0x30f01ffffffffffUL, 0xfcf0fe0000000000UL, 0, 32),
+/* pmc33 */ PMC_D(PFM_REG_W , "PMC33", 0x0, 0xfffffe0000000000UL, 0, 33),
+/* pmc34 */ PMC_D(PFM_REG_W , "PMC34", 0xf01ffffffffffUL, 0xfff0fe0000000000UL, 0, 34),
+/* pmc35 */ PMC_D(PFM_REG_W , "PMC35", 0x0,  0x1ffffffffffUL, 0, 35),
+/* pmc36 */ PMC_D(PFM_REG_W , "PMC36", 0xfffffff0UL, 0xfffffffffffffff0UL, 0, 36),
+/* pmc37 */ PMC_D(PFM_REG_W , "PMC37", 0x0, 0xffffffffffffc000UL, 0, 37),
+/* pmc38 */ PMC_D(PFM_REG_W , "PMC38", 0xdb6UL, 0xffffffffffffdb6dUL, 0, 38),
+/* pmc39 */ PMC_D(PFM_REG_W , "PMC39", 0x0, 0xffffffffffff0030UL, 0, 39),
+/* pmc40 */ PMC_D(PFM_REG_W , "PMC40", 0x2000000UL, 0xfffffffffff0fe30UL, 0, 40),
+/* pmc41 */ PMC_D(PFM_REG_W , "PMC41", 0x00002078fefefefeUL, 0xfffe1fffe7e7e7e7UL, 0, 41),
+/* pmc42 */ PMC_D(PFM_REG_W , "PMC42", 0x0, 0xfff800b0UL, 0, 42),
+/* pmc43 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc48 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc56 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc64 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc72 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc80 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc88 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc96 */  PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc104 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc112 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc120 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc128 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc136 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc144 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc152 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc160 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc168 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc176 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc184 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc192 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc200 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc208 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc216 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc224 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc232 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc240 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc248 */ PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA, PMX_NA,
+/* pmc256 */ PMC_D(PFM_REG_W, "IBR0", 0x0, 0, 0, 0),
+/* pmc257 */ PMC_D(PFM_REG_W, "IBR1", 0x0, 0x8000000000000000UL, 0, 1),
+/* pmc258 */ PMC_D(PFM_REG_W, "IBR2", 0x0, 0, 0, 2),
+/* pmc259 */ PMC_D(PFM_REG_W, "IBR3", 0x0, 0x8000000000000000UL, 0, 3),
+/* pmc260 */ PMC_D(PFM_REG_W, "IBR4", 0x0, 0, 0, 4),
+/* pmc261 */ PMC_D(PFM_REG_W, "IBR5", 0x0, 0x8000000000000000UL, 0, 5),
+/* pmc262 */ PMC_D(PFM_REG_W, "IBR6", 0x0, 0, 0, 6),
+/* pmc263 */ PMC_D(PFM_REG_W, "IBR7", 0x0, 0x8000000000000000UL, 0, 7),
+/* pmc264 */ PMC_D(PFM_REG_W, "DBR0", 0x0, 0, 0, 0),
+/* pmc265 */ PMC_D(PFM_REG_W, "DBR1", 0x0, 0xc000000000000000UL, 0, 1),
+/* pmc266 */ PMC_D(PFM_REG_W, "DBR2", 0x0, 0, 0, 2),
+/* pmc267 */ PMC_D(PFM_REG_W, "DBR3", 0x0, 0xc000000000000000UL, 0, 3),
+/* pmc268 */ PMC_D(PFM_REG_W, "DBR4", 0x0, 0, 0, 4),
+/* pmc269 */ PMC_D(PFM_REG_W, "DBR5", 0x0, 0xc000000000000000UL, 0, 5),
+/* pmc270 */ PMC_D(PFM_REG_W, "DBR6", 0x0, 0, 0, 6),
+/* pmc271 */ PMC_D(PFM_REG_W, "DBR7", 0x0, 0xc000000000000000UL, 0, 7)
+};
+#define PFM_MONT_NUM_PMCS ARRAY_SIZE(pfm_mont_pmc_desc)
+
+static struct pfm_regmap_desc pfm_mont_pmd_desc[] = {
+/* pmd0  */ PMX_NA,
+/* pmd1  */ PMX_NA,
+/* pmd2  */ PMX_NA,
+/* pmd3  */ PMX_NA,
+/* pmd4  */ PMD_DP(PFM_REG_C, "PMD4", 4, 1ull << 4),
+/* pmd5  */ PMD_DP(PFM_REG_C, "PMD5", 5, 1ull << 5),
+/* pmd6  */ PMD_DP(PFM_REG_C, "PMD6", 6, 1ull << 6),
+/* pmd7  */ PMD_DP(PFM_REG_C, "PMD7", 7, 1ull << 7),
+/* pmd8  */ PMD_DP(PFM_REG_C, "PMD8", 8, 1ull << 8),
+/* pmd9  */ PMD_DP(PFM_REG_C, "PMD9", 9, 1ull << 9),
+/* pmd10 */ PMD_DP(PFM_REG_C, "PMD10", 10, 1ull << 10),
+/* pmd11 */ PMD_DP(PFM_REG_C, "PMD11", 11, 1ull << 11),
+/* pmd12 */ PMD_DP(PFM_REG_C, "PMD12", 12, 1ull << 12),
+/* pmd13 */ PMD_DP(PFM_REG_C, "PMD13", 13, 1ull << 13),
+/* pmd14 */ PMD_DP(PFM_REG_C, "PMD14", 14, 1ull << 14),
+/* pmd15 */ PMD_DP(PFM_REG_C, "PMD15", 15, 1ull << 15),
+/* pmd16 */ PMX_NA,
+/* pmd17 */ PMX_NA,
+/* pmd18 */ PMX_NA,
+/* pmd19 */ PMX_NA,
+/* pmd20 */ PMX_NA,
+/* pmd21 */ PMX_NA,
+/* pmd22 */ PMX_NA,
+/* pmd23 */ PMX_NA,
+/* pmd24 */ PMX_NA,
+/* pmd25 */ PMX_NA,
+/* pmd26 */ PMX_NA,
+/* pmd27 */ PMX_NA,
+/* pmd28 */ PMX_NA,
+/* pmd29 */ PMX_NA,
+/* pmd30 */ PMX_NA,
+/* pmd31 */ PMX_NA,
+/* pmd32 */ PMD_DP(PFM_REG_I, "PMD32", 32, 1ull << 40),
+/* pmd33 */ PMD_DP(PFM_REG_I, "PMD33", 33, 1ull << 40),
+/* pmd34 */ PMD_DP(PFM_REG_I, "PMD34", 34, 1ull << 37),
+/* pmd35 */ PMD_DP(PFM_REG_I, "PMD35", 35, 1ull << 37),
+/* pmd36 */ PMD_DP(PFM_REG_I, "PMD36", 36, 1ull << 40),
+/* pmd37 */ PMX_NA,
+/* pmd38 */ PMD_DP(PFM_REG_I, "PMD38", 38, (1ull<<39)|(1ull<<42)),
+/* pmd39 */ PMD_DP(PFM_REG_I, "PMD39", 39, (1ull<<39)|(1ull<<42)),
+/* pmd40 */ PMX_NA,
+/* pmd41 */ PMX_NA,
+/* pmd42 */ PMX_NA,
+/* pmd43 */ PMX_NA,
+/* pmd44 */ PMX_NA,
+/* pmd45 */ PMX_NA,
+/* pmd46 */ PMX_NA,
+/* pmd47 */ PMX_NA,
+/* pmd48 */ PMD_DP(PFM_REG_I, "PMD48", 48, (1ull<<39)|(1ull<<42)),
+/* pmd49 */ PMD_DP(PFM_REG_I, "PMD49", 49, (1ull<<39)|(1ull<<42)),
+/* pmd50 */ PMD_DP(PFM_REG_I, "PMD50", 50, (1ull<<39)|(1ull<<42)),
+/* pmd51 */ PMD_DP(PFM_REG_I, "PMD51", 51, (1ull<<39)|(1ull<<42)),
+/* pmd52 */ PMD_DP(PFM_REG_I, "PMD52", 52, (1ull<<39)|(1ull<<42)),
+/* pmd53 */ PMD_DP(PFM_REG_I, "PMD53", 53, (1ull<<39)|(1ull<<42)),
+/* pmd54 */ PMD_DP(PFM_REG_I, "PMD54", 54, (1ull<<39)|(1ull<<42)),
+/* pmd55 */ PMD_DP(PFM_REG_I, "PMD55", 55, (1ull<<39)|(1ull<<42)),
+/* pmd56 */ PMD_DP(PFM_REG_I, "PMD56", 56, (1ull<<39)|(1ull<<42)),
+/* pmd57 */ PMD_DP(PFM_REG_I, "PMD57", 57, (1ull<<39)|(1ull<<42)),
+/* pmd58 */ PMD_DP(PFM_REG_I, "PMD58", 58, (1ull<<39)|(1ull<<42)),
+/* pmd59 */ PMD_DP(PFM_REG_I, "PMD59", 59, (1ull<<39)|(1ull<<42)),
+/* pmd60 */ PMD_DP(PFM_REG_I, "PMD60", 60, (1ull<<39)|(1ull<<42)),
+/* pmd61 */ PMD_DP(PFM_REG_I, "PMD61", 61, (1ull<<39)|(1ull<<42)),
+/* pmd62 */ PMD_DP(PFM_REG_I, "PMD62", 62, (1ull<<39)|(1ull<<42)),
+/* pmd63 */ PMD_DP(PFM_REG_I, "PMD63", 63, (1ull<<39)|(1ull<<42))
+};
+#define PFM_MONT_NUM_PMDS ARRAY_SIZE(pfm_mont_pmd_desc)
+
+static int pfm_mont_has_ht;
+
+static int pfm_mont_pmc_check(struct pfm_context *ctx,
+			      struct pfm_event_set *set,
+			      struct pfarg_pmc *req)
+{
+	struct pfm_arch_context *ctx_arch;
+	u64 val32 = 0, val38 = 0, val41 = 0;
+	u64 tmpval;
+	u16 cnum;
+	int ret = 0, check_case1 = 0;
+	int is_system;
+
+	tmpval = req->reg_value;
+	cnum = req->reg_num;
+	ctx_arch = pfm_ctx_arch(ctx);
+	is_system = ctx->flags.system;
+
+#define PFM_MONT_PMC_PM_POS6	(1UL<<6)
+#define PFM_MONT_PMC_PM_POS4	(1UL<<4)
+
+	switch (cnum) {
+	case  4:
+	case  5:
+	case  6:
+	case  7:
+	case  8:
+	case  9:
+		if (is_system)
+			tmpval |= PFM_MONT_PMC_PM_POS6;
+		else
+			tmpval &= ~PFM_MONT_PMC_PM_POS6;
+		break;
+	case 10:
+	case 11:
+	case 12:
+	case 13:
+	case 14:
+	case 15:
+		if ((req->reg_flags & PFM_REGFL_NO_EMUL64) == 0) {
+			if (pfm_mont_has_ht) {
+				PFM_INFO("perfmon: Errata 121 PMD10/PMD15 cannot be used to overflow"
+					 "when threads on on");
+				return -EINVAL;
+			}
+		}
+		if (is_system)
+			tmpval |= PFM_MONT_PMC_PM_POS6;
+		else
+			tmpval &= ~PFM_MONT_PMC_PM_POS6;
+		break;
+	case 39:
+	case 40:
+	case 42:
+		if (pfm_mont_has_ht && ((req->reg_value >> 8) & 0x7) == 4) {
+			PFM_INFO("perfmon: Errata 120: IP-EAR not available when threads are on");
+			return -EINVAL;
+		}
+		if (is_system)
+			tmpval |= PFM_MONT_PMC_PM_POS6;
+		else
+			tmpval &= ~PFM_MONT_PMC_PM_POS6;
+		break;
+
+	case  32:
+		val32 = tmpval;
+		val38 = set->pmcs[38];
+		val41 = set->pmcs[41];
+		check_case1 = 1;
+		break;
+
+	case  37:
+		if (is_system)
+			tmpval |= PFM_MONT_PMC_PM_POS4;
+		else
+			tmpval &= ~PFM_MONT_PMC_PM_POS4;
+		break;
+
+	case  38:
+		val38 = tmpval;
+		val32 = set->pmcs[32];
+		val41 = set->pmcs[41];
+		check_case1 = 1;
+		break;
+	case  41:
+		val41 = tmpval;
+		val32 = set->pmcs[32];
+		val38 = set->pmcs[38];
+		check_case1 = 1;
+		break;
+	}
+
+	if (check_case1) {
+		ret = (((val41 >> 45) & 0xf) == 0 && ((val32>>57) & 0x1) == 0)
+		     && ((((val38>>1) & 0x3) == 0x2 || ((val38>>1) & 0x3) == 0)
+		     || (((val38>>4) & 0x3) == 0x2 || ((val38>>4) & 0x3) == 0));
+		if (ret) {
+			PFM_DBG("perfmon: invalid config pmc38=0x%lx "
+				"pmc41=0x%lx pmc32=0x%lx",
+				val38, val41, val32);
+			return -EINVAL;
+		}
+	}
+
+	/*
+	 * check if configuration implicitely activates the use of the
+	 * debug registers. If true, then we ensure that this is possible
+	 * and that we do not pick up stale value in the HW registers.
+	 */
+
+	/*
+	 *
+	 * pmc41 is "active" if:
+	 * 	one of the pmc41.cfgdtagXX field is different from 0x3
+	 * AND
+	 * 	the corsesponding pmc41.en_dbrpXX is set.
+	 * AND
+	 *	ctx_fl_use_dbr (dbr not yet used)
+	 */
+	if (cnum == 41
+	    && (tmpval & 0x1e00000000000)
+		&& (tmpval & 0x18181818) != 0x18181818
+		&& ctx_arch->flags.use_dbr == 0) {
+		PFM_DBG("pmc41=0x%lx active, clearing dbr", tmpval);
+		ret = pfm_ia64_mark_dbregs_used(ctx, set);
+		if (ret)
+			return ret;
+	}
+	/*
+	 * we must clear the (instruction) debug registers if:
+	 * 	pmc38.ig_ibrpX is 0 (enabled)
+	 * and
+	 * 	fl_use_dbr == 0 (dbr not yet used)
+	 */
+	if (cnum == 38 && ((tmpval & 0x492) != 0x492)
+		&& ctx_arch->flags.use_dbr == 0) {
+		PFM_DBG("pmc38=0x%lx active pmc38, clearing ibr", tmpval);
+		ret = pfm_ia64_mark_dbregs_used(ctx, set);
+		if (ret)
+			return ret;
+
+	}
+	req->reg_value = tmpval;
+	return 0;
+}
+
+static void pfm_handle_errata(void)
+{
+	pfm_mont_has_ht = 1;
+
+	PFM_INFO("activating workaround for errata 120 "
+		 "(Disable IP-EAR when threads are on)");
+
+	PFM_INFO("activating workaround for Errata 121 "
+		 "(PMC10-PMC15 cannot be used to overflow"
+		 " when threads are on");
+}
+static int pfm_mont_probe_pmu(void)
+{
+	if (local_cpu_data->family != 0x20)
+		return -1;
+
+	/*
+	 * the 2 errata must be activated when
+	 * threads are/can be enabled
+	 */
+	if (is_multithreading_enabled())
+		pfm_handle_errata();
+
+	return 0;
+}
+
+/*
+ * impl_pmcs, impl_pmds are computed at runtime to minimize errors!
+ */
+static struct pfm_pmu_config pfm_mont_pmu_conf = {
+	.pmu_name = "Montecito",
+	.counter_width = 47,
+	.pmd_desc = pfm_mont_pmd_desc,
+	.pmc_desc = pfm_mont_pmc_desc,
+	.num_pmc_entries = PFM_MONT_NUM_PMCS,
+	.num_pmd_entries = PFM_MONT_NUM_PMDS,
+	.pmc_write_check = pfm_mont_pmc_check,
+	.probe_pmu = pfm_mont_probe_pmu,
+	.version = "1.0",
+	.pmu_info = &pfm_mont_pmu_info,
+	.flags = PFM_PMU_BUILTIN_FLAG,
+	.owner = THIS_MODULE
+};
+
+static int __init pfm_mont_pmu_init_module(void)
+{
+	return pfm_pmu_register(&pfm_mont_pmu_conf);
+}
+
+static void __exit pfm_mont_pmu_cleanup_module(void)
+{
+	pfm_pmu_unregister(&pfm_mont_pmu_conf);
+}
+
+module_init(pfm_mont_pmu_init_module);
+module_exit(pfm_mont_pmu_cleanup_module);
Index: linux-2.6.31-master/include/linux/Kbuild
===================================================================
--- linux-2.6.31-master.orig/include/linux/Kbuild
+++ linux-2.6.31-master/include/linux/Kbuild
@@ -165,6 +165,10 @@ header-y += utime.h
 header-y += veth.h
 header-y += videotext.h
 header-y += x25.h
+ifeq ($(ARCH), "ia64")
+header-y += perfmon.h
+header-y += perfmon_dfl_smpl.h
+endif
 
 unifdef-y += acct.h
 unifdef-y += adb.h
Index: linux-2.6.31-master/include/linux/perfmon.h
===================================================================
--- /dev/null
+++ linux-2.6.31-master/include/linux/perfmon.h
@@ -0,0 +1,213 @@
+/*
+ * Copyright (c) 2001-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+
+#ifndef __LINUX_PERFMON_H__
+#define __LINUX_PERFMON_H__
+
+/*
+ * This file contains all the user visible generic definitions for the
+ * interface. Model-specific user-visible definitions are located in
+ * the asm/perfmon.h file.
+ */
+
+/*
+ * include arch-specific user interface definitions
+ */
+#include <asm/perfmon.h>
+
+/*
+ * defined by each arch
+ */
+#define PFM_MAX_PMCS	PFM_ARCH_MAX_PMCS
+#define PFM_MAX_PMDS	PFM_ARCH_MAX_PMDS
+
+/*
+ * number of elements for each type of bitvector
+ * all bitvectors use u64 fixed size type on all architectures.
+ */
+#define PFM_BVSIZE(x)	(((x)+(sizeof(__u64)<<3)-1) / (sizeof(__u64)<<3))
+#define PFM_PMD_BV	PFM_BVSIZE(PFM_MAX_PMDS)
+#define PFM_PMC_BV	PFM_BVSIZE(PFM_MAX_PMCS)
+
+/*
+ * register flags layout:
+ * bit[00-15] : generic flags
+ * bit[16-31] : arch-specific flags
+ *
+ * PFM_REGFL_NO_EMUL64: must be set on the PMC controlling the PMD
+ */
+#define PFM_REGFL_OVFL_NOTIFY	0x1	/* PMD: send notification on event */
+#define PFM_REGFL_RANDOM	0x2	/* PMD: randomize value after event */
+#define PFM_REGFL_NO_EMUL64	0x4	/* PMC: no 64-bit emulation */
+
+/*
+ * event set flags layout:
+ * bits[00-15] : generic flags
+ * bits[16-31] : arch-specific flags (see asm/perfmon.h)
+ */
+#define PFM_SETFL_OVFL_SWITCH	0x01 /* enable switch on overflow */
+#define PFM_SETFL_TIME_SWITCH	0x02 /* enable switch on timeout */
+
+/*
+ * argument to pfm_create_context() system call
+ * structure shared with user level
+ */
+struct pfarg_ctx {
+	__u32		ctx_flags;	  /* noblock/block/syswide */
+	__u32		ctx_reserved1;	  /* for future use */
+	__u64		ctx_reserved2[7]; /* for future use */
+};
+
+/*
+ * context flags layout:
+ * bits[00-15]: generic flags
+ * bits[16-31]: arch-specific flags (see perfmon_const.h)
+ */
+#define PFM_FL_NOTIFY_BLOCK    	 0x01	/* block task on user notifications */
+#define PFM_FL_SYSTEM_WIDE	 0x02	/* create a system wide context */
+#define PFM_FL_OVFL_NO_MSG	 0x80   /* no overflow msgs */
+
+/*
+ * argument to pfm_write_pmcs() system call.
+ * structure shared with user level
+ */
+struct pfarg_pmc {
+	__u16 reg_num;		/* which register */
+	__u16 reg_set;		/* event set for this register */
+	__u32 reg_flags;	/* REGFL flags */
+	__u64 reg_value;	/* pmc value */
+	__u64 reg_reserved2[4];	/* for future use */
+};
+
+/*
+ * argument to pfm_write_pmds() and pfm_read_pmds() system calls.
+ * structure shared with user level
+ */
+struct pfarg_pmd {
+	__u16 reg_num;	   	/* which register */
+	__u16 reg_set;	   	/* event set for this register */
+	__u32 reg_flags; 	/* REGFL flags */
+	__u64 reg_value;	/* initial pmc/pmd value */
+	__u64 reg_long_reset;	/* value to reload after notification */
+	__u64 reg_short_reset;  /* reset after counter overflow */
+	__u64 reg_last_reset_val;	/* return: PMD last reset value */
+	__u64 reg_ovfl_switch_cnt;	/* #overflows before switch */
+	__u64 reg_reset_pmds[PFM_PMD_BV]; /* reset on overflow */
+	__u64 reg_smpl_pmds[PFM_PMD_BV];  /* record in sample */
+	__u64 reg_smpl_eventid; /* opaque event identifier */
+	__u64 reg_random_mask; 	/* bitmask used to limit random value */
+	__u32 reg_random_seed;  /* seed for randomization (OBSOLETE) */
+	__u32 reg_reserved2[7];	/* for future use */
+};
+
+/*
+ * optional argument to pfm_start() system call. Pass NULL if not needed.
+ * structure shared with user level
+ */
+struct pfarg_start {
+	__u16 start_set;	/* event set to start with */
+	__u16 start_reserved1;	/* for future use */
+	__u32 start_reserved2;	/* for future use */
+	__u64 reserved3[3];	/* for future use */
+};
+
+/*
+ * argument to pfm_load_context() system call.
+ * structure shared with user level
+ */
+struct pfarg_load {
+	__u32	load_pid;	   /* thread or CPU to attach to */
+	__u16	load_set;	   /* set to load first */
+	__u16	load_reserved1;	   /* for future use */
+	__u64	load_reserved2[3]; /* for future use */
+};
+
+/*
+ * argument to pfm_create_evtsets() and pfm_delete_evtsets() system calls.
+ * structure shared with user level.
+ */
+struct pfarg_setdesc {
+	__u16	set_id;		  /* which set */
+	__u16	set_reserved1;	  /* for future use */
+	__u32	set_flags; 	  /* SETFL flags  */
+	__u64	set_timeout;	  /* switch timeout in nsecs */
+	__u64	reserved[6];	  /* for future use */
+};
+
+/*
+ * argument to pfm_getinfo_evtsets() system call.
+ * structure shared with user level
+ */
+struct pfarg_setinfo {
+	__u16	set_id;			/* which set */
+	__u16	set_reserved1;		/* for future use */
+	__u32	set_flags;		/* out: SETFL flags */
+	__u64 	set_ovfl_pmds[PFM_PMD_BV]; /* out: last ovfl PMDs */
+	__u64	set_runs;		/* out: #times the set was active */
+	__u64	set_timeout;		/* out: eff/leftover timeout (nsecs) */
+	__u64	set_act_duration;	/* out: time set was active in nsecs */
+	__u64	set_avail_pmcs[PFM_PMC_BV];/* out: available PMCs */
+	__u64	set_avail_pmds[PFM_PMD_BV];/* out: available PMDs */
+	__u64	set_reserved3[6];	/* for future use */
+};
+
+/*
+ * default value for the user and group security parameters in
+ * /proc/sys/kernel/perfmon/sys_group
+ * /proc/sys/kernel/perfmon/task_group
+ */
+#define PFM_GROUP_PERM_ANY	-1	/* any user/group */
+
+/*
+ * overflow notification message.
+ * structure shared with user level
+ */
+struct pfarg_ovfl_msg {
+	__u32 		msg_type;	/* message type: PFM_MSG_OVFL */
+	__u32		msg_ovfl_pid;	/* process id */
+	__u16 		msg_active_set;	/* active set at overflow */
+	__u16 		msg_ovfl_cpu;	/* cpu of PMU interrupt */
+	__u32		msg_ovfl_tid;	/* thread id */
+	__u64		msg_ovfl_ip;    /* IP on PMU intr */
+	__u64		msg_ovfl_pmds[PFM_PMD_BV];/* overflowed PMDs */
+};
+
+#define PFM_MSG_OVFL	1	/* an overflow happened */
+#define PFM_MSG_END	2	/* task to which context was attached ended */
+
+/*
+ * generic notification message (union).
+ * union shared with user level
+ */
+union pfarg_msg {
+	__u32	type;
+	struct pfarg_ovfl_msg pfm_ovfl_msg;
+};
+
+/*
+ * perfmon version number
+ */
+#define PFM_VERSION_MAJ		 2U
+#define PFM_VERSION_MIN		 82U
+#define PFM_VERSION		 (((PFM_VERSION_MAJ&0xffff)<<16)|\
+				  (PFM_VERSION_MIN & 0xffff))
+#define PFM_VERSION_MAJOR(x)	 (((x)>>16) & 0xffff)
+#define PFM_VERSION_MINOR(x)	 ((x) & 0xffff)
+
+#endif /* __LINUX_PERFMON_H__ */
Index: linux-2.6.31-master/include/linux/perfmon_dfl_smpl.h
===================================================================
--- /dev/null
+++ linux-2.6.31-master/include/linux/perfmon_dfl_smpl.h
@@ -0,0 +1,78 @@
+/*
+ * Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+ *               Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file implements the new dfl sampling buffer format
+ * for perfmon2 subsystem.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#ifndef __PERFMON_DFL_SMPL_H__
+#define __PERFMON_DFL_SMPL_H__ 1
+
+/*
+ * format specific parameters (passed at context creation)
+ */
+struct pfm_dfl_smpl_arg {
+	__u64 buf_size;		/* size of the buffer in bytes */
+	__u32 buf_flags;	/* buffer specific flags */
+	__u32 reserved1;	/* for future use */
+	__u64 reserved[6];	/* for future use */
+};
+
+/*
+ * This header is at the beginning of the sampling buffer returned to the user.
+ * It is directly followed by the first record.
+ */
+struct pfm_dfl_smpl_hdr {
+	__u64 hdr_count;	/* how many valid entries */
+	__u64 hdr_cur_offs;	/* current offset from top of buffer */
+	__u64 hdr_overflows;	/* #overflows for buffer */
+	__u64 hdr_buf_size;	/* bytes in the buffer */
+	__u64 hdr_min_buf_space;/* minimal buffer size (internal use) */
+	__u32 hdr_version;	/* smpl format version */
+	__u32 hdr_buf_flags;	/* copy of buf_flags */
+	__u64 hdr_reserved[10];	/* for future use */
+};
+
+/*
+ * Entry header in the sampling buffer.  The header is directly followed
+ * with the values of the PMD registers of interest saved in increasing
+ * index order: PMD4, PMD5, and so on. How many PMDs are present depends
+ * on how the session was programmed.
+ *
+ * In the case where multiple counters overflow at the same time, multiple
+ * entries are written consecutively.
+ *
+ * last_reset_value member indicates the initial value of the overflowed PMD.
+ */
+struct pfm_dfl_smpl_entry {
+	__u32	pid;		/* thread id (for NPTL, this is gettid()) */
+	__u16	ovfl_pmd;	/* index of overflowed PMD for this sample */
+	__u16	reserved;	/* for future use */
+	__u64	last_reset_val;	/* initial value of overflowed PMD */
+	__u64	ip;		/* where did the overflow intr happened */
+	__u64	tstamp;		/* overflow timetamp */
+	__u16	cpu;		/* cpu on which the overfow occurred */
+	__u16	set;		/* event set active when overflow ocurred */
+	__u32	tgid;		/* thread group id (getpid() for NPTL) */
+};
+
+#define PFM_DFL_SMPL_VERSION_MAJ 1U
+#define PFM_DFL_SMPL_VERSION_MIN 0U
+#define PFM_DFL_SMPL_VERSION (((PFM_DFL_SMPL_VERSION_MAJ&0xffff)<<16)|\
+				(PFM_DFL_SMPL_VERSION_MIN & 0xffff))
+
+#endif /* __PERFMON_DFL_SMPL_H__ */
Index: linux-2.6.31-master/include/linux/perfmon_fmt.h
===================================================================
--- /dev/null
+++ linux-2.6.31-master/include/linux/perfmon_fmt.h
@@ -0,0 +1,83 @@
+/*
+ * Copyright (c) 2001-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * Interface for custom sampling buffer format modules
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#ifndef __PERFMON_FMT_H__
+#define __PERFMON_FMT_H__ 1
+
+#include <linux/kobject.h>
+
+typedef int (*fmt_validate_t)(u32 flags, u16 npmds, void *arg);
+typedef	int (*fmt_getsize_t)(u32 flags, void *arg, size_t *size);
+typedef int (*fmt_init_t)(struct pfm_context *ctx, void *buf, u32 flags,
+			  u16 nmpds, void *arg);
+typedef int (*fmt_restart_t)(struct pfm_context *ctx, u32 *ovfl_ctrl);
+typedef int (*fmt_exit_t)(void *buf);
+typedef int (*fmt_handler_t)(struct pfm_context *ctx,
+			     unsigned long ip, u64 stamp, void *data);
+typedef int (*fmt_load_t)(struct pfm_context *ctx);
+typedef int (*fmt_unload_t)(struct pfm_context *ctx);
+
+typedef int (*fmt_stop_t)(struct pfm_context *ctx);
+typedef int (*fmt_start_t)(struct pfm_context *ctx);
+
+struct pfm_smpl_fmt {
+	char		*fmt_name;	/* name of the format (required) */
+	size_t		fmt_arg_size;	/* size of fmt args for ctx create */
+	u32		fmt_flags;	/* format specific flags */
+	u32		fmt_version;	/* format version number */
+
+	fmt_validate_t	fmt_validate;	/* validate context flags */
+	fmt_getsize_t	fmt_getsize;	/* get size for sampling buffer */
+	fmt_init_t	fmt_init;	/* initialize buffer area */
+	fmt_handler_t	fmt_handler;	/* overflow handler (required) */
+	fmt_restart_t	fmt_restart;	/* restart after notification  */
+	fmt_exit_t	fmt_exit;	/* context termination */
+	fmt_load_t	fmt_load;	/* load context */
+	fmt_unload_t	fmt_unload;	/* unload context */
+	fmt_start_t	fmt_start;	/* start monitoring */
+	fmt_stop_t	fmt_stop;	/* stop monitoring */
+
+	struct list_head fmt_list;	/* internal use only */
+
+	struct kobject	kobj;		/* sysfs internal use only */
+	struct module	*owner;		/* pointer to module owner */
+	u32		fmt_qdepth;	/* Max notify queue depth (required) */
+};
+#define to_smpl_fmt(n) container_of(n, struct pfm_smpl_fmt, kobj)
+
+#define PFM_FMTFL_IS_BUILTIN	0x1	/* fmt is compiled in */
+/*
+ * we need to know whether the format is builtin or compiled
+ * as a module
+ */
+#ifdef MODULE
+#define PFM_FMT_BUILTIN_FLAG	0	/* not built as a module */
+#else
+#define PFM_FMT_BUILTIN_FLAG	PFM_PMUFL_IS_BUILTIN /* built as a module */
+#endif
+
+int pfm_fmt_register(struct pfm_smpl_fmt *fmt);
+int pfm_fmt_unregister(struct pfm_smpl_fmt *fmt);
+void pfm_sysfs_builtin_fmt_add(void);
+
+int  pfm_sysfs_add_fmt(struct pfm_smpl_fmt *fmt);
+void pfm_sysfs_remove_fmt(struct pfm_smpl_fmt *fmt);
+
+#endif /* __PERFMON_FMT_H__ */
Index: linux-2.6.31-master/include/linux/perfmon_kern.h
===================================================================
--- /dev/null
+++ linux-2.6.31-master/include/linux/perfmon_kern.h
@@ -0,0 +1,543 @@
+/*
+ * Copyright (c) 2001-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+
+#ifndef __LINUX_PERFMON_KERN_H__
+#define __LINUX_PERFMON_KERN_H__
+/*
+ * This file contains all the definitions of data structures, variables, macros
+ * that are to be shared between generic code and arch-specific code
+ *
+ * For generic only definitions, use perfmon/perfmon_priv.h
+ */
+#ifdef CONFIG_PERFMON
+
+#include <linux/file.h>
+#include <linux/sched.h>
+#include <linux/perfmon.h>
+
+/*
+ * system adminstrator configuration controls available via
+ * the /sys/kernel/perfmon interface
+ */
+struct pfm_controls {
+	u32	debug;		/* debugging control bitmask */
+	gid_t	sys_group;	/* gid to create a syswide context */
+	gid_t	task_group;	/* gid to create a per-task context */
+	u32	flags;		/* control flags (see below) */
+	size_t	arg_mem_max;	/* maximum vector argument size */
+	size_t	smpl_buffer_mem_max; /* max buf mem, -1 for infinity */
+};
+extern struct pfm_controls pfm_controls;
+
+/*
+ * control flags
+ */
+#define PFM_CTRL_FL_RW_EXPERT	0x1 /* bypass reserved fields on read/write */
+
+/*
+ * software PMD
+ */
+struct pfm_pmd {
+	u64 value;			/* 64-bit value */
+	u64 lval;			/* last reset value */
+	u64 ovflsw_thres;		/* #ovfls left before switch */
+	u64 long_reset;			/* long reset value on overflow */
+	u64 short_reset;		/* short reset value on overflow */
+	u64 reset_pmds[PFM_PMD_BV];	/* pmds to reset on overflow */
+	u64 smpl_pmds[PFM_PMD_BV];	/* pmds to record on overflow */
+	u64 mask;			/* range mask for random value */
+	u64 ovflsw_ref_thres;		/* #ovfls before next set */
+	u64 eventid;			/* opaque event identifier */
+	u32 flags;			/* notify/do not notify */
+};
+
+/*
+ * event_set: encapsulates the full PMU state
+ */
+struct pfm_event_set {
+	struct list_head list;		/* ordered chain of sets */
+	u16 id;				/* set identification */
+	u16 pad0;			/* paddding */
+	u32 flags;			/* public flags */
+	u32 priv_flags;			/* private flags (see below) */
+	u64 runs;			/* # of activations */
+	u32 npend_ovfls;		/* number of pending PMD overflow */
+	u32 pad2;			/* padding */
+	u64 used_pmds[PFM_PMD_BV];	/* used PMDs */
+	u64 povfl_pmds[PFM_PMD_BV];	/* pending overflowed PMDs */
+	u64 ovfl_pmds[PFM_PMD_BV];	/* last overflowed PMDs */
+	u64 reset_pmds[PFM_PMD_BV];	/* PMDs to reset after overflow */
+	u64 ovfl_notify[PFM_PMD_BV];	/* notify on overflow */
+	u64 used_pmcs[PFM_PMC_BV];	/* used PMCs */
+	u64 pmcs[PFM_MAX_PMCS];		/* PMC values */
+
+	struct pfm_pmd pmds[PFM_MAX_PMDS];
+
+	ktime_t hrtimer_exp;		/* switch timeout reference */
+	ktime_t hrtimer_rem;		/* per-thread remainder timeout */
+
+	u64 duration_start;		/* start time in ns */
+	u64 duration;			/* total active ns */
+};
+
+/*
+ * common private event set flags (priv_flags)
+ *
+ * upper 16 bits: for arch-specific use
+ * lower 16 bits: for common use
+ */
+#define PFM_SETFL_PRIV_MOD_PMDS 0x1 /* PMD register(s) modified */
+#define PFM_SETFL_PRIV_MOD_PMCS 0x2 /* PMC register(s) modified */
+#define PFM_SETFL_PRIV_SWITCH	0x4 /* must switch set on restart */
+#define PFM_SETFL_PRIV_MOD_BOTH	(PFM_SETFL_PRIV_MOD_PMDS \
+				| PFM_SETFL_PRIV_MOD_PMCS)
+
+/*
+ * context flags
+ */
+struct pfm_context_flags {
+	unsigned int block:1;		/* task blocks on user notifications */
+	unsigned int system:1;		/* do system wide monitoring */
+	unsigned int no_msg:1;		/* no message sent on overflow */
+	unsigned int switch_ovfl:1;	/* switch set on counter ovfl */
+	unsigned int switch_time:1;	/* switch set on timeout */
+	unsigned int started:1;		/* pfm_start() issued */
+	unsigned int work_type:2;	/* type of work for pfm_handle_work */
+	unsigned int mmap_nlock:1;	/* no lock in pfm_release_buf_space */
+	unsigned int ia64_v20_compat:1;	/* context is IA-64 v2.0 mode */
+	unsigned int can_restart:8;	/* allowed to issue a PFM_RESTART */
+	unsigned int reset_count:8;	/* number of pending resets */
+	unsigned int is_self:1;		/* per-thread and self-montoring */
+	unsigned int reserved:5;	/* for future use */
+};
+
+/*
+ * values for work_type (TIF_PERFMON_WORK must be set)
+ */
+#define PFM_WORK_NONE	0	/* nothing to do */
+#define PFM_WORK_RESET	1	/* reset overflowed counters */
+#define PFM_WORK_BLOCK	2	/* block current thread */
+#define PFM_WORK_ZOMBIE	3	/* cleanup zombie context */
+
+/*
+ * overflow description argument passed to sampling format
+ */
+struct pfm_ovfl_arg {
+	u16 ovfl_pmd;		/* index of overflowed PMD  */
+	u16 active_set;		/* set active at the time of the overflow */
+	u32 ovfl_ctrl;		/* control flags */
+	u64 pmd_last_reset;	/* last reset value of overflowed PMD */
+	u64 smpl_pmds_values[PFM_MAX_PMDS]; /* values of other PMDs */
+	u64 pmd_eventid;	/* eventid associated with PMD */
+	u16 num_smpl_pmds;	/* number of PMDS in smpl_pmd_values */
+};
+/*
+ * depth of message queue
+ *
+ * Depth cannot be bigger than 255 (see reset_count)
+ */
+#define PFM_MSGS_ORDER		3 /* log2(number of messages) */
+#define PFM_MSGS_COUNT		(1<<PFM_MSGS_ORDER) /* number of messages */
+#define PFM_MSGQ_MASK		(PFM_MSGS_COUNT-1)
+
+/*
+ * perfmon context state
+ */
+#define PFM_CTX_UNLOADED	1 /* context is not loaded onto any task */
+#define PFM_CTX_LOADED		2 /* context is loaded onto a task */
+#define PFM_CTX_MASKED		3 /* context is loaded, monitoring is masked */
+#define PFM_CTX_ZOMBIE		4 /* context lost owner but still attached */
+
+/*
+ * registers description
+ */
+struct pfm_regdesc {
+	u64 pmcs[PFM_PMC_BV];		/* available PMC */
+	u64 pmds[PFM_PMD_BV];		/* available PMD */
+	u64 rw_pmds[PFM_PMD_BV];	/* available RW PMD */
+	u64 intr_pmds[PFM_PMD_BV];	/* PMD generating intr */
+	u64 cnt_pmds[PFM_PMD_BV];	/* PMD counters */
+	u16 max_pmc;			/* highest+1 avail PMC */
+	u16 max_pmd;			/* highest+1 avail PMD */
+	u16 max_rw_pmd;			/* highest+1 avail RW PMD */
+	u16 first_intr_pmd;		/* first intr PMD */
+	u16 max_intr_pmd;		/* highest+1 intr PMD */
+	u16 num_rw_pmd;			/* number of avail RW PMD */
+	u16 num_pmcs;			/* number of logical PMCS */
+	u16 num_pmds;			/* number of logical PMDS */
+	u16 num_counters;		/* number of counting PMD */
+};
+
+/*
+ * context: contains all the state of a session
+ */
+struct pfm_context {
+	spinlock_t		lock;		/* context protection */
+
+	struct pfm_context_flags flags;
+	u32			state;		/* current state */
+	struct task_struct 	*task;		/* attached task */
+
+	struct completion       restart_complete;/* block on notification */
+	u64 			last_act;	/* last activation */
+	u32			last_cpu;   	/* last CPU used (SMP only) */
+	u32			cpu;		/* cpu bound to context */
+
+	struct pfm_smpl_fmt	*smpl_fmt;	/* sampling format callbacks */
+	void			*smpl_addr;	/* user smpl buffer base */
+	size_t			smpl_size;	/* user smpl buffer size */
+	void			*smpl_real_addr;/* actual smpl buffer base */
+	size_t			smpl_real_size; /* actual smpl buffer size */
+
+	wait_queue_head_t 	msgq_wait;	/* pfm_read() wait queue */
+
+	union pfarg_msg		msgq[PFM_MSGS_COUNT];
+	int			msgq_head;
+	int			msgq_tail;
+
+	struct fasync_struct	*async_queue;	/* async notification */
+
+	struct pfm_event_set	*active_set;	/* active set */
+	struct list_head	set_list;	/* ordered list of sets */
+
+	struct pfm_regdesc	regs;		/* registers available to context */
+
+	/*
+	 * save stack space by allocating temporary variables for
+	 * pfm_overflow_handler() in pfm_context
+	 */
+	struct pfm_ovfl_arg 	ovfl_arg;
+	u64			tmp_ovfl_notify[PFM_PMD_BV];
+};
+
+/*
+ * ovfl_ctrl bitmask (used by interrupt handler)
+ */
+#define PFM_OVFL_CTRL_NOTIFY	0x1	/* notify user */
+#define PFM_OVFL_CTRL_RESET	0x2	/* reset overflowed pmds */
+#define PFM_OVFL_CTRL_MASK	0x4	/* mask monitoring */
+#define PFM_OVFL_CTRL_SWITCH	0x8	/* switch sets */
+
+/*
+ * logging
+ */
+#define PFM_ERR(f, x...)  pr_err(f "\n", ## x)
+#define PFM_WARN(f, x...) pr_warning(f "\n", ## x)
+#define PFM_LOG(f, x...)  pr_notice(f "\n", ## x)
+#define PFM_INFO(f, x...) pr_info(f "\n", ## x)
+
+/*
+ * debugging
+ *
+ * Printk rate limiting is enforced to avoid getting flooded with too many
+ * error messages on the console (which could render the machine unresponsive).
+ * To get full debug output (turn off ratelimit):
+ * 	$ echo 0 >/proc/sys/kernel/printk_ratelimit
+ *
+ * debug is a bitmask where bits are defined as follows:
+ * bit  0: enable non-interrupt code degbug messages
+ * bit  1: enable interrupt code debug messages
+ */
+#ifdef CONFIG_PERFMON_DEBUG
+#define _PFM_DBG(lm, f, x...) \
+			pr_debug("%s.%d: CPU%d [%d]: " f "\n", \
+			       __func__, __LINE__, \
+			       smp_processor_id(), current->pid , ## x)
+
+#define PFM_DBG(f, x...) _PFM_DBG(0x1, f, ##x)
+#define PFM_DBG_ovfl(f, x...) _PFM_DBG(0x2, f, ## x)
+#else
+#define PFM_DBG(f, x...)	do {} while (0)
+#define PFM_DBG_ovfl(f, x...)	do {} while (0)
+#endif
+
+extern struct pfm_pmu_config  *pfm_pmu_conf;
+extern int perfmon_disabled;
+
+static inline struct pfm_arch_context *pfm_ctx_arch(struct pfm_context *c)
+{
+	return (struct pfm_arch_context *)(c+1);
+}
+
+int  pfm_get_args(void __user *ureq, size_t sz, size_t lsz, void *laddr,
+		  void **req, void **to_free);
+
+int pfm_get_smpl_arg(char __user *fmt_uname, void __user *uaddr, size_t usize,
+		     void **arg, struct pfm_smpl_fmt **fmt);
+
+int __pfm_write_pmcs(struct pfm_context *ctx, struct pfarg_pmc *req,
+		     int count);
+int __pfm_write_pmds(struct pfm_context *ctx, struct pfarg_pmd *req, int count,
+		     int compat);
+int __pfm_read_pmds(struct pfm_context *ctx, struct pfarg_pmd *req, int count);
+
+int __pfm_load_context(struct pfm_context *ctx, struct pfarg_load *req,
+		       struct task_struct *task);
+int __pfm_unload_context(struct pfm_context *ctx, int *can_release);
+
+int __pfm_stop(struct pfm_context *ctx, int *release_info);
+int  __pfm_restart(struct pfm_context *ctx, int *unblock);
+int __pfm_start(struct pfm_context *ctx, struct pfarg_start *start);
+
+void pfm_free_context(struct pfm_context *ctx);
+
+void pfm_smpl_buf_space_release(struct pfm_context *ctx, size_t size);
+
+int pfm_check_task_state(struct pfm_context *ctx, int check_mask,
+			 unsigned long *flags, void **resume);
+/*
+ * check_mask bitmask values for pfm_check_task_state()
+ */
+#define PFM_CMD_STOPPED		0x01	/* command needs thread stopped */
+#define PFM_CMD_UNLOADED	0x02	/* command needs ctx unloaded */
+#define PFM_CMD_UNLOAD		0x04	/* command is unload */
+
+int __pfm_create_context(struct pfarg_ctx *req,
+			 struct pfm_smpl_fmt *fmt,
+			 void *fmt_arg,
+			 int mode,
+			 struct pfm_context **new_ctx);
+
+struct pfm_event_set *pfm_find_set(struct pfm_context *ctx, u16 set_id,
+				   int alloc);
+
+int pfm_pmu_conf_get(int autoload);
+void pfm_pmu_conf_put(void);
+
+int pfm_session_allcpus_acquire(void);
+void pfm_session_allcpus_release(void);
+
+int pfm_smpl_buf_alloc(struct pfm_context *ctx, size_t rsize);
+void pfm_smpl_buf_free(struct pfm_context *ctx);
+
+struct pfm_smpl_fmt *pfm_smpl_fmt_get(char *name);
+void pfm_smpl_fmt_put(struct pfm_smpl_fmt *fmt);
+
+void pfm_interrupt_handler(unsigned long iip, struct pt_regs *regs);
+
+void pfm_resume_task(struct task_struct *t, void *data);
+
+#include <linux/perfmon_pmu.h>
+#include <linux/perfmon_fmt.h>
+
+extern const struct file_operations pfm_file_ops;
+/*
+ * upper limit for count in calls that take vector arguments. This is used
+ * to prevent for multiplication overflow when we compute actual storage size
+ */
+#define PFM_MAX_ARG_COUNT(m) (INT_MAX/sizeof(*(m)))
+
+#define cast_ulp(_x) ((unsigned long *)_x)
+
+#define PFM_NORMAL      0
+#define PFM_COMPAT      1
+
+void __pfm_exit_thread(void);
+void pfm_ctxsw_in(struct task_struct *prev, struct task_struct *next);
+void pfm_ctxsw_out(struct task_struct *prev, struct task_struct *next);
+void pfm_handle_work(struct pt_regs *regs);
+void __pfm_init_percpu(void *dummy);
+void pfm_save_pmds(struct pfm_context *ctx, struct pfm_event_set *set);
+
+static inline void pfm_exit_thread(void)
+{
+	if (current->pfm_context)
+		__pfm_exit_thread();
+}
+
+/*
+ * include arch-specific kernel level definitions
+ */
+#include <asm/perfmon_kern.h>
+
+static inline void pfm_copy_thread(struct task_struct *task)
+{
+	/*
+	 * context or perfmon TIF state  is NEVER inherited
+	 * in child task. Holds for per-thread and system-wide
+	 */
+	task->pfm_context = NULL;
+	clear_tsk_thread_flag(task, TIF_PERFMON_CTXSW);
+	clear_tsk_thread_flag(task, TIF_PERFMON_WORK);
+	pfm_arch_disarm_handle_work(task);
+}
+
+
+/*
+ * read a single PMD register.
+ *
+ * virtual PMD registers have special handler.
+ * Depends on definitions in asm/perfmon_kern.h
+ */
+static inline u64 pfm_read_pmd(struct pfm_context *ctx, unsigned int cnum)
+{
+	if (unlikely(pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_V))
+		return pfm_pmu_conf->pmd_sread(ctx, cnum);
+
+	return pfm_arch_read_pmd(ctx, cnum);
+}
+/*
+ * write a single PMD register.
+ *
+ * virtual PMD registers have special handler.
+ * Depends on definitions in asm/perfmon_kern.h
+ */
+static inline void pfm_write_pmd(struct pfm_context *ctx, unsigned int cnum,
+				 u64 value)
+{
+	/*
+	 * PMD writes are ignored for read-only registers
+	 */
+	if (pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_RO)
+		return;
+
+	if (pfm_pmu_conf->pmd_desc[cnum].type & PFM_REG_V) {
+		pfm_pmu_conf->pmd_swrite(ctx, cnum, value);
+		return;
+	}
+	/*
+	 * clear unimplemented bits
+	 */
+	value &= ~pfm_pmu_conf->pmd_desc[cnum].rsvd_msk;
+
+	pfm_arch_write_pmd(ctx, cnum, value);
+}
+
+void __pfm_init_percpu(void *dummy);
+
+static inline void pfm_init_percpu(void)
+{
+	__pfm_init_percpu(NULL);
+}
+
+/*
+ * pfm statistics are available via debugfs
+ * and perfmon subdir.
+ *
+ * When adding/removing new stats, make sure you also
+ * update the name table in perfmon_debugfs.c
+ */
+enum pfm_stats_names {
+	PFM_ST_ovfl_intr_all_count = 0,
+	PFM_ST_ovfl_intr_ns,
+	PFM_ST_ovfl_intr_spurious_count,
+	PFM_ST_ovfl_intr_replay_count,
+	PFM_ST_ovfl_intr_regular_count,
+	PFM_ST_handle_work_count,
+	PFM_ST_ovfl_notify_count,
+	PFM_ST_reset_pmds_count,
+	PFM_ST_pfm_restart_count,
+	PFM_ST_fmt_handler_calls,
+	PFM_ST_fmt_handler_ns,
+	PFM_ST_set_switch_count,
+	PFM_ST_set_switch_ns,
+	PFM_ST_set_switch_exp,
+	PFM_ST_ctxswin_count,
+	PFM_ST_ctxswin_ns,
+	PFM_ST_handle_timeout_count,
+	PFM_ST_ovfl_intr_nmi_count,
+	PFM_ST_ctxswout_count,
+	PFM_ST_ctxswout_ns,
+	PFM_ST_LAST	/* last entry marked */
+};
+#define PFM_NUM_STATS PFM_ST_LAST
+
+struct pfm_stats {
+	u64 v[PFM_NUM_STATS];
+	struct dentry *dirs[PFM_NUM_STATS];
+	struct dentry *cpu_dir;
+	char cpu_name[8];
+};
+
+#ifdef CONFIG_PERFMON_DEBUG_FS
+#define pfm_stats_get(x)  __get_cpu_var(pfm_stats).v[PFM_ST_##x]
+#define pfm_stats_inc(x)  __get_cpu_var(pfm_stats).v[PFM_ST_##x]++
+#define pfm_stats_add(x, y)  __get_cpu_var(pfm_stats).v[PFM_ST_##x] += (y)
+void pfm_reset_stats(int cpu);
+#else
+#define pfm_stats_get(x)
+#define pfm_stats_inc(x)
+#define pfm_stats_add(x, y)
+static inline void pfm_reset_stats(int cpu)
+{}
+#endif
+
+
+
+DECLARE_PER_CPU(struct pfm_context *, pmu_ctx);
+DECLARE_PER_CPU(struct pfm_stats, pfm_stats);
+DECLARE_PER_CPU(struct task_struct *, pmu_owner);
+
+void pfm_cpu_disable(void);
+
+
+/*
+ * max vector argument elements for local storage (no kmalloc/kfree)
+ * The PFM_ARCH_PM*_ARG should be defined in perfmon_kern.h.
+ * If not, default (conservative) values are used
+ */
+#ifndef PFM_ARCH_PMC_STK_ARG
+#define PFM_ARCH_PMC_STK_ARG	1
+#endif
+
+#ifndef PFM_ARCH_PMD_STK_ARG
+#define PFM_ARCH_PMD_STK_ARG	1
+#endif
+
+#define PFM_PMC_STK_ARG	PFM_ARCH_PMC_STK_ARG
+#define PFM_PMD_STK_ARG	PFM_ARCH_PMD_STK_ARG
+
+#else /* !CONFIG_PERFMON */
+
+
+/*
+ * perfmon hooks are nops when CONFIG_PERFMON is undefined
+ */
+static inline void pfm_cpu_disable(void)
+{}
+
+static inline void pfm_exit_thread(void)
+{}
+
+static inline void pfm_handle_work(struct pt_regs *regs)
+{}
+
+static inline void pfm_copy_thread(struct task_struct *t)
+{}
+
+static inline void pfm_ctxsw_in(struct task_struct *p, struct task_struct *n)
+{}
+
+static inline void pfm_ctxsw_out(struct task_struct *p, struct task_struct *n)
+{}
+
+static inline void pfm_session_allcpus_release(void)
+{}
+
+static inline int pfm_session_allcpus_acquire(void)
+{
+	return 0;
+}
+
+static inline void pfm_init_percpu(void)
+{}
+
+#endif /* CONFIG_PERFMON */
+
+#endif /* __LINUX_PERFMON_KERN_H__ */
Index: linux-2.6.31-master/include/linux/perfmon_pmu.h
===================================================================
--- /dev/null
+++ linux-2.6.31-master/include/linux/perfmon_pmu.h
@@ -0,0 +1,193 @@
+/*
+ * Copyright (c) 2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * Interface for PMU description modules
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#ifndef __PERFMON_PMU_H__
+#define __PERFMON_PMU_H__ 1
+
+/*
+ * generic information about a PMC or PMD register
+ *
+ * Dependency bitmasks:
+ *  They are used to allow lazy save/restore in the context switch
+ *  code. To avoid picking up stale configuration from a previous
+ *  thread. Usng the bitmask, the generic read/write routines can
+ *  ensure that all registers needed to support the measurement are
+ *  restored properly on context switch in.
+ */
+struct pfm_regmap_desc {
+	u16  type;		/* role of the register */
+	u16  reserved1;		/* for future use */
+	u32  reserved2;		/* for future use */
+	u64  dfl_val;		/* power-on default value (quiescent) */
+	u64  rsvd_msk;		/* reserved bits: 1 means reserved */
+	u64  no_emul64_msk;	/* bits to clear for PFM_REGFL_NO_EMUL64 */
+	unsigned long hw_addr;	/* HW register address or index */
+	struct kobject	kobj;	/* for internal use only */
+	char *desc;		/* HW register description string */
+	u64 dep_pmcs[PFM_PMC_BV];/* depending PMC registers */
+};
+#define to_reg(n) container_of(n, struct pfm_regmap_desc, kobj)
+
+/*
+ * pfm_reg_desc helper macros
+ */
+#define PMC_D(t, d, v, r, n, h) \
+	{ .type = t,          \
+	  .desc = d,          \
+	  .dfl_val = v,       \
+	  .rsvd_msk = r,      \
+	  .no_emul64_msk = n, \
+	  .hw_addr = h	      \
+	}
+
+#define PMD_D(t, d, h)        \
+	{ .type = t,          \
+	  .desc = d,          \
+	  .rsvd_msk = 0,      \
+	  .no_emul64_msk = 0, \
+	  .hw_addr = h	      \
+	}
+
+#define PMD_DR(t, d, h, r)    \
+	{ .type = t,          \
+	  .desc = d,          \
+	  .rsvd_msk = r,      \
+	  .no_emul64_msk = 0, \
+	  .hw_addr = h	      \
+	}
+
+#define PMX_NA \
+	{ .type = PFM_REG_NA }
+
+#define PMD_DP(t, d, h, p)    \
+	{ .type = t,          \
+	  .desc = d,          \
+	  .rsvd_msk = 0,      \
+	  .no_emul64_msk = 0, \
+	  .dep_pmcs[0] = p,   \
+	  .hw_addr = h	      \
+	}
+
+/*
+ * type of a PMU register (16-bit bitmask) for use with pfm_reg_desc.type
+ */
+#define PFM_REG_NA	0x00  /* not avail. (not impl.,no access) must be 0 */
+#define PFM_REG_I	0x01  /* PMC/PMD: implemented */
+#define PFM_REG_WC	0x02  /* PMC: has write_checker */
+#define PFM_REG_C64	0x04  /* PMD: 64-bit virtualization */
+#define PFM_REG_RO	0x08  /* PMD: read-only (writes ignored) */
+#define PFM_REG_V	0x10  /* PMD: virtual reg */
+#define PFM_REG_INTR	0x20  /* PMD: register can generate interrupt */
+#define PFM_REG_SYS	0x40  /* PMC/PMD: register is for system-wide only */
+#define PFM_REG_THR	0x80  /* PMC/PMD: register is for per-thread only */
+#define PFM_REG_NO64	0x100 /* PMC: supports PFM_REGFL_NO_EMUL64 */
+
+/*
+ * define some shortcuts for common types
+ */
+#define PFM_REG_W	(PFM_REG_WC|PFM_REG_I)
+#define PFM_REG_W64	(PFM_REG_WC|PFM_REG_NO64|PFM_REG_I)
+#define PFM_REG_C	(PFM_REG_C64|PFM_REG_INTR|PFM_REG_I)
+#define PFM_REG_I64	(PFM_REG_NO64|PFM_REG_I)
+#define PFM_REG_IRO	(PFM_REG_I|PFM_REG_RO)
+
+typedef int (*pfm_pmc_check_t)(struct pfm_context *ctx,
+			       struct pfm_event_set *set,
+			       struct pfarg_pmc *req);
+
+typedef int (*pfm_pmd_check_t)(struct pfm_context *ctx,
+			       struct pfm_event_set *set,
+			       struct pfarg_pmd *req);
+
+
+typedef u64 (*pfm_sread_t)(struct pfm_context *ctx, unsigned int cnum);
+typedef void (*pfm_swrite_t)(struct pfm_context *ctx, unsigned int cnum, u64 val);
+typedef void (*pfm_hotplug_t)(unsigned long action, unsigned int cpu);
+/*
+ * structure used by pmu description modules
+ *
+ * probe_pmu() routine return value:
+ * 	- 1 means recognized PMU
+ * 	- 0 means not recognized PMU
+ */
+struct pfm_pmu_config {
+	char *pmu_name;				/* PMU family name */
+	char *version;				/* config module version */
+
+	int counter_width;			/* width of hardware counter */
+
+	struct pfm_regmap_desc	*pmc_desc;	/* PMC register descriptions */
+	struct pfm_regmap_desc	*pmd_desc;	/* PMD register descriptions */
+
+	pfm_pmc_check_t		pmc_write_check;/* write checker (optional) */
+	pfm_pmd_check_t		pmd_write_check;/* write checker (optional) */
+	pfm_pmd_check_t		pmd_read_check;	/* read checker (optional) */
+
+	pfm_sread_t		pmd_sread;	/* virtual pmd read */
+	pfm_swrite_t		pmd_swrite;	/* virtual pmd write */
+	pfm_hotplug_t		hotplug_handler;/* handle CPU hotplug event */
+
+	int             	(*probe_pmu)(void);/* probe PMU routine */
+
+	u16			num_pmc_entries;/* #entries in pmc_desc */
+	u16			num_pmd_entries;/* #entries in pmd_desc */
+
+	void			*pmu_info;	/* model-specific infos */
+	u32			flags;		/* set of flags */
+
+	struct module		*owner;		/* pointer to module struct */
+
+	/*
+	 * fields computed internally, do not set in module
+	 */
+	struct pfm_regdesc	regs_all;	/* regs available to all */
+	struct pfm_regdesc	regs_thr;	/* regs avail per-thread */
+	struct pfm_regdesc	regs_sys;	/* regs avail system-wide */
+
+	u64			ovfl_mask;	/* overflow mask */
+};
+
+static inline void *pfm_pmu_info(void)
+{
+	return pfm_pmu_conf->pmu_info;
+}
+
+/*
+ * pfm_pmu_config flags
+ */
+#define PFM_PMUFL_IS_BUILTIN	0x1	/* pmu config is compiled in */
+
+/*
+ * we need to know whether the PMU description is builtin or compiled
+ * as a module
+ */
+#ifdef MODULE
+#define PFM_PMU_BUILTIN_FLAG	0	/* not built as a module */
+#else
+#define PFM_PMU_BUILTIN_FLAG	PFM_PMUFL_IS_BUILTIN /* built as a module */
+#endif
+
+int pfm_pmu_register(struct pfm_pmu_config *cfg);
+void pfm_pmu_unregister(struct pfm_pmu_config *cfg);
+
+int pfm_sysfs_remove_pmu(struct pfm_pmu_config *pmu);
+int pfm_sysfs_add_pmu(struct pfm_pmu_config *pmu);
+
+#endif /* __PERFMON_PMU_H__ */
Index: linux-2.6.31-master/include/linux/sched.h
===================================================================
--- linux-2.6.31-master.orig/include/linux/sched.h
+++ linux-2.6.31-master/include/linux/sched.h
@@ -101,6 +101,7 @@ struct bio;
 struct fs_struct;
 struct bts_context;
 struct perf_event_context;
+struct pfm_context;
 
 /*
  * List of flags we want to share for kernel threads,
@@ -1539,6 +1540,9 @@ struct task_struct {
 	unsigned long trace_recursion;
 #endif /* CONFIG_TRACING */
 	unsigned long stack_start;
+#ifdef CONFIG_PERFMON
+	struct pfm_context *pfm_context;
+#endif
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
Index: linux-2.6.31-master/include/linux/syscalls.h
===================================================================
--- linux-2.6.31-master.orig/include/linux/syscalls.h
+++ linux-2.6.31-master/include/linux/syscalls.h
@@ -29,6 +29,13 @@ struct msqid_ds;
 struct new_utsname;
 struct nfsctl_arg;
 struct __old_kernel_stat;
+struct pfarg_ctx;
+struct pfarg_pmc;
+struct pfarg_pmd;
+struct pfarg_start;
+struct pfarg_load;
+struct pfarg_setinfo;
+struct pfarg_setdesc;
 struct pollfd;
 struct rlimit;
 struct rusage;
@@ -875,6 +882,29 @@ asmlinkage long sys_ppoll(struct pollfd 
 
 int kernel_execve(const char *filename, char *const argv[], char *const envp[]);
 
+asmlinkage long sys_pfm_create_context(struct pfarg_ctx __user *ureq,
+				       void __user *uarg, size_t smpl_size);
+asmlinkage long sys_pfm_write_pmcs(int fd, struct pfarg_pmc __user *ureq,
+				   int count);
+asmlinkage long sys_pfm_write_pmds(int fd, struct pfarg_pmd __user *ureq,
+				   int count);
+asmlinkage long sys_pfm_read_pmds(int fd, struct pfarg_pmd __user *ureq,
+				  int count);
+asmlinkage long sys_pfm_restart(int fd);
+asmlinkage long sys_pfm_stop(int fd);
+asmlinkage long sys_pfm_start(int fd, struct pfarg_start __user *ureq);
+asmlinkage long sys_pfm_load_context(int fd, struct pfarg_load __user *ureq);
+asmlinkage long sys_pfm_unload_context(int fd);
+asmlinkage long sys_pfm_delete_evtsets(int fd,
+				       struct pfarg_setinfo __user *ureq,
+				       int count);
+asmlinkage long sys_pfm_create_evtsets(int fd,
+				       struct pfarg_setdesc __user *ureq,
+				       int count);
+asmlinkage long sys_pfm_getinfo_evtsets(int fd,
+					struct pfarg_setinfo __user *ureq,
+					int count);
+
 
 asmlinkage long sys_perf_event_open(
 		struct perf_event_attr __user *attr_uptr,
Index: linux-2.6.31-master/kernel/sched.c
===================================================================
--- linux-2.6.31-master.orig/kernel/sched.c
+++ linux-2.6.31-master/kernel/sched.c
@@ -71,6 +71,7 @@
 #include <linux/debugfs.h>
 #include <linux/ctype.h>
 #include <linux/ftrace.h>
+#include <linux/perfmon_kern.h>
 
 #include <asm/tlb.h>
 #include <asm/irq_regs.h>
Index: linux-2.6.31-master/kernel/sys_ni.c
===================================================================
--- linux-2.6.31-master.orig/kernel/sys_ni.c
+++ linux-2.6.31-master/kernel/sys_ni.c
@@ -134,6 +134,19 @@ cond_syscall(sys_io_cancel);
 cond_syscall(sys_io_getevents);
 cond_syscall(sys_syslog);
 
+cond_syscall(sys_pfm_create_context);
+cond_syscall(sys_pfm_write_pmcs);
+cond_syscall(sys_pfm_write_pmds);
+cond_syscall(sys_pfm_read_pmds);
+cond_syscall(sys_pfm_restart);
+cond_syscall(sys_pfm_start);
+cond_syscall(sys_pfm_stop);
+cond_syscall(sys_pfm_load_context);
+cond_syscall(sys_pfm_unload_context);
+cond_syscall(sys_pfm_create_evtsets);
+cond_syscall(sys_pfm_delete_evtsets);
+cond_syscall(sys_pfm_getinfo_evtsets);
+
 /* arch-specific weak syscall entries */
 cond_syscall(sys_pciconfig_read);
 cond_syscall(sys_pciconfig_write);
Index: linux-2.6.31-master/perfmon/Makefile
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/Makefile
@@ -0,0 +1,12 @@
+#
+# Copyright (c) 2005-2006 Hewlett-Packard Development Company, L.P.
+# Contributed by Stephane Eranian <eranian@hpl.hp.com>
+#
+obj-y = perfmon_init.o perfmon_rw.o perfmon_res.o           \
+	perfmon_pmu.o perfmon_sysfs.o perfmon_syscalls.o    \
+	perfmon_file.o perfmon_ctxsw.o perfmon_intr.o	    \
+	perfmon_dfl_smpl.o perfmon_sets.o perfmon_hotplug.o \
+	perfmon_msg.o perfmon_smpl.o perfmon_attach.o       \
+	perfmon_activate.o perfmon_ctx.o perfmon_fmt.o
+
+obj-$(CONFIG_PERFMON_DEBUG_FS) +=  perfmon_debugfs.o
Index: linux-2.6.31-master/perfmon/perfmon_activate.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_activate.c
@@ -0,0 +1,276 @@
+/*
+ * perfmon_activate.c: perfmon2 start/stop functions
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+/**
+ * __pfm_start - activate monitoring
+ * @ctx: context to operate on
+ * @start: pfarg_start as passed by user
+ *
+ * When operating in per-thread mode and not self-monitoring, the monitored
+ * thread must be stopped. Activation will be effective next time the thread
+ * is context switched in.
+ *
+ * The pfarg_start argument is optional and may be used to designate
+ * the initial event set to activate. When not provided, the last active
+ * set is used. For the first activation, set0 is used when start is NULL.
+ *
+ * On some architectures, e.g., IA-64, it may be possible to start monitoring
+ * without calling this function under certain conditions (per-thread and self
+ * monitoring). In this case, either set0 or the last active set is used.
+ *
+ * the context is locked and interrupts are disabled.
+ */
+int __pfm_start(struct pfm_context *ctx, struct pfarg_start *start)
+{
+	struct task_struct *task, *owner_task;
+	struct pfm_smpl_fmt *fmt;
+	struct pfm_event_set *new_set, *old_set;
+	int is_self;
+
+	task = ctx->task;
+
+	/*
+	 * UNLOADED: error
+	 * LOADED  : normal start, nop if started unless set is different
+	 * MASKED  : nop or change set when unmasking
+	 * ZOMBIE  : cannot happen
+	 */
+	if (ctx->state == PFM_CTX_UNLOADED)
+		return -EINVAL;
+
+	old_set = new_set = ctx->active_set;
+	fmt = ctx->smpl_fmt;
+
+	/*
+	 * always the case for system-wide
+	 */
+	if (task == NULL)
+		task = current;
+
+	is_self = task == current;
+
+	/*
+	 * argument is provided?
+	 */
+	if (start) {
+		/*
+		 * find the set to load first
+		 */
+		new_set = pfm_find_set(ctx, start->start_set, 0);
+		if (new_set == NULL) {
+			PFM_DBG("event set%u does not exist",
+				start->start_set);
+			return -EINVAL;
+		}
+	}
+
+	PFM_DBG("cur_set=%u req_set=%u", old_set->id, new_set->id);
+
+	/*
+	 * if we need to change the active set we need
+	 * to check if we can access the PMU
+	 */
+	if (new_set != old_set) {
+
+		owner_task = __get_cpu_var(pmu_owner);
+		/*
+		 * system-wide: must run on the right CPU
+		 * per-thread : must be the owner of the PMU context
+		 *
+		 * pfm_switch_sets() returns with monitoring stopped
+		 */
+		if (is_self) {
+			pfm_switch_sets(ctx, new_set, PFM_PMD_RESET_LONG, 1);
+		} else {
+			/*
+			 * In a UP kernel, the PMU may contain the state
+			 * of the task we want to operate on, yet the task
+			 * may be switched out (lazy save). We need to save
+			 * current state (old_set), switch active_set and
+			 * mark it for reload.
+			 */
+			if (owner_task == task)
+				pfm_save_pmds(ctx, old_set);
+			ctx->active_set = new_set;
+			new_set->priv_flags |= PFM_SETFL_PRIV_MOD_BOTH;
+		}
+	}
+
+	/*
+	 * mark as started
+	 * must be done before calling pfm_arch_start()
+	 */
+	ctx->flags.started = 1;
+
+	pfm_arch_start(task, ctx);
+
+	if (fmt && fmt->fmt_start)
+		(*fmt->fmt_start)(ctx);
+
+	/*
+	 * we check whether we had a pending ovfl before restarting.
+	 * If so we need to regenerate the interrupt to make sure we
+	 * keep recorded samples. For non-self monitoring this check
+	 * is done in the pfm_ctxswin_thread() routine.
+	 *
+	 * we check new_set/old_set because pfm_switch_sets() already
+	 * takes care of replaying the pending interrupts
+	 */
+	if (is_self && new_set != old_set && new_set->npend_ovfls) {
+		pfm_arch_resend_irq(ctx);
+		pfm_stats_inc(ovfl_intr_replay_count);
+	}
+
+	/*
+	 * always start with full timeout
+	 */
+	new_set->hrtimer_rem = new_set->hrtimer_exp;
+
+	/*
+	 * activate timeout for system-wide, self-montoring
+	 * Always start with full timeout
+	 * Timeout is at least one tick away, so no risk of
+	 * having hrtimer_start() trying to wakeup softirqd
+	 * and thus causing troubles. This cannot happen anmyway
+	 * because cb_mode = HRTIMER_CB_IRQSAFE_NO_SOFTIRQ
+	 */
+	if (is_self && new_set->flags & PFM_SETFL_TIME_SWITCH) {
+		hrtimer_start(&__get_cpu_var(pfm_hrtimer),
+			      new_set->hrtimer_rem,
+			      HRTIMER_MODE_REL);
+
+		PFM_DBG("set%u started timeout=%lld",
+			new_set->id,
+			(unsigned long long)new_set->hrtimer_rem.tv64);
+	}
+
+	/*
+	 * we restart total duration even if context was
+	 * already started. In that case, counts are simply
+	 * reset.
+	 *
+	 * For per-thread, if not self-monitoring, the statement
+	 * below will have no effect because thread is stopped.
+	 * The field is reset of ctxsw in.
+	 */
+	new_set->duration_start = sched_clock();
+
+	return 0;
+}
+
+/**
+ * __pfm_stop - stop monitoring
+ * @ctx: context to operate on
+ * @release_info: infos for caller (see below)
+ *
+ * When operating in per-thread* mode and when not self-monitoring,
+ * the monitored thread must be stopped.
+ *
+ * the context is locked and interrupts are disabled.
+ *
+ * release_info value upon return:
+ * 	- bit 0 : unused
+ * 	- bit 1 : when set, must cancel hrtimer
+ */
+int __pfm_stop(struct pfm_context *ctx, int *release_info)
+{
+	struct pfm_event_set *set;
+	struct task_struct *task;
+	struct pfm_smpl_fmt *fmt;
+	u64 now;
+	int state;
+
+	*release_info = 0;
+
+	now = sched_clock();
+	state = ctx->state;
+	set = ctx->active_set;
+
+	/*
+	 * context must be attached (zombie cannot happen)
+	 */
+	if (state == PFM_CTX_UNLOADED)
+		return -EINVAL;
+
+	task = ctx->task;
+	fmt = ctx->smpl_fmt;
+
+	PFM_DBG("ctx_task=[%d] ctx_state=%d is_system=%d",
+		task ? task->pid : -1,
+		state,
+		!task);
+
+	/*
+	 * this happens for system-wide context
+	 */
+	if (task == NULL)
+		task = current;
+
+	/*
+	 * compute elapsed time
+	 *
+	 * unless masked, compute elapsed duration, stop timeout
+	 */
+	if (task == current && state == PFM_CTX_LOADED) {
+		/*
+		 * timeout cancel must be deferred until context is
+		 * unlocked to avoid race with pfm_handle_switch_timeout()
+		 */
+		if (set->flags & PFM_SETFL_TIME_SWITCH)
+			*release_info |= 0x2;
+
+		set->duration += now - set->duration_start;
+	}
+
+	pfm_arch_stop(task, ctx);
+
+	ctx->flags.started = 0;
+
+	if (fmt && fmt->fmt_stop)
+		(*fmt->fmt_stop)(ctx);
+
+	/*
+	 * starting now, in-flight PMU interrupt for this context
+	 * are treated as spurious
+	 */
+	return 0;
+}
Index: linux-2.6.31-master/perfmon/perfmon_attach.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_attach.c
@@ -0,0 +1,487 @@
+/*
+ * perfmon_attach.c: perfmon2 load/unload functions
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+/**
+ * __pfm_load_context_sys - attach context to a CPU in system-wide mode
+ * @ctx: context to operate on
+ * @set_id: set to activate first
+ * @cpu: CPU to monitor
+ *
+ * The cpu specified in the pfarg_load.load_pid  argument must be the current
+ * CPU.
+ *
+ * The function must be called with the context locked and interrupts disabled.
+ */
+static int pfm_load_ctx_sys(struct pfm_context *ctx, u16 set_id, u32 cpu)
+{
+	struct pfm_event_set *set;
+	int mycpu;
+	int ret;
+
+	mycpu = smp_processor_id();
+
+	/*
+	 * system-wide: check we are running on the desired CPU
+	 */
+	if (cpu != mycpu) {
+		PFM_DBG("wrong CPU: asking %u but on %u", cpu, mycpu);
+		return -EINVAL;
+	}
+
+	/*
+	 * initialize sets
+	 */
+	set = pfm_prepare_sets(ctx, set_id);
+	if (!set) {
+		PFM_DBG("event set%u does not exist", set_id);
+		return -EINVAL;
+	}
+
+	PFM_DBG("set=%u set_flags=0x%x", set->id, set->flags);
+
+	ctx->cpu = mycpu;
+	ctx->task = NULL;
+	ctx->active_set = set;
+
+	/*
+	 * perform any architecture specific actions
+	 */
+	ret = pfm_arch_load_context(ctx);
+	if (ret)
+		goto error_noload;
+
+	ret = pfm_smpl_buf_load_context(ctx);
+	if (ret)
+		goto error;
+
+	/*
+	 * now reserve the session, before we can proceed with
+	 * actually accessing the PMU hardware
+	 */
+	ret = pfm_session_acquire(1, mycpu);
+	if (ret)
+		goto error_smpl;
+
+
+	/*
+	 * caller must be on monitored CPU to access PMU, thus this is
+	 * a form of self-monitoring
+	 */
+	ctx->flags.is_self = 1;
+
+	set->runs++;
+
+	/*
+	 * load PMD from set
+	 * load PMC from set
+	 */
+	pfm_arch_restore_pmds(ctx, set);
+	pfm_arch_restore_pmcs(ctx, set);
+
+	/*
+	 * set new ownership
+	 */
+	pfm_set_pmu_owner(NULL, ctx);
+
+	/*
+	 * reset pending work
+	 */
+	ctx->flags.work_type = PFM_WORK_NONE;
+	ctx->flags.reset_count = 0;
+
+	/*
+	 * reset message queue
+	 */
+	ctx->msgq_head = ctx->msgq_tail = 0;
+
+	ctx->state = PFM_CTX_LOADED;
+
+	return 0;
+error_smpl:
+	pfm_smpl_buf_unload_context(ctx);
+error:
+	pfm_arch_unload_context(ctx);
+error_noload:
+	return ret;
+}
+
+/**
+ * __pfm_load_context_thread - attach context to a thread
+ * @ctx: context to operate on
+ * @set_id: first set
+ * @task: threadf to attach to
+ *
+ * The function must be called with the context locked and interrupts disabled.
+ */
+static int pfm_load_ctx_thread(struct pfm_context *ctx, u16 set_id,
+			       struct task_struct *task)
+{
+	struct pfm_event_set *set;
+	struct pfm_context *old;
+	int ret;
+
+	PFM_DBG("load_pid=%d set=%u", task->pid, set_id);
+	/*
+	 * per-thread:
+	 *   - task to attach to is checked in sys_pfm_load_context() to avoid
+	 *     locking issues. if found, and not self,  task refcount was
+	 *     incremented.
+	 */
+	old = cmpxchg(&task->pfm_context, NULL, ctx);
+	if (old) {
+		PFM_DBG("load_pid=%d has a context "
+			"old=%p new=%p cur=%p",
+			task->pid,
+			old,
+			ctx,
+			task->pfm_context);
+		return -EEXIST;
+	}
+
+	/*
+	 * initialize sets
+	 */
+	set = pfm_prepare_sets(ctx, set_id);
+	if (!set) {
+		PFM_DBG("event set%u does not exist", set_id);
+		return -EINVAL;
+	}
+
+
+	ctx->task = task;
+	ctx->cpu = -1;
+	ctx->active_set = set;
+
+	/*
+	 * perform any architecture specific actions
+	 */
+	ret = pfm_arch_load_context(ctx);
+	if (ret)
+		goto error_noload;
+
+	ret = pfm_smpl_buf_load_context(ctx);
+	if (ret)
+		goto error;
+	/*
+	 * now reserve the session, before we can proceed with
+	 * actually accessing the PMU hardware
+	 */
+	ret = pfm_session_acquire(0, -1);
+	if (ret)
+		goto error_smpl;
+
+
+	set->runs++;
+	if (ctx->task != current) {
+
+		ctx->flags.is_self = 0;
+
+		/* force a full reload */
+		ctx->last_act = PFM_INVALID_ACTIVATION;
+		ctx->last_cpu = -1;
+		set->priv_flags |= PFM_SETFL_PRIV_MOD_BOTH;
+
+	} else {
+		pfm_check_save_prev_ctx();
+
+		ctx->last_cpu = smp_processor_id();
+		__get_cpu_var(pmu_activation_number)++;
+		ctx->last_act = __get_cpu_var(pmu_activation_number);
+
+		ctx->flags.is_self = 1;
+
+		/*
+		 * load PMD from set
+		 * load PMC from set
+		 */
+		pfm_arch_restore_pmds(ctx, set);
+		pfm_arch_restore_pmcs(ctx, set);
+
+		/*
+		 * set new ownership
+		 */
+		pfm_set_pmu_owner(ctx->task, ctx);
+	}
+	set_tsk_thread_flag(task, TIF_PERFMON_CTXSW);
+
+	/*
+	 * reset pending work
+	 */
+	ctx->flags.work_type = PFM_WORK_NONE;
+	ctx->flags.reset_count = 0;
+
+	/*
+	 * reset message queue
+	 */
+	ctx->msgq_head = ctx->msgq_tail = 0;
+
+	ctx->state = PFM_CTX_LOADED;
+
+	return 0;
+
+error_smpl:
+	pfm_smpl_buf_unload_context(ctx);
+error:
+	pfm_arch_unload_context(ctx);
+	ctx->task = NULL;
+error_noload:
+	/*
+	 * detach context
+	 */
+	task->pfm_context = NULL;
+	return ret;
+}
+
+/**
+ * __pfm_load_context - attach context to a CPU or thread
+ * @ctx: context to operate on
+ * @load: pfarg_load as passed by user
+ * @task: thread to attach to, NULL for system-wide
+ */
+int __pfm_load_context(struct pfm_context *ctx, struct pfarg_load *load,
+		       struct task_struct *task)
+{
+	if (ctx->flags.system)
+		return pfm_load_ctx_sys(ctx, load->load_set, load->load_pid);
+	return pfm_load_ctx_thread(ctx, load->load_set, task);
+}
+
+/**
+ * pfm_update_ovfl_pmds - account for pending ovfls on PMDs
+ * @ctx: context to operate on
+ *
+ * This function is always called after pfm_stop has been issued
+ */
+static void pfm_update_ovfl_pmds(struct pfm_context *ctx)
+{
+	struct pfm_event_set *set;
+	u64 *cnt_pmds;
+	u64 ovfl_mask;
+	u16 num_ovfls, i, first;
+
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+	first = ctx->regs.first_intr_pmd;
+	cnt_pmds = ctx->regs.cnt_pmds;
+
+	/*
+	 * look for pending interrupts and adjust PMD values accordingly
+	 */
+	list_for_each_entry(set, &ctx->set_list, list) {
+
+		if (!set->npend_ovfls)
+			continue;
+
+		num_ovfls = set->npend_ovfls;
+		PFM_DBG("set%u nintrs=%u", set->id, num_ovfls);
+
+		for (i = first; num_ovfls; i++) {
+			if (test_bit(i, cast_ulp(set->povfl_pmds))) {
+				/* only correct value for counters */
+				if (test_bit(i, cast_ulp(cnt_pmds)))
+					set->pmds[i].value += 1 + ovfl_mask;
+				num_ovfls--;
+			}
+			PFM_DBG("pmd%u set=%u val=0x%llx",
+				i,
+				set->id,
+				(unsigned long long)set->pmds[i].value);
+		}
+		/*
+		 * we need to clear to prevent a pfm_getinfo_evtsets() from
+		 * returning stale data even after the context is unloaded
+		 */
+		set->npend_ovfls = 0;
+		bitmap_zero(cast_ulp(set->povfl_pmds), ctx->regs.max_intr_pmd);
+	}
+}
+
+
+/**
+ * __pfm_unload_context - detach context from CPU or thread
+ * @ctx: context to operate on
+ * @release_info: pointer to return info (see below)
+ *
+ * The function must be called with the context locked and interrupts disabled.
+ *
+ * release_info value upon return:
+ * 	- bit 0: when set, must free context
+ * 	- bit 1: when set, must cancel hrtimer
+ */
+int __pfm_unload_context(struct pfm_context *ctx, int *release_info)
+{
+	struct task_struct *task;
+	int ret;
+
+	PFM_DBG("ctx_state=%d task [%d]",
+		ctx->state,
+		ctx->task ? ctx->task->pid : -1);
+
+	*release_info = 0;
+
+	/*
+	 * unload only when necessary
+	 */
+	if (ctx->state == PFM_CTX_UNLOADED)
+		return 0;
+
+	task = ctx->task;
+
+	/*
+	 * stop monitoring
+	 */
+	ret = __pfm_stop(ctx, release_info);
+	if (ret)
+		return ret;
+
+	ctx->state = PFM_CTX_UNLOADED;
+	ctx->flags.can_restart = 0;
+
+	/*
+	 * save active set
+	 * UP:
+	 * 	if not current task and due to lazy, state may
+	 * 	still be live
+	 * for system-wide, guaranteed to run on correct CPU
+	 */
+	if (__get_cpu_var(pmu_ctx) == ctx) {
+		/*
+		 * pending overflows have been saved by pfm_stop()
+		 */
+		pfm_save_pmds(ctx, ctx->active_set);
+		pfm_set_pmu_owner(NULL, NULL);
+		PFM_DBG("released ownership");
+	}
+
+	/*
+	 * account for pending overflows
+	 */
+	pfm_update_ovfl_pmds(ctx);
+
+	pfm_smpl_buf_unload_context(ctx);
+
+	/*
+	 * arch-specific unload operations
+	 */
+	pfm_arch_unload_context(ctx);
+
+	/*
+	 * per-thread: disconnect from monitored task
+	 */
+	if (task) {
+		task->pfm_context = NULL;
+		ctx->task = NULL;
+		clear_tsk_thread_flag(task, TIF_PERFMON_CTXSW);
+		clear_tsk_thread_flag(task, TIF_PERFMON_WORK);
+		pfm_arch_disarm_handle_work(task);
+	}
+	/*
+	 * session can be freed, must have interrupts enabled
+	 * thus we release in the caller. Bit 0 signals to the
+	 * caller that the session can be released.
+	 */
+	*release_info |= 0x1;
+
+	return 0;
+}
+
+/**
+ * __pfm_exit_thread - detach and free context on thread exit
+ */
+void __pfm_exit_thread(void)
+{
+	struct pfm_context *ctx;
+	unsigned long flags;
+	int free_ok = 0, release_info = 0;
+	int ret;
+
+	ctx  = current->pfm_context;
+
+	BUG_ON(ctx->flags.system);
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	PFM_DBG("state=%d is_self=%d", ctx->state, ctx->flags.is_self);
+
+	/*
+	 * __pfm_unload_context() cannot fail
+	 * in the context states we are interested in
+	 */
+	switch (ctx->state) {
+	case PFM_CTX_LOADED:
+	case PFM_CTX_MASKED:
+		__pfm_unload_context(ctx, &release_info);
+		/*
+		 * end notification only sent for non
+		 * self-monitoring context
+		 */
+		if (!ctx->flags.is_self)
+			pfm_end_notify(ctx);
+		break;
+	case PFM_CTX_ZOMBIE:
+		__pfm_unload_context(ctx, &release_info);
+		free_ok = 1;
+		break;
+	default:
+		BUG_ON(ctx->state != PFM_CTX_LOADED);
+		break;
+	}
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	/*
+	 * cancel timer now that context is unlocked
+	 */
+	if (release_info & 0x2) {
+		ret = hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
+		PFM_DBG("timeout cancel=%d", ret);
+	}
+
+	if (release_info & 0x1)
+		pfm_session_release(0, 0);
+
+	/*
+	 * All memory free operations (especially for vmalloc'ed memory)
+	 * MUST be done with interrupts ENABLED.
+	 */
+	if (free_ok)
+		pfm_free_context(ctx);
+}
Index: linux-2.6.31-master/perfmon/perfmon_ctx.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_ctx.c
@@ -0,0 +1,301 @@
+/*
+ * perfmon_ctx.c: perfmon2 context functions
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+extern asmlinkage long sys_close(int fd);
+/*
+ * context memory pool pointer
+ */
+static struct kmem_cache *pfm_ctx_cachep;
+
+/**
+ * pfm_free_context - de-allocate context and associated resources
+ * @ctx: context to free
+ */
+void pfm_free_context(struct pfm_context *ctx)
+{
+	pfm_arch_context_free(ctx);
+
+	pfm_free_sets(ctx);
+
+	pfm_smpl_buf_free(ctx);
+
+	PFM_DBG("free ctx @0x%p", ctx);
+	kmem_cache_free(pfm_ctx_cachep, ctx);
+	/*
+	 * decrease refcount on:
+	 * 	- PMU description table
+	 * 	- sampling format
+	 */
+	pfm_pmu_conf_put();
+	pfm_pmu_release();
+}
+
+/**
+ * pfm_ctx_flags_sane - check if context flags passed by user are okay
+ * @ctx_flags: flags passed user on pfm_create_context
+ *
+ * return:
+ * 	 0 if successful
+ * 	<0 and error code otherwise
+ */
+static inline int pfm_ctx_flags_sane(u32 ctx_flags)
+{
+	if (ctx_flags & PFM_FL_SYSTEM_WIDE) {
+		if (ctx_flags & PFM_FL_NOTIFY_BLOCK) {
+			PFM_DBG("cannot use blocking mode in syswide mode");
+			return -EINVAL;
+		}
+	}
+	return 0;
+}
+
+/**
+ * pfm_ctx_permissions - check authorization to create new context
+ * @ctx_flags: context flags passed by user
+ *
+ * check for permissions to create a context.
+ *
+ * A sysadmin may decide to restrict creation of per-thread
+ * and/or system-wide context to a group of users using the
+ * group id via /sys/kernel/perfmon/task_group  and
+ * /sys/kernel/perfmon/sys_group.
+ *
+ * Once we identify a user level package which can be used
+ * to grant/revoke Linux capabilites at login via PAM, we will
+ * be able to use capabilities. We would also need to increase
+ * the size of cap_t to support more than 32 capabilities (it
+ * is currently defined as u32 and 32 capabilities are alrady
+ * defined).
+ */
+static inline int pfm_ctx_permissions(u32 ctx_flags)
+{
+	if ((ctx_flags & PFM_FL_SYSTEM_WIDE)
+	    && pfm_controls.sys_group != PFM_GROUP_PERM_ANY
+	    && !in_group_p(pfm_controls.sys_group)) {
+		PFM_DBG("user group not allowed to create a syswide ctx");
+		return -EPERM;
+	} else if (pfm_controls.task_group != PFM_GROUP_PERM_ANY
+		   && !in_group_p(pfm_controls.task_group)) {
+		PFM_DBG("user group not allowed to create a task context");
+		return -EPERM;
+	}
+	return 0;
+}
+
+/**
+ * __pfm_create_context - allocate and initialize a perfmon context
+ * @req : pfarg_ctx from user
+ * @fmt : pointer sampling format, NULL if not used
+ * @fmt_arg: pointer to argument to sampling format, NULL if not used
+ * @mode: PFM_NORMAL or PFM_COMPAT(IA-64 v2.0 compatibility)
+ * @ctx : address of new context upon succesful return, undefined otherwise
+ *
+ * function used to allocate a new context. A context is allocated along
+ * with the default event set. If a sampling format is used, the buffer
+ * may be allocated and initialized.
+ *
+ * The file descriptor identifying the context is allocated and returned
+ * to caller.
+ *
+ * This function operates with no locks and interrupts are enabled.
+ * return:
+ * 	>=0: the file descriptor to identify the context
+ * 	<0 : the error code
+ */
+int __pfm_create_context(struct pfarg_ctx *req,
+			 struct pfm_smpl_fmt *fmt,
+			 void *fmt_arg,
+			 int mode,
+			 struct pfm_context **new_ctx)
+{
+	struct pfm_context *ctx;
+	u32 ctx_flags;
+	int fd, ret;
+
+	ctx_flags = req->ctx_flags;
+
+	/* Increase refcount on PMU description */
+	ret = pfm_pmu_conf_get(1);
+	if (ret < 0)
+		goto error_conf;
+
+	ret = pfm_ctx_flags_sane(ctx_flags);
+	if (ret < 0)
+		goto error_smpl;
+
+	ret = pfm_ctx_permissions(ctx_flags);
+	if (ret < 0)
+		goto error_smpl;
+
+	/*
+	 * we can use GFP_KERNEL and potentially sleep because we do
+	 * not hold any lock at this point.
+	 */
+	might_sleep();
+	ret = -ENOMEM;
+	ctx = kmem_cache_zalloc(pfm_ctx_cachep, GFP_KERNEL);
+	if (!ctx)
+		goto error_smpl;
+
+	PFM_DBG("alloc ctx @0x%p", ctx);
+
+	INIT_LIST_HEAD(&ctx->set_list);
+	spin_lock_init(&ctx->lock);
+	init_completion(&ctx->restart_complete);
+	init_waitqueue_head(&ctx->msgq_wait);
+
+	/*
+	 * context is unloaded
+	 */
+	ctx->state = PFM_CTX_UNLOADED;
+
+	/*
+	 * initialization of context's flags
+	 * must be done before pfm_find_set()
+	 */
+	ctx->flags.block = (ctx_flags & PFM_FL_NOTIFY_BLOCK) ? 1 : 0;
+	ctx->flags.system = (ctx_flags & PFM_FL_SYSTEM_WIDE) ? 1: 0;
+	ctx->flags.no_msg = (ctx_flags & PFM_FL_OVFL_NO_MSG) ? 1: 0;
+	ctx->flags.ia64_v20_compat = mode == PFM_COMPAT ? 1 : 0;
+
+	/*
+	 * link to format, must be done first for correct
+	 * error handling in pfm_free_context()
+	 */
+	ctx->smpl_fmt = fmt;
+
+	ret = pfm_pmu_acquire(ctx);
+	if (ret)
+		goto error_alloc;
+	/*
+	 * check if PMU is usable
+	 */
+	if (!(ctx->regs.num_pmcs && ctx->regs.num_pmds)) {
+		PFM_DBG("no usable PMU registers");
+		ret = -EBUSY;
+		goto error_alloc;
+	}
+
+	/*
+	 * initialize arch-specific section
+	 * must be done before fmt_init()
+	 */
+	ret = pfm_arch_context_create(ctx, ctx_flags);
+	if (ret)
+		goto error_alloc;
+
+	/*
+	 * add initial set
+	 */
+	ret = -ENOMEM;
+	if (pfm_create_initial_set(ctx))
+		goto error_alloc;
+
+	ret = fd = pfm_alloc_fd(ctx);
+	if (ret < 0)
+		goto error_alloc;
+
+	/*
+	 * does the user want to sample?
+	 * must be done after pfm_pmu_acquire() because
+	 * needs ctx->regs
+	 */
+	if (fmt) {
+		ret = pfm_setup_smpl_fmt(ctx, ctx_flags, fmt_arg, fd);
+		if (ret)
+			goto error_fd;
+	}
+
+	ctx->last_act = PFM_INVALID_ACTIVATION;
+	ctx->last_cpu = -1;
+
+	/*
+	 * initialize notification message queue
+	 */
+	ctx->msgq_head = ctx->msgq_tail = 0;
+
+	PFM_DBG("flags=0x%x sys=%d block=%d no_msg=%d"
+		" use_fmt=%d fd=%d mode=%d",
+		ctx_flags,
+		ctx->flags.system,
+		ctx->flags.block,
+		ctx->flags.no_msg,
+		!!fmt,
+		ret, mode);
+
+	if (new_ctx)
+		*new_ctx = ctx;
+
+	return fd;
+error_fd:
+	sys_close(fd);
+error_alloc:
+	/*
+	 * free arch, sets, smpl_buffer
+ 	 * conf, pmu
+ 	 */
+	pfm_free_context(ctx);
+	return ret;
+error_smpl:
+	pfm_pmu_conf_put();
+error_conf:
+	pfm_smpl_fmt_put(fmt);
+	return ret;
+}
+
+/**
+ * pfm_init_ctx -- initialize context SLAB
+ *
+ * called from pfm_init
+ */
+int __init pfm_init_ctx(void)
+{
+	pfm_ctx_cachep = kmem_cache_create("pfm_context",
+				   sizeof(struct pfm_context)+PFM_ARCH_CTX_SIZE,
+				   SLAB_HWCACHE_ALIGN, 0, NULL);
+	if (!pfm_ctx_cachep) {
+		PFM_ERR("cannot initialize context slab");
+		return -ENOMEM;
+	}
+	return 0;
+}
Index: linux-2.6.31-master/perfmon/perfmon_ctxsw.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_ctxsw.c
@@ -0,0 +1,337 @@
+/*
+ * perfmon_cxtsw.c: perfmon2 context switch code
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+void pfm_save_pmds(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	u64 val, ovfl_mask;
+	u64 *cnt_pmds;
+	int i;
+
+        cnt_pmds = ctx->regs.cnt_pmds;
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+
+	/*
+	 * save HW PMD, for counters, reconstruct 64-bit value
+	 */
+	for_each_bit(i, cast_ulp(set->used_pmds), ctx->regs.max_pmd) {
+		val = pfm_read_pmd(ctx, i);
+		if (likely(test_bit(i, cast_ulp(cnt_pmds))))
+			val = (set->pmds[i].value & ~ovfl_mask) |
+				(val & ovfl_mask);
+		set->pmds[i].value = val;
+	}
+	pfm_arch_clear_pmd_ovfl_cond(ctx, set);
+}
+
+/*
+ * interrupts are  disabled (no preemption)
+ */
+void __pfm_ctxswin_thread(struct task_struct *task,
+				 struct pfm_context *ctx, u64 now)
+{
+	u64 cur_act;
+	struct pfm_event_set *set;
+	int reload_pmcs, reload_pmds;
+	int mycpu, is_active;
+
+	mycpu = smp_processor_id();
+
+	cur_act = __get_cpu_var(pmu_activation_number);
+	/*
+	 * we need to lock context because it could be accessed
+	 * from another CPU. Normally the schedule() functions
+	 * has masked interrupts which should be enough to
+	 * protect against PMU interrupts.
+	 */
+	spin_lock(&ctx->lock);
+
+	is_active = pfm_arch_is_active(ctx);
+
+	set = ctx->active_set;
+
+	/*
+	 * in case fo zombie, we do not complete ctswin of the
+	 * PMU, and we force a call to pfm_handle_work() to finish
+	 * cleanup, i.e., free context + smpl_buff. The reason for
+	 * deferring to pfm_handle_work() is that it is not possible
+	 * to vfree() with interrupts disabled.
+	 */
+	if (unlikely(ctx->state == PFM_CTX_ZOMBIE)) {
+		pfm_post_work(task, ctx, PFM_WORK_ZOMBIE);
+		goto done;
+	}
+
+	/*
+	 * if we were the last user of the PMU on that CPU,
+	 * then nothing to do except restore psr
+	 */
+	if (ctx->last_cpu == mycpu && ctx->last_act == cur_act) {
+		/*
+		 * check for forced reload conditions
+		 */
+		reload_pmcs = set->priv_flags & PFM_SETFL_PRIV_MOD_PMCS;
+		reload_pmds = set->priv_flags & PFM_SETFL_PRIV_MOD_PMDS;
+	} else {
+#ifndef CONFIG_SMP
+		pfm_check_save_prev_ctx();
+#endif
+		reload_pmcs = 1;
+		reload_pmds = 1;
+	}
+	/* consumed */
+	set->priv_flags &= ~PFM_SETFL_PRIV_MOD_BOTH;
+
+	/*
+	 * record current activation for this context
+	 */
+	__get_cpu_var(pmu_activation_number)++;
+	ctx->last_cpu = mycpu;
+	ctx->last_act = __get_cpu_var(pmu_activation_number);
+
+	/*
+	 * establish new ownership.
+	 */
+	pfm_set_pmu_owner(task, ctx);
+
+	if (reload_pmds)
+		pfm_arch_restore_pmds(ctx, set);
+
+	/*
+	 * need to check if had in-flight interrupt in
+	 * pfm_ctxswout_thread(). If at least one bit set, then we must replay
+	 * the interrupt to avoid losing some important performance data.
+	 *
+	 * npend_ovfls is cleared in interrupt handler
+	 */
+	if (set->npend_ovfls) {
+		pfm_arch_resend_irq(ctx);
+		pfm_stats_inc(ovfl_intr_replay_count);
+	}
+
+	if (reload_pmcs)
+		pfm_arch_restore_pmcs(ctx, set);
+
+	pfm_arch_ctxswin_thread(task, ctx);
+	/*
+	 * set->duration does not count when context in MASKED state.
+	 * set->duration_start is reset in unmask_monitoring()
+	 */
+	set->duration_start = now;
+
+	/*
+	 * re-arm switch timeout, if necessary
+	 * Timeout is active only if monitoring is active,
+	 * i.e., LOADED + started
+	 *
+	 * We reload the remainder timeout or the full timeout.
+	 * Remainder is recorded on context switch out or in
+	 * pfm_load_context()
+	 */
+	if (ctx->state == PFM_CTX_LOADED
+	    && (set->flags & PFM_SETFL_TIME_SWITCH) && is_active) {
+		pfm_restart_timer(ctx, set);
+		/* careful here as pfm_restart_timer may switch sets */
+	}
+done:
+	spin_unlock(&ctx->lock);
+}
+
+/*
+ * interrupts are masked, runqueue lock is held.
+ *
+ * In UP. we simply stop monitoring and leave the state
+ * in place, i.e., lazy save
+ */
+void __pfm_ctxswout_thread(struct task_struct *task,
+				  struct pfm_context *ctx, u64 now)
+{
+	struct pfm_event_set *set;
+	int need_save_pmds, is_active;
+
+	/*
+	 * we need to lock context because it could be accessed
+	 * from another CPU. Normally the schedule() functions
+	 * has masked interrupts which should be enough to
+	 * protect against PMU interrupts.
+	 */
+
+	spin_lock(&ctx->lock);
+
+	is_active = pfm_arch_is_active(ctx);
+	set = ctx->active_set;
+
+	/*
+	 * stop monitoring and
+	 * collect pending overflow information
+	 * needed on ctxswin. We cannot afford to lose
+	 * a PMU interrupt.
+	 */
+	need_save_pmds = pfm_arch_ctxswout_thread(task, ctx);
+
+	if (ctx->state == PFM_CTX_LOADED) {
+		/*
+		 * accumulate only when set is actively monitoring,
+		 */
+		set->duration += now - set->duration_start;
+
+		/*
+		 * record remaining timeout
+		 * reload in pfm_ctxsw_in()
+		 */
+		if (is_active && (set->flags & PFM_SETFL_TIME_SWITCH)) {
+			struct hrtimer *h = NULL;
+			h = &__get_cpu_var(pfm_hrtimer);
+			hrtimer_cancel(h);
+			set->hrtimer_rem = hrtimer_get_remaining(h);
+			PFM_DBG_ovfl("hrtimer=%lld",
+				     (long long)set->hrtimer_rem.tv64);
+		}
+	}
+
+#ifdef CONFIG_SMP
+		/*
+	 * On some architectures, it is necessary to read the
+	 * PMD registers to check for pending overflow in
+	 * pfm_arch_ctxswout_thread(). In that case, saving of
+	 * the PMDs  may be  done there and not here.
+	 */
+	if (need_save_pmds)
+		pfm_save_pmds(ctx, set);
+
+	/*
+	 * in SMP, release ownership of this PMU.
+	 * PMU interrupts are masked, so nothing
+	 * can happen.
+	 */
+	pfm_set_pmu_owner(NULL, NULL);
+#endif
+	spin_unlock(&ctx->lock);
+}
+
+/*
+ *
+ */
+static void __pfm_ctxswout_sys(struct task_struct *prev,
+			       struct task_struct *next)
+{
+	struct pfm_context *ctx;
+
+	ctx = __get_cpu_var(pmu_ctx);
+	BUG_ON(!ctx);
+
+	/*
+	 * propagate TIF_PERFMON_CTXSW to ensure that:
+	 * - previous task has TIF_PERFMON_CTXSW cleared, in case it is
+	 *   scheduled onto another CPU where there is syswide monitoring
+	 * - next task has TIF_PERFMON_CTXSW set to ensure it will come back
+	 *   here when context switched out
+	 */
+	clear_tsk_thread_flag(prev, TIF_PERFMON_CTXSW);
+	set_tsk_thread_flag(next, TIF_PERFMON_CTXSW);
+
+	/*
+	 * nothing to do until actually started
+	 * XXX: assumes no mean to start from user level
+	 */
+	if (!ctx->flags.started)
+		return;
+
+	pfm_arch_ctxswout_sys(prev, ctx);
+}
+
+/*
+ *
+ */
+static void __pfm_ctxswin_sys(struct task_struct *prev,
+			      struct task_struct *next)
+{
+	struct pfm_context *ctx;
+
+	ctx = __get_cpu_var(pmu_ctx);
+	BUG_ON(!ctx);
+
+	/*
+	 * nothing to do until actually started
+	 * XXX: assumes no mean to start from user level
+	 */
+	if (!ctx->flags.started)
+		return;
+
+	pfm_arch_ctxswin_sys(next, ctx);
+}
+
+void pfm_ctxsw_out(struct task_struct *prev,
+		   struct task_struct *next)
+{
+	struct pfm_context *ctxp;
+	u64 now;
+
+	now = sched_clock();
+
+	ctxp = prev->pfm_context;
+
+	if (ctxp)
+		__pfm_ctxswout_thread(prev, ctxp, now);
+	else
+		__pfm_ctxswout_sys(prev, next);
+
+	pfm_stats_inc(ctxswout_count);
+	pfm_stats_add(ctxswout_ns, sched_clock() - now);
+}
+
+void pfm_ctxsw_in(struct task_struct *prev,
+		  struct task_struct *next)
+{
+	struct pfm_context *ctxn;
+	u64 now;
+
+	now = sched_clock();
+
+	ctxn = next->pfm_context;
+
+	if (ctxn)
+		__pfm_ctxswin_thread(next, ctxn, now);
+	else
+		__pfm_ctxswin_sys(prev, next);
+
+	pfm_stats_inc(ctxswin_count);
+	pfm_stats_add(ctxswin_ns, sched_clock() - now);
+}
Index: linux-2.6.31-master/perfmon/perfmon_debugfs.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_debugfs.c
@@ -0,0 +1,168 @@
+/*
+ * perfmon_debugfs.c: perfmon2 statistics interface to debugfs
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 2007 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/debugfs.h>
+#include <linux/perfmon_kern.h>
+
+/*
+ * to make the statistics visible to user space:
+ * $ mount -t debugfs none /mnt
+ * $ cd /mnt/perfmon
+ * then choose a CPU subdir
+ */
+DECLARE_PER_CPU(struct pfm_stats, pfm_stats);
+
+static struct dentry *pfm_debugfs_dir;
+
+void pfm_reset_stats(int cpu)
+{
+	struct pfm_stats *st;
+	unsigned long flags;
+
+	st = &per_cpu(pfm_stats, cpu);
+
+	local_irq_save(flags);
+	memset(st->v, 0, sizeof(st->v));
+	local_irq_restore(flags);
+}
+
+static const char *pfm_stats_strs[] = {
+	"ovfl_intr_all_count",
+	"ovfl_intr_ns",
+	"ovfl_intr_spurious_count",
+	"ovfl_intr_replay_count",
+	"ovfl_intr_regular_count",
+	"handle_work_count",
+	"ovfl_notify_count",
+	"reset_pmds_count",
+	"pfm_restart_count",
+	"fmt_handler_calls",
+	"fmt_handler_ns",
+	"set_switch_count",
+	"set_switch_ns",
+	"set_switch_exp",
+	"ctxswin_count",
+	"ctxswin_ns",
+	"handle_timeout_count",
+	"ovfl_intr_nmi_count",
+	"ctxswout_count",
+	"ctxswout_ns",
+};
+#define PFM_NUM_STRS ARRAY_SIZE(pfm_stats_strs)
+
+void pfm_debugfs_del_cpu(int cpu)
+{
+	struct pfm_stats *st;
+	int i;
+
+	st = &per_cpu(pfm_stats, cpu);
+
+	for (i = 0; i < PFM_NUM_STATS; i++) {
+		if (st->dirs[i])
+			debugfs_remove(st->dirs[i]);
+		st->dirs[i] = NULL;
+	}
+	if (st->cpu_dir)
+		debugfs_remove(st->cpu_dir);
+	st->cpu_dir = NULL;
+}
+
+int pfm_debugfs_add_cpu(int cpu)
+{
+	struct pfm_stats *st;
+	int i;
+
+	/*
+	 * sanity check between stats names and the number
+	 * of entries in the pfm_stats value array.
+	 */
+	if (PFM_NUM_STRS != PFM_NUM_STATS) {
+		PFM_ERR("PFM_NUM_STRS != PFM_NUM_STATS error");
+		return -1;
+	}
+
+	st = &per_cpu(pfm_stats, cpu);
+	sprintf(st->cpu_name, "cpu%d", cpu);
+
+	st->cpu_dir = debugfs_create_dir(st->cpu_name, pfm_debugfs_dir);
+	if (!st->cpu_dir)
+		return -1;
+
+	for (i = 0; i < PFM_NUM_STATS; i++) {
+		st->dirs[i] = debugfs_create_u64(pfm_stats_strs[i],
+						 S_IRUGO,
+						 st->cpu_dir,
+						 &st->v[i]);
+		if (!st->dirs[i])
+			goto error;
+	}
+	pfm_reset_stats(cpu);
+	return 0;
+error:
+	while (i >= 0) {
+		debugfs_remove(st->dirs[i]);
+		i--;
+	}
+	debugfs_remove(st->cpu_dir);
+	return -1;
+}
+
+/*
+ * called once from pfm_init()
+ */
+int __init pfm_init_debugfs(void)
+{
+	int cpu1, cpu2, ret;
+
+	pfm_debugfs_dir = debugfs_create_dir("perfmon", NULL);
+	if (!pfm_debugfs_dir)
+		return -1;
+
+	for_each_online_cpu(cpu1) {
+		ret = pfm_debugfs_add_cpu(cpu1);
+		if (ret)
+			goto error;
+	}
+	return 0;
+error:
+	for_each_online_cpu(cpu2) {
+		if (cpu2 == cpu1)
+			break;
+		pfm_debugfs_del_cpu(cpu2);
+	}
+	return -1;
+}
Index: linux-2.6.31-master/perfmon/perfmon_dfl_smpl.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_dfl_smpl.c
@@ -0,0 +1,298 @@
+/*
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ *               Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This file implements the new default sampling buffer format
+ * for the perfmon2 subsystem.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+
+#include <linux/perfmon_kern.h>
+#include <linux/perfmon_dfl_smpl.h>
+
+MODULE_AUTHOR("Stephane Eranian <eranian@hpl.hp.com>");
+MODULE_DESCRIPTION("new perfmon default sampling format");
+MODULE_LICENSE("GPL");
+
+static int pfm_dfl_fmt_validate(u32 ctx_flags, u16 npmds, void *data)
+{
+	struct pfm_dfl_smpl_arg *arg = data;
+	u64 min_buf_size;
+
+	if (data == NULL) {
+		PFM_DBG("no argument passed");
+		return -EINVAL;
+	}
+
+	/*
+	 * sanity check in case size_t is smaller then u64
+	 */
+#if BITS_PER_LONG == 4
+#define MAX_SIZE_T	(1ULL<<(sizeof(size_t)<<3))
+	if (sizeof(size_t) < sizeof(arg->buf_size)) {
+		if (arg->buf_size >= MAX_SIZE_T)
+			return -ETOOBIG;
+	}
+#endif
+
+	/*
+	 * compute min buf size. npmds is the maximum number
+	 * of implemented PMD registers.
+	 */
+	min_buf_size = sizeof(struct pfm_dfl_smpl_hdr)
+		+ (sizeof(struct pfm_dfl_smpl_entry) + (npmds*sizeof(u64)));
+
+	PFM_DBG("validate ctx_flags=0x%x flags=0x%x npmds=%u "
+		"min_buf_size=%llu buf_size=%llu\n",
+		ctx_flags,
+		arg->buf_flags,
+		npmds,
+		(unsigned long long)min_buf_size,
+		(unsigned long long)arg->buf_size);
+
+	/*
+	 * must hold at least the buffer header + one minimally sized entry
+	 */
+	if (arg->buf_size < min_buf_size)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int pfm_dfl_fmt_get_size(u32 flags, void *data, size_t *size)
+{
+	struct pfm_dfl_smpl_arg *arg = data;
+
+	/*
+	 * size has been validated in default_validate
+	 * we can never loose bits from buf_size.
+	 */
+	*size = (size_t)arg->buf_size;
+
+	return 0;
+}
+
+static int pfm_dfl_fmt_init(struct pfm_context *ctx, void *buf, u32 ctx_flags,
+			    u16 npmds, void *data)
+{
+	struct pfm_dfl_smpl_hdr *hdr;
+	struct pfm_dfl_smpl_arg *arg = data;
+
+	hdr = buf;
+
+	hdr->hdr_version = PFM_DFL_SMPL_VERSION;
+	hdr->hdr_buf_size = arg->buf_size;
+	hdr->hdr_buf_flags = arg->buf_flags;
+	hdr->hdr_cur_offs = sizeof(*hdr);
+	hdr->hdr_overflows = 0;
+	hdr->hdr_count = 0;
+	hdr->hdr_min_buf_space = sizeof(struct pfm_dfl_smpl_entry) + (npmds*sizeof(u64));
+	/*
+	 * due to cache aliasing, it may be necessary to flush the cache
+	 * on certain architectures (e.g., MIPS)
+	 */
+	pfm_cacheflush(hdr, sizeof(*hdr));
+
+	PFM_DBG("buffer=%p buf_size=%llu hdr_size=%zu hdr_version=%u.%u "
+		  "min_space=%llu npmds=%u",
+		  buf,
+		  (unsigned long long)hdr->hdr_buf_size,
+		  sizeof(*hdr),
+		  PFM_VERSION_MAJOR(hdr->hdr_version),
+		  PFM_VERSION_MINOR(hdr->hdr_version),
+		  (unsigned long long)hdr->hdr_min_buf_space,
+		  npmds);
+
+	return 0;
+}
+
+/*
+ * called from pfm_overflow_handler() to record a new sample
+ *
+ * context is locked, interrupts are disabled (no preemption)
+ */
+static int pfm_dfl_fmt_handler(struct pfm_context *ctx,
+			       unsigned long ip, u64 tstamp, void *data)
+{
+	struct pfm_dfl_smpl_hdr *hdr;
+	struct pfm_dfl_smpl_entry *ent;
+	struct pfm_ovfl_arg *arg;
+	void *cur, *last;
+	u64 *e;
+	size_t entry_size, min_size;
+	u16 npmds, i;
+	u16 ovfl_pmd;
+	void *buf;
+
+	hdr = ctx->smpl_addr;
+	arg = &ctx->ovfl_arg;
+
+        buf = hdr;
+	cur = buf+hdr->hdr_cur_offs;
+	last = buf+hdr->hdr_buf_size;
+	ovfl_pmd = arg->ovfl_pmd;
+	min_size = hdr->hdr_min_buf_space;
+
+	/*
+	 * precheck for sanity
+	 */
+	if ((last - cur) < min_size)
+		goto full;
+
+	npmds = arg->num_smpl_pmds;
+
+	ent = (struct pfm_dfl_smpl_entry *)cur;
+
+	entry_size = sizeof(*ent) + (npmds << 3);
+
+	/* position for first pmd */
+	e = (u64 *)(ent+1);
+
+	hdr->hdr_count++;
+
+	PFM_DBG_ovfl("count=%llu cur=%p last=%p free_bytes=%zu ovfl_pmd=%d "
+		     "npmds=%u",
+		     (unsigned long long)hdr->hdr_count,
+		     cur, last,
+		     (last-cur),
+		     ovfl_pmd,
+		     npmds);
+
+	/*
+	 * current = task running at the time of the overflow.
+	 *
+	 * per-task mode:
+	 * 	- this is usually the task being monitored.
+	 * 	  Under certain conditions, it might be a different task
+	 *
+	 * system-wide:
+	 * 	- this is not necessarily the task controlling the session
+	 */
+	ent->pid = current->pid;
+	ent->ovfl_pmd = ovfl_pmd;
+	ent->last_reset_val = arg->pmd_last_reset;
+
+	/*
+	 * where did the fault happen (includes slot number)
+	 */
+	ent->ip = ip;
+
+	ent->tstamp = tstamp;
+	ent->cpu = smp_processor_id();
+	ent->set = arg->active_set;
+	ent->tgid = current->tgid;
+
+	/*
+	 * selectively store PMDs in increasing index number
+	 */
+	if (npmds) {
+		u64 *val = arg->smpl_pmds_values;
+		for (i = 0; i < npmds; i++)
+			*e++ = *val++;
+	}
+
+	/*
+	 * update position for next entry
+	 */
+	hdr->hdr_cur_offs += entry_size;
+	cur += entry_size;
+
+	pfm_cacheflush(hdr, sizeof(*hdr));
+	pfm_cacheflush(ent, entry_size);
+
+	/*
+	 * post check to avoid losing the last sample
+	 */
+	if ((last - cur) < min_size)
+		goto full;
+
+	/* reset before returning from interrupt handler */
+	arg->ovfl_ctrl = PFM_OVFL_CTRL_RESET;
+
+	return 0;
+full:
+	PFM_DBG_ovfl("sampling buffer full free=%zu, count=%llu",
+		     last-cur,
+		     (unsigned long long)hdr->hdr_count);
+
+	/*
+	 * increment number of buffer overflows.
+	 * important to detect duplicate set of samples.
+	 */
+	hdr->hdr_overflows++;
+
+	/*
+	 * request notification and masking of monitoring.
+	 * Notification is still subject to the overflowed
+	 * register having the FL_NOTIFY flag set.
+	 */
+	arg->ovfl_ctrl = PFM_OVFL_CTRL_NOTIFY | PFM_OVFL_CTRL_MASK;
+
+	return -ENOBUFS; /* we are full, sorry */
+}
+
+static int pfm_dfl_fmt_restart(struct pfm_context *ctx, u32 *ovfl_ctrl)
+{
+	struct pfm_dfl_smpl_hdr *hdr;
+
+	hdr = ctx->smpl_addr;
+
+	hdr->hdr_count = 0;
+	hdr->hdr_cur_offs = sizeof(*hdr);
+
+	pfm_cacheflush(hdr, sizeof(*hdr));
+
+	*ovfl_ctrl = PFM_OVFL_CTRL_RESET;
+
+	return 0;
+}
+
+static int pfm_dfl_fmt_exit(void *buf)
+{
+	return 0;
+}
+
+static struct pfm_smpl_fmt dfl_fmt = {
+	.fmt_name = "default",
+	.fmt_version = 0x10000,
+	.fmt_arg_size = sizeof(struct pfm_dfl_smpl_arg),
+	.fmt_validate = pfm_dfl_fmt_validate,
+	.fmt_getsize = pfm_dfl_fmt_get_size,
+	.fmt_init = pfm_dfl_fmt_init,
+	.fmt_handler = pfm_dfl_fmt_handler,
+	.fmt_restart = pfm_dfl_fmt_restart,
+	.fmt_exit = pfm_dfl_fmt_exit,
+	.fmt_flags = PFM_FMT_BUILTIN_FLAG,
+	.owner = THIS_MODULE
+};
+
+static int pfm_dfl_fmt_init_module(void)
+{
+	return pfm_fmt_register(&dfl_fmt);
+}
+
+static void pfm_dfl_fmt_cleanup_module(void)
+{
+	pfm_fmt_unregister(&dfl_fmt);
+}
+
+module_init(pfm_dfl_fmt_init_module);
+module_exit(pfm_dfl_fmt_cleanup_module);
Index: linux-2.6.31-master/perfmon/perfmon_file.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_file.c
@@ -0,0 +1,646 @@
+/*
+ * perfmon_file.c: perfmon2 file input/output functions
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/file.h>
+#include <linux/poll.h>
+#include <linux/vfs.h>
+#include <linux/pagemap.h>
+#include <linux/mount.h>
+#include <linux/anon_inodes.h>
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+#define PFMFS_MAGIC 0xa0b4d889	/* perfmon filesystem magic number */
+
+struct pfm_controls pfm_controls = {
+	.sys_group = PFM_GROUP_PERM_ANY,
+	.task_group = PFM_GROUP_PERM_ANY,
+	.arg_mem_max = PAGE_SIZE,
+	.smpl_buffer_mem_max = ~0,
+};
+EXPORT_SYMBOL(pfm_controls);
+
+static int __init enable_debug(char *str)
+{
+	pfm_controls.debug = 1;
+	PFM_INFO("debug output enabled\n");
+	return 1;
+}
+__setup("perfmon_debug", enable_debug);
+
+int pfm_buf_map_pagefault(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+	void *kaddr;
+	unsigned long address;
+	struct pfm_context *ctx;
+	size_t size;
+
+	address = (unsigned long)vmf->virtual_address;
+
+	ctx = vma->vm_private_data;
+	if (ctx == NULL) {
+		PFM_DBG("no ctx");
+		return VM_FAULT_SIGBUS;
+	}
+	/*
+	 * size available to user (maybe different from real_smpl_size
+	 */
+	size = ctx->smpl_size;
+
+	if ((address < vma->vm_start) ||
+	    (address >= (vma->vm_start + size)))
+		return VM_FAULT_SIGBUS;
+
+	kaddr = ctx->smpl_addr + (address - vma->vm_start);
+
+	vmf->page = vmalloc_to_page(kaddr);
+	get_page(vmf->page);
+
+	PFM_DBG("[%d] start=%p ref_count=%d",
+		current->pid,
+		kaddr, page_count(vmf->page));
+
+	return 0;
+}
+
+/*
+ * we need to determine whther or not we are closing the last reference
+ * to the file and thus are going to end up in pfm_close() which eventually
+ * calls pfm_release_buf_space(). In that function, we update the accouting
+ * for locked_vm given that we are actually freeing the sampling buffer. The
+ * issue is that there are multiple paths leading to pfm_release_buf_space(),
+ * from exit(), munmap(), close(). The path coming from munmap() is problematic
+ * becuse do_munmap() grabs mmap_sem in write-mode which is also what
+ * pfm_release_buf_space does. To avoid deadlock, we need to determine where
+ * we are calling from and skip the locking. The vm_ops->close() callback
+ * is invoked for each remove_vma() independently of the number of references
+ * left on the file descriptor, therefore simple reference counter does not
+ * work. We need to determine if this is the last call, and then set a flag
+ * to skip the locking.
+ */
+static void pfm_buf_map_close(struct vm_area_struct *vma)
+{
+	struct file *file;
+	struct pfm_context *ctx;
+
+	file = vma->vm_file;
+	ctx = vma->vm_private_data;
+
+	/*
+	 * if file is going to close, then pfm_close() will
+	 * be called, do not lock in pfm_release_buf
+	 */
+	if (atomic_long_read(&file->f_count) == 1)
+		ctx->flags.mmap_nlock = 1;
+}
+
+/*
+ * we do not have a close callback because, the locked
+ * memory accounting must be done when the actual buffer
+ * is freed. Munmap does not free the page backing the vma
+ * because they may still be in use by the PMU interrupt handler.
+ */
+struct vm_operations_struct pfm_buf_map_vm_ops = {
+	.fault = pfm_buf_map_pagefault,
+	.close = pfm_buf_map_close
+};
+
+static int pfm_mmap_buffer(struct pfm_context *ctx, struct vm_area_struct *vma,
+			   size_t size)
+{
+	if (ctx->smpl_addr == NULL) {
+		PFM_DBG("no sampling buffer to map");
+		return -EINVAL;
+	}
+
+	if (size > ctx->smpl_size) {
+		PFM_DBG("mmap size=%zu >= actual buf size=%zu",
+			size,
+			ctx->smpl_size);
+		return -EINVAL;
+	}
+
+	vma->vm_ops = &pfm_buf_map_vm_ops;
+	vma->vm_private_data = ctx;
+
+	return 0;
+}
+
+static int pfm_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	size_t size;
+	struct pfm_context *ctx;
+	unsigned long flags;
+	int ret;
+
+	PFM_DBG("pfm_file_ops");
+
+	ctx  = file->private_data;
+	size = (vma->vm_end - vma->vm_start);
+
+	if (ctx == NULL)
+		return -EINVAL;
+
+	ret = -EINVAL;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	if (vma->vm_flags & VM_WRITE) {
+		PFM_DBG("cannot map buffer for writing");
+		goto done;
+	}
+
+	PFM_DBG("vm_pgoff=%lu size=%zu vm_start=0x%lx",
+		vma->vm_pgoff,
+		size,
+		vma->vm_start);
+
+	ret = pfm_mmap_buffer(ctx, vma, size);
+	if (ret == 0)
+		vma->vm_flags |= VM_RESERVED;
+
+	PFM_DBG("ret=%d vma_flags=0x%lx vma_start=0x%lx vma_size=%lu",
+		ret,
+		vma->vm_flags,
+		vma->vm_start,
+		vma->vm_end-vma->vm_start);
+done:
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	return ret;
+}
+
+/*
+ * Extract one message from queue.
+ *
+ * return:
+ * 	-EAGAIN:  when non-blocking and nothing is* in the queue.
+ * 	-ERESTARTSYS: when blocking and signal is pending
+ * 	Otherwise returns size of message (sizeof(pfarg_msg))
+ */
+ssize_t __pfm_read(struct pfm_context *ctx, union pfarg_msg *msg_buf, int non_block)
+{
+	ssize_t ret = 0;
+	unsigned long flags;
+	DECLARE_WAITQUEUE(wait, current);
+
+	/*
+	 * we must masks interrupts to avoid a race condition
+	 * with the PMU interrupt handler.
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	while (pfm_msgq_is_empty(ctx)) {
+
+		/*
+		 * handle non-blocking reads
+		 * return -EAGAIN
+		 */
+		ret = -EAGAIN;
+		if (non_block)
+			break;
+
+		add_wait_queue(&ctx->msgq_wait, &wait);
+		set_current_state(TASK_INTERRUPTIBLE);
+
+		spin_unlock_irqrestore(&ctx->lock, flags);
+
+		schedule();
+
+		/*
+		 * during this window, another thread may call
+		 * pfm_read() and steal our message
+		 */
+
+		spin_lock_irqsave(&ctx->lock, flags);
+
+		remove_wait_queue(&ctx->msgq_wait, &wait);
+		set_current_state(TASK_RUNNING);
+
+		/*
+		 * check for pending signals
+		 * return -ERESTARTSYS
+		 */
+		ret = -ERESTARTSYS;
+		if (signal_pending(current))
+			break;
+
+		/*
+		 * we may have a message
+		 */
+		ret = 0;
+	}
+
+	/*
+	 * extract message
+	 */
+	if (ret == 0) {
+		/*
+		 * copy the oldest message into msg_buf.
+		 * We cannot directly call copy_to_user()
+		 * because interrupts masked. This is done
+		 * in the caller
+		 */
+		pfm_get_next_msg(ctx, msg_buf);
+
+		ret = sizeof(*msg_buf);
+
+		PFM_DBG("extracted type=%d", msg_buf->type);
+	}
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	PFM_DBG("blocking=%d ret=%zd", non_block, ret);
+
+	return ret;
+}
+
+static ssize_t pfm_read(struct file *filp, char __user *buf, size_t size,
+			loff_t *ppos)
+{
+	struct pfm_context *ctx;
+	union pfarg_msg msg_buf;
+	int non_block, ret;
+
+	PFM_DBG_ovfl("buf=%p size=%zu", buf, size);
+
+	ctx = filp->private_data;
+	if (ctx == NULL) {
+		PFM_ERR("no ctx for pfm_read");
+		return -EINVAL;
+	}
+
+	non_block = filp->f_flags & O_NONBLOCK;
+
+#ifdef CONFIG_IA64_PERFMON_COMPAT
+	/*
+	 * detect IA-64 v2.0 context read (message size is different)
+	 * nops on all other architectures
+	 */
+	if (unlikely(ctx->flags.ia64_v20_compat))
+		return pfm_arch_compat_read(ctx,  buf, non_block, size);
+#endif
+	/*
+	 * cannot extract partial messages.
+	 * check even when there is no message
+	 *
+	 * cannot extract more than one message per call. Bytes
+	 * above sizeof(msg) are ignored.
+	 */
+	if (size < sizeof(msg_buf)) {
+		PFM_DBG("message is too small size=%zu must be >=%zu)",
+			size,
+			sizeof(msg_buf));
+		return -EINVAL;
+	}
+
+	ret =  __pfm_read(ctx, &msg_buf, non_block);
+	if (ret > 0) {
+		if (copy_to_user(buf, &msg_buf, sizeof(msg_buf)))
+			ret = -EFAULT;
+	}
+	PFM_DBG_ovfl("ret=%d", ret);
+	return ret;
+}
+
+static ssize_t pfm_write(struct file *file, const char __user *ubuf,
+			  size_t size, loff_t *ppos)
+{
+	PFM_DBG("pfm_write called");
+	return -EINVAL;
+}
+
+static unsigned int pfm_poll(struct file *filp, poll_table *wait)
+{
+	struct pfm_context *ctx;
+	unsigned long flags;
+	unsigned int mask = 0;
+
+	PFM_DBG("pfm_file_ops");
+
+	if (filp->f_op != &pfm_file_ops) {
+		PFM_ERR("pfm_poll bad magic");
+		return 0;
+	}
+
+	ctx = filp->private_data;
+	if (ctx == NULL) {
+		PFM_ERR("pfm_poll no ctx");
+		return 0;
+	}
+
+	PFM_DBG("before poll_wait");
+
+	poll_wait(filp, &ctx->msgq_wait, wait);
+
+	/*
+	 * pfm_msgq_is_empty() is non-atomic
+	 *
+	 * filp is protected by fget() at upper level
+	 * context cannot be closed by another thread.
+	 *
+	 * There may be a race with a PMU interrupt adding
+	 * messages to the queue. But we are interested in
+	 * queue not empty, so adding more messages should
+	 * not really be a problem.
+	 *
+	 * There may be a race with another thread issuing
+	 * a read() and stealing messages from the queue thus
+	 * may return the wrong answer. This could potentially
+	 * lead to a blocking read, because nothing is
+	 * available in the queue
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	if (!pfm_msgq_is_empty(ctx))
+		mask =  POLLIN | POLLRDNORM;
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	PFM_DBG("after poll_wait mask=0x%x", mask);
+
+	return mask;
+}
+
+static int pfm_ioctl(struct inode *inode, struct file *file, unsigned int cmd,
+		     unsigned long arg)
+{
+	PFM_DBG("pfm_ioctl called");
+	return -EINVAL;
+}
+
+/*
+ * interrupt cannot be masked when entering this function
+ */
+static inline int __pfm_fasync(int fd, struct file *filp,
+			       struct pfm_context *ctx, int on)
+{
+	int ret;
+
+	PFM_DBG("in  fd=%d on=%d async_q=%p",
+		fd,
+		on,
+		ctx->async_queue);
+
+	ret = fasync_helper(fd, filp, on, &ctx->async_queue);
+
+	PFM_DBG("out fd=%d on=%d async_q=%p ret=%d",
+		fd,
+		on,
+		ctx->async_queue, ret);
+
+	return ret;
+}
+
+static int pfm_fasync(int fd, struct file *filp, int on)
+{
+	struct pfm_context *ctx;
+	int ret;
+
+	PFM_DBG("pfm_file_ops");
+
+	ctx = filp->private_data;
+	if (ctx == NULL) {
+		PFM_ERR("pfm_fasync no ctx");
+		return -EBADF;
+	}
+
+	/*
+	 * we cannot mask interrupts during this call because this may
+	 * may go to sleep if memory is not readily avalaible.
+	 *
+	 * We are protected from the context disappearing by the
+	 * get_fd()/put_fd() done in caller. Serialization of this function
+	 * is ensured by caller.
+	 */
+	ret = __pfm_fasync(fd, filp, ctx, on);
+
+	PFM_DBG("pfm_fasync called on fd=%d on=%d async_queue=%p ret=%d",
+		fd,
+		on,
+		ctx->async_queue, ret);
+
+	return ret;
+}
+
+#ifdef CONFIG_SMP
+static void __pfm_close_remote_cpu(void *info)
+{
+	struct pfm_context *ctx = info;
+	int can_release;
+
+	BUG_ON(ctx != __get_cpu_var(pmu_ctx));
+
+	/*
+	 * we are in IPI interrupt handler which has always higher
+	 * priority than PMU interrupt, therefore we do not need to
+	 * mask interrupts. context locking is not needed because we
+	 * are in close(), no more user references.
+	 *
+	 * can_release is ignored, release done on calling CPU
+	 */
+	__pfm_unload_context(ctx, &can_release);
+
+	/*
+	 * we cannot free context here because we are in_interrupt().
+	 * we free on the calling CPU
+	 */
+}
+
+static int pfm_close_remote_cpu(u32 cpu, struct pfm_context *ctx)
+{
+	BUG_ON(irqs_disabled());
+	return smp_call_function_single(cpu, __pfm_close_remote_cpu, ctx, 1);
+}
+#endif /* CONFIG_SMP */
+
+/*
+ * called either on explicit close() or from exit_files().
+ * Only the LAST user of the file gets to this point, i.e., it is
+ * called only ONCE.
+ *
+ * IMPORTANT: we get called ONLY when the refcnt on the file gets to zero
+ * (fput()),i.e, last task to access the file. Nobody else can access the
+ * file at this point.
+ *
+ * When called from exit_files(), the VMA has been freed because exit_mm()
+ * is executed before exit_files().
+ *
+ * When called from exit_files(), the current task is not yet ZOMBIE but we
+ * flush the PMU state to the context.
+ */
+int __pfm_close(struct pfm_context *ctx, struct file *filp)
+{
+	unsigned long flags;
+	int state;
+	int can_free = 1, can_unload = 1;
+	int is_system, can_release = 0;
+	u32 cpu;
+
+	/*
+	 * no risk of ctx of filp disappearing so we can operate outside
+	 * of spin_lock(). fasync_helper() runs with interrupts masked,
+	 * thus there is no risk with the PMU interrupt handler
+	 *
+	 * In case of zombie, we will not have the async struct anymore
+	 * thus kill_fasync() will not do anything
+	 *
+	 * fd is not used when removing the entry so we pass -1
+	 */
+	if (filp->f_flags & FASYNC)
+		__pfm_fasync (-1, filp, ctx, 0);
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	state = ctx->state;
+	is_system = ctx->flags.system;
+	cpu = ctx->cpu;
+
+	PFM_DBG("state=%d", state);
+
+	/*
+	 * check if unload is needed
+	 */
+	if (state == PFM_CTX_UNLOADED)
+		goto doit;
+
+#ifdef CONFIG_SMP
+	/*
+	 * we need to release the resource on the ORIGINAL cpu.
+	 * we need to release the context lock to avoid deadlocks
+	 * on the original CPU, especially in the context switch
+	 * routines. It is safe to unlock because we are in close(),
+	 * in other words, there is no more access from user level.
+	 * we can also unmask interrupts on this CPU because the
+	 * context is running on the original CPU. Context will be
+	 * unloaded and the session will be released on the original
+	 * CPU. Upon return, the caller is guaranteed that the context
+	 * is gone from original CPU.
+	 */
+	if (is_system && cpu != smp_processor_id()) {
+		spin_unlock_irqrestore(&ctx->lock, flags);
+		pfm_close_remote_cpu(cpu, ctx);
+		can_release = 1;
+		goto free_it;
+	}
+
+	if (!is_system && ctx->task != current) {
+		/*
+		 * switch context to zombie state
+		 */
+		ctx->state = PFM_CTX_ZOMBIE;
+
+		PFM_DBG("zombie ctx for [%d]", ctx->task->pid);
+		/*
+		 * must check if other thread is using block overflow
+		 * notification mode. If so make sure it will not block
+		 * because there will not be any pfm_restart() issued.
+		 * When the thread notices the ZOMBIE state, it will clean
+		 * up what is left of the context
+		 */
+		if (state == PFM_CTX_MASKED && ctx->flags.block) {
+			/*
+			 * force task to wake up from MASKED state
+			 */
+			PFM_DBG("waking up [%d]", ctx->task->pid);
+
+			complete(&ctx->restart_complete);
+		}
+		/*
+		 * PMU session will be release by monitored task when it notices
+		 * ZOMBIE state as part of pfm_unload_context()
+		 */
+		can_unload = can_free = 0;
+	}
+#endif
+	if (can_unload)
+		__pfm_unload_context(ctx, &can_release);
+doit:
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+#ifdef CONFIG_SMP
+free_it:
+#endif
+	if (can_release)
+		pfm_session_release(is_system, cpu);
+
+	if (can_free)
+		pfm_free_context(ctx);
+
+	return 0;
+}
+
+static int pfm_close(struct inode *inode, struct file *filp)
+{
+	struct pfm_context *ctx;
+
+	PFM_DBG("called filp=%p", filp);
+
+	ctx = filp->private_data;
+	if (ctx == NULL) {
+		PFM_ERR("no ctx");
+		return -EBADF;
+	}
+	return __pfm_close(ctx, filp);
+}
+
+static int pfm_no_open(struct inode *irrelevant, struct file *dontcare)
+{
+	PFM_DBG("pfm_file_ops");
+
+	return -ENXIO;
+}
+
+
+const struct file_operations pfm_file_ops = {
+	.llseek = no_llseek,
+	.read = pfm_read,
+	.write = pfm_write,
+	.poll = pfm_poll,
+	.ioctl = pfm_ioctl,
+	.open = pfm_no_open, /* special open to disallow open via /proc */
+	.fasync = pfm_fasync,
+	.release = pfm_close,
+	.mmap = pfm_mmap
+};
+
+int pfm_alloc_fd(struct pfm_context *ctx)
+{
+	return anon_inode_getfd("[pfmfd]", &pfm_file_ops, ctx, O_RDONLY);
+}
Index: linux-2.6.31-master/perfmon/perfmon_fmt.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_fmt.c
@@ -0,0 +1,219 @@
+/*
+ * perfmon_fmt.c: perfmon2 sampling buffer format management
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_smpl_fmt_lock);
+static LIST_HEAD(pfm_smpl_fmt_list);
+
+static inline int fmt_is_mod(struct pfm_smpl_fmt *f)
+{
+	return !(f->fmt_flags & PFM_FMTFL_IS_BUILTIN);
+}
+
+static struct pfm_smpl_fmt *pfm_find_fmt(char *name)
+{
+	struct pfm_smpl_fmt *entry;
+
+	list_for_each_entry(entry, &pfm_smpl_fmt_list, fmt_list) {
+		if (!strcmp(entry->fmt_name, name))
+			return entry;
+	}
+	return NULL;
+}
+/*
+ * find a buffer format based on its name
+ */
+struct pfm_smpl_fmt *pfm_smpl_fmt_get(char *name)
+{
+	struct pfm_smpl_fmt *fmt;
+
+	spin_lock(&pfm_smpl_fmt_lock);
+
+	fmt = pfm_find_fmt(name);
+
+	/*
+	 * increase module refcount
+	 */
+	if (fmt && fmt_is_mod(fmt) && !try_module_get(fmt->owner))
+		fmt = NULL;
+
+	spin_unlock(&pfm_smpl_fmt_lock);
+
+	return fmt;
+}
+
+void pfm_smpl_fmt_put(struct pfm_smpl_fmt *fmt)
+{
+	if (fmt == NULL || !fmt_is_mod(fmt))
+		return;
+	BUG_ON(fmt->owner == NULL);
+
+	spin_lock(&pfm_smpl_fmt_lock);
+	module_put(fmt->owner);
+	spin_unlock(&pfm_smpl_fmt_lock);
+}
+
+int pfm_fmt_register(struct pfm_smpl_fmt *fmt)
+{
+	int ret = 0;
+
+	if (perfmon_disabled) {
+		PFM_INFO("perfmon disabled, cannot add sampling format");
+		return -ENOSYS;
+	}
+
+	/* some sanity checks */
+	if (fmt == NULL) {
+		PFM_INFO("perfmon: NULL format for register");
+		return -EINVAL;
+	}
+
+	if (fmt->fmt_name == NULL) {
+		PFM_INFO("perfmon: format has no name");
+		return -EINVAL;
+	}
+
+	if (fmt->fmt_qdepth > PFM_MSGS_COUNT) {
+		PFM_INFO("perfmon: format %s requires %u msg queue depth (max %d)",
+		       fmt->fmt_name,
+		       fmt->fmt_qdepth,
+		       PFM_MSGS_COUNT);
+		return -EINVAL;
+	}
+
+	/*
+	 * fmt is missing the initialization of .owner = THIS_MODULE
+	 * this is only valid when format is compiled as a module
+	 */
+	if (fmt->owner == NULL && fmt_is_mod(fmt)) {
+		PFM_INFO("format %s has no module owner", fmt->fmt_name);
+		return -EINVAL;
+	}
+	/*
+	 * we need at least a handler
+	 */
+	if (fmt->fmt_handler == NULL) {
+		PFM_INFO("format %s has no handler", fmt->fmt_name);
+		return -EINVAL;
+	}
+
+	/*
+	 * format argument size cannot be bigger than PAGE_SIZE
+	 */
+	if (fmt->fmt_arg_size > PAGE_SIZE) {
+		PFM_INFO("format %s arguments too big", fmt->fmt_name);
+		return -EINVAL;
+	}
+
+	spin_lock(&pfm_smpl_fmt_lock);
+
+	/*
+	 * because of sysfs, we cannot have two formats with the same name
+	 */
+	if (pfm_find_fmt(fmt->fmt_name)) {
+		PFM_INFO("format %s already registered", fmt->fmt_name);
+		ret = -EBUSY;
+		goto out;
+	}
+
+	ret = pfm_sysfs_add_fmt(fmt);
+	if (ret) {
+		PFM_INFO("sysfs cannot add format entry for %s", fmt->fmt_name);
+		goto out;
+	}
+
+	list_add(&fmt->fmt_list, &pfm_smpl_fmt_list);
+
+	PFM_INFO("added sampling format %s", fmt->fmt_name);
+out:
+	spin_unlock(&pfm_smpl_fmt_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(pfm_fmt_register);
+
+int pfm_fmt_unregister(struct pfm_smpl_fmt *fmt)
+{
+	struct pfm_smpl_fmt *fmt2;
+	int ret = 0;
+
+	if (!fmt || !fmt->fmt_name) {
+		PFM_DBG("invalid fmt");
+		return -EINVAL;
+	}
+
+	spin_lock(&pfm_smpl_fmt_lock);
+
+	fmt2 = pfm_find_fmt(fmt->fmt_name);
+	if (!fmt) {
+		PFM_INFO("unregister failed, format not registered");
+		ret = -EINVAL;
+		goto out;
+	}
+	list_del_init(&fmt->fmt_list);
+
+	pfm_sysfs_remove_fmt(fmt);
+
+	PFM_INFO("removed sampling format: %s", fmt->fmt_name);
+
+out:
+	spin_unlock(&pfm_smpl_fmt_lock);
+	return ret;
+
+}
+EXPORT_SYMBOL(pfm_fmt_unregister);
+
+/*
+ * we defer adding the builtin formats to /sys/kernel/perfmon/formats
+ * until after the pfm sysfs subsystem is initialized. This function
+ * is called from pfm_init_sysfs()
+ */
+void __init pfm_sysfs_builtin_fmt_add(void)
+{
+	struct pfm_smpl_fmt *entry;
+
+	/*
+	 * locking not needed, kernel not fully booted
+	 * when called
+	 */
+	list_for_each_entry(entry, &pfm_smpl_fmt_list, fmt_list) {
+		pfm_sysfs_add_fmt(entry);
+	}
+}
Index: linux-2.6.31-master/perfmon/perfmon_hotplug.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_hotplug.c
@@ -0,0 +1,158 @@
+/*
+ * perfmon_hotplug.c: handling of CPU hotplug
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/perfmon_kern.h>
+#include <linux/cpu.h>
+#include "perfmon_priv.h"
+
+#ifndef CONFIG_HOTPLUG_CPU
+void pfm_cpu_disable(void)
+{}
+
+int __init pfm_init_hotplug(void)
+{
+	return 0;
+}
+#else /* CONFIG_HOTPLUG_CPU */
+/*
+ * CPU hotplug event nofication callback
+ *
+ * We use the callback to do manage the sysfs interface.
+ * Note that the actual shutdown of monitoring on the CPU
+ * is done in pfm_cpu_disable(), see comments there for more
+ * information.
+ */
+static int pfm_cpu_notify(struct notifier_block *nfb,
+			  unsigned long action, void *hcpu)
+{
+	unsigned int cpu = (unsigned long)hcpu;
+
+	/* no PMU description loaded */
+	if (pfm_pmu_conf_get(0))
+		return NOTIFY_OK;
+
+	switch (action) {
+	case CPU_ONLINE:
+		pfm_debugfs_add_cpu(cpu);
+		PFM_INFO("CPU%d is online", cpu);
+		break;
+	case CPU_UP_PREPARE:
+		PFM_INFO("CPU%d prepare online", cpu);
+		break;
+	case CPU_UP_CANCELED:
+		pfm_debugfs_del_cpu(cpu);
+		PFM_INFO("CPU%d is up canceled", cpu);
+		break;
+	case CPU_DOWN_PREPARE:
+		PFM_INFO("CPU%d prepare offline", cpu);
+		break;
+	case CPU_DOWN_FAILED:
+		PFM_INFO("CPU%d is down failed", cpu);
+		break;
+	case CPU_DEAD:
+		pfm_debugfs_del_cpu(cpu);
+		PFM_INFO("CPU%d is offline", cpu);
+		break;
+	}
+	/*
+	 * call PMU module handler if any
+	 */
+	if (pfm_pmu_conf->hotplug_handler)
+		pfm_pmu_conf->hotplug_handler(action, cpu);
+
+	pfm_pmu_conf_put();
+	return NOTIFY_OK;
+}
+
+/*
+ * called from cpu_disable() to detach the perfmon context
+ * from the CPU going down.
+ *
+ * We cannot use the cpu hotplug notifier because we MUST run
+ * on the CPU that is going down to save the PMU state
+ */
+void pfm_cpu_disable(void)
+{
+	struct pfm_context *ctx;
+	unsigned long flags;
+	int is_system, release_info = 0;
+	u32 cpu;
+	int r;
+
+	ctx = __get_cpu_var(pmu_ctx);
+	if (ctx == NULL)
+		return;
+
+	is_system = ctx->flags.system;
+	cpu = ctx->cpu;
+
+	/*
+	 * context is LOADED or MASKED
+	 *
+	 * we unload from CPU. That stops monitoring and does
+	 * all the bookeeping of saving values and updating duration
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+	if (is_system)
+		__pfm_unload_context(ctx, &release_info);
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	/*
+	 * cancel timer
+	 */
+	if (release_info & 0x2) {
+		r = hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
+		PFM_DBG("timeout cancel=%d", r);
+	}
+
+	if (release_info & 0x1)
+		pfm_session_release(is_system, cpu);
+}
+
+static struct notifier_block pfm_cpu_notifier = {
+	.notifier_call = pfm_cpu_notify
+};
+
+int __init pfm_init_hotplug(void)
+{
+	int ret = 0;
+	/*
+	 * register CPU hotplug event notifier
+	 */
+	ret = register_cpu_notifier(&pfm_cpu_notifier);
+	if (!ret)
+		PFM_LOG("CPU hotplug support enabled");
+	return ret;
+}
+#endif /* CONFIG_HOTPLUG_CPU */
Index: linux-2.6.31-master/perfmon/perfmon_init.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_init.c
@@ -0,0 +1,130 @@
+/*
+ * perfmon.c: perfmon2 global initialization functions
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+/*
+ * external variables
+ */
+DEFINE_PER_CPU(struct task_struct *, pmu_owner);
+DEFINE_PER_CPU(struct pfm_context  *, pmu_ctx);
+DEFINE_PER_CPU(u64, pmu_activation_number);
+DEFINE_PER_CPU(struct pfm_stats, pfm_stats);
+DEFINE_PER_CPU(struct hrtimer, pfm_hrtimer);
+
+EXPORT_PER_CPU_SYMBOL(pmu_ctx);
+
+int perfmon_disabled;	/* >0 if perfmon is disabled */
+
+/*
+ * called from cpu_init() and pfm_pmu_register()
+ */
+void __pfm_init_percpu(void *dummy)
+{
+	struct hrtimer *h;
+
+	h = &__get_cpu_var(pfm_hrtimer);
+
+	pfm_arch_init_percpu();
+
+	/*
+	 * initialize per-cpu high res timer
+	 */
+	hrtimer_init(h, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+#ifdef CONFIG_HIGH_RES_TIMERS
+	/*
+	 * avoid potential deadlock on the runqueue lock
+	 * during context switch when multiplexing. Situation
+	 * arises on architectures which run switch_to() with
+	 * the runqueue lock held, e.g., x86. On others, e.g.,
+	 * IA-64, the problem does not exist.
+	 * Setting the callback mode to HRTIMER_CB_IRQSAFE_UNOCKED
+	 * such that the callback routine is only called on hardirq
+	 * context not on softirq, thus the context switch will not
+	 * end up trying to wakeup the softirqd
+	 */
+	//h->cb_mode = HRTIMER_CB_IRQSAFE_UNLOCKED;
+#endif
+	h->function = pfm_handle_switch_timeout;
+}
+
+/*
+ * global initialization routine, executed only once
+ */
+int __init pfm_init(void)
+{
+	PFM_LOG("version %u.%u", PFM_VERSION_MAJ, PFM_VERSION_MIN);
+
+	if (pfm_init_ctx())
+		goto error_disable;
+
+
+	if (pfm_init_sets())
+		goto error_disable;
+
+	if (pfm_init_sysfs())
+		goto error_disable;
+
+	/* not critical, so no error checking */
+	pfm_init_debugfs();
+
+	/*
+	 * one time, arch-specific global initialization
+	 */
+	if (pfm_arch_init())
+		goto error_disable;
+
+	if (pfm_init_hotplug())
+		goto error_disable;
+	return 0;
+
+error_disable:
+	PFM_ERR("perfmon is disabled due to initialization error");
+	perfmon_disabled = 1;
+	return -1;
+}
+
+/*
+ * must use subsys_initcall() to ensure that the perfmon2 core
+ * is initialized before any PMU description module when they are
+ * compiled in.
+ */
+subsys_initcall(pfm_init);
Index: linux-2.6.31-master/perfmon/perfmon_intr.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_intr.c
@@ -0,0 +1,626 @@
+/*
+ * perfmon_intr.c: perfmon2 interrupt handling
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+/**
+ * pfm_intr_process_64bit_ovfls - handle 64-bit counter emulation
+ * @ctx: context to operate on
+ * @set: set to operate on
+ *
+ * The function returns the number of 64-bit overflows detected.
+ *
+ * 64-bit software pmds are updated for overflowed pmd registers
+ * the set->reset_pmds is updated to the list of pmds to reset
+ *
+ * In any case, set->npend_ovfls is cleared
+ */
+static u16 pfm_intr_process_64bit_ovfls(struct pfm_context *ctx,
+					struct pfm_event_set *set,
+					u32 *ovfl_ctrl)
+{
+	u64 ovfl_thres, old_val, new_val, ovfl_mask;
+	u16 num_64b_ovfls, has_ovfl_sw, must_switch;
+	u16 max_pmd;
+	int i;
+
+	num_64b_ovfls = must_switch = 0;
+	max_pmd = ctx->regs.max_pmd;
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+
+	has_ovfl_sw = set->flags & PFM_SETFL_OVFL_SWITCH;
+
+	bitmap_zero(cast_ulp(set->reset_pmds), max_pmd);
+
+	for_each_bit(i, cast_ulp(set->povfl_pmds), max_pmd) {
+		/*
+		 * Update software value for counters ONLY
+		 *
+		 * Note that the pmd is not necessarily 0 at this point as
+		 * qualified events may have happened before the PMU was
+		 * frozen. The residual count is not taken into consideration
+		 * here but will be with any read of the pmd
+		 */
+		ovfl_thres = set->pmds[i].ovflsw_thres;
+
+		if (likely(test_bit(i, cast_ulp(ctx->regs.cnt_pmds)))) {
+			old_val = new_val = set->pmds[i].value;
+			new_val += 1 + ovfl_mask;
+			set->pmds[i].value = new_val;
+		}  else {
+			/*
+			 * for non counters which interrupt, e.g., AMD IBS,
+			 * we consider this equivalent to a 64-bit counter
+			 * overflow.
+			 */
+			old_val = 1; new_val = 0;
+		}
+
+		/*
+		 * check for 64-bit overflow condition
+		 */
+		if (likely(old_val > new_val)) {
+			num_64b_ovfls++;
+			if (has_ovfl_sw && ovfl_thres > 0) {
+				if (ovfl_thres == 1)
+					must_switch = 1;
+				set->pmds[i].ovflsw_thres = ovfl_thres - 1;
+			}
+
+			/*
+			 * what to reset because of this overflow
+			 * - the overflowed register
+			 * - its reset_smpls
+			 */
+			__set_bit(i, cast_ulp(set->reset_pmds));
+
+			bitmap_or(cast_ulp(set->reset_pmds),
+				  cast_ulp(set->reset_pmds),
+				  cast_ulp(set->pmds[i].reset_pmds),
+				  max_pmd);
+		} else {
+			/*
+			 * only keep track of 64-bit overflows or
+			 * assimilated
+			 */
+			__clear_bit(i, cast_ulp(set->povfl_pmds));
+
+			/*
+			 * on some PMU, it may be necessary to re-arm the PMD
+			 */
+			pfm_arch_ovfl_reset_pmd(ctx, i);
+		}
+
+		PFM_DBG_ovfl("ovfl=%s pmd%u new=0x%llx old=0x%llx "
+			     "hw_pmd=0x%llx o_pmds=0x%llx must_switch=%u "
+			     "o_thres=%llu o_thres_ref=%llu",
+			     old_val > new_val ? "64-bit" : "HW",
+			     i,
+			     (unsigned long long)new_val,
+			     (unsigned long long)old_val,
+			     (unsigned long long)pfm_read_pmd(ctx, i),
+			     (unsigned long long)set->povfl_pmds[0],
+			     must_switch,
+			     (unsigned long long)set->pmds[i].ovflsw_thres,
+			     (unsigned long long)set->pmds[i].ovflsw_ref_thres);
+	}
+	/*
+	 * update public bitmask of 64-bit overflowed pmds
+	 */
+	if (num_64b_ovfls)
+		bitmap_copy(cast_ulp(set->ovfl_pmds),
+			    cast_ulp(set->povfl_pmds),
+			    max_pmd);
+
+	if (must_switch)
+		*ovfl_ctrl |= PFM_OVFL_CTRL_SWITCH;
+
+	/*
+	 * mark the overflows as consumed
+	 */
+	set->npend_ovfls = 0;
+	bitmap_zero(cast_ulp(set->povfl_pmds), max_pmd);
+
+	return num_64b_ovfls;
+}
+
+/**
+ * pfm_intr_get_smpl_pmds_values - copy 64-bit pmd values for sampling format
+ * @ctx: context to work on
+ * @set: current event set
+ * @arg: overflow arg to be passed to format
+ * @smpl_pmds: list of PMDs of interest for the overflowed register
+ *
+ * build an array of 46-bit PMD values based on smpl_pmds. Values are
+ * stored in increasing order of the PMD indexes
+ */
+static void pfm_intr_get_smpl_pmds_values(struct pfm_context *ctx,
+					  struct pfm_event_set *set,
+					  struct pfm_ovfl_arg *arg,
+					  u64 *smpl_pmds)
+{
+	u16 j, k;
+	u64 new_val, ovfl_mask;
+	u64 *cnt_pmds;
+
+	cnt_pmds = ctx->regs.cnt_pmds;
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+
+	k = 0;
+	for_each_bit(j, cast_ulp(smpl_pmds), ctx->regs.max_pmd) {
+		new_val = pfm_read_pmd(ctx, j);
+
+		/* for counters, build 64-bit value */
+		if (test_bit(j, cast_ulp(cnt_pmds)))
+			new_val = (set->pmds[j].value & ~ovfl_mask)
+				| (new_val & ovfl_mask);
+
+		arg->smpl_pmds_values[k++] = new_val;
+
+		PFM_DBG_ovfl("s_pmd_val[%u]=pmd%u=0x%llx", k, j,
+			     (unsigned long long)new_val);
+	}
+	arg->num_smpl_pmds = k;
+}
+
+/**
+ * pfm_intr_process_smpl_fmt -- handle sampling format callback
+ * @ctx: context to work on
+ * @set: current event set
+ * @ip: interrupted instruction pointer
+ * @now: timestamp
+ * @num_ovfls: number of 64-bit overflows
+ * @ovfl_ctrl: set of controls for interrupt handler tail processing
+ * @regs: register state
+ *
+ * Prepare argument (ovfl_arg) to be passed to sampling format callback, then
+ * invoke the callback (fmt_handler)
+ */
+static int pfm_intr_process_smpl_fmt(struct pfm_context *ctx,
+				    struct pfm_event_set *set,
+				    unsigned long ip,
+				    u64 now,
+				    u64 num_ovfls,
+				    u32 *ovfl_ctrl,
+				    struct pt_regs *regs)
+{
+	struct pfm_ovfl_arg *ovfl_arg;
+	u64 start_cycles, end_cycles;
+	u16 max_pmd;
+	int i, ret = 0;
+
+	ovfl_arg = &ctx->ovfl_arg;
+	ovfl_arg->active_set = set->id;
+	max_pmd = ctx->regs.max_pmd;
+
+	/*
+	 * go over all 64-bit overflow
+	 */
+	for_each_bit(i, cast_ulp(set->ovfl_pmds), max_pmd) {
+		/*
+		 * prepare argument to fmt_handler
+		 */
+		ovfl_arg->ovfl_pmd = i;
+		ovfl_arg->ovfl_ctrl = 0;
+
+		ovfl_arg->pmd_last_reset = set->pmds[i].lval;
+		ovfl_arg->pmd_eventid = set->pmds[i].eventid;
+		ovfl_arg->num_smpl_pmds = 0;
+
+		/*
+		 * copy values of pmds of interest, if any
+		 * Sampling format may use them
+		 * We do not initialize the unused smpl_pmds_values
+		 */
+		if (!bitmap_empty(cast_ulp(set->pmds[i].smpl_pmds), max_pmd))
+			pfm_intr_get_smpl_pmds_values(ctx, set, ovfl_arg,
+						      set->pmds[i].smpl_pmds);
+
+		pfm_stats_inc(fmt_handler_calls);
+
+		/*
+		 * call format record (handler) routine
+		 */
+		start_cycles = sched_clock();
+		ret = (*ctx->smpl_fmt->fmt_handler)(ctx, ip, now, regs);
+		end_cycles = sched_clock();
+
+		/*
+		 * The reset_pmds mask is constructed automatically
+		 * on overflow. When the actual reset takes place
+		 * depends on the masking, switch and notification
+		 * status. It may be deferred until pfm_restart().
+		 */
+		*ovfl_ctrl |= ovfl_arg->ovfl_ctrl;
+
+		pfm_stats_add(fmt_handler_ns, end_cycles - start_cycles);
+	}
+	/*
+	 * when the format cannot handle the rest of the overflow, we abort
+	 */
+	if (ret)
+		PFM_DBG_ovfl("handler aborted at PMD%u ret=%d", i, ret);
+	return ret;
+}
+/**
+ * pfm_overflow_handler - main overflow processing routine.
+ * @ctx: context to work on (always current context)
+ * @set: current event set
+ * @ip: interrupt instruction pointer
+ * @regs: machine state
+ *
+ * set->num_ovfl_pmds is 0 when returning from this function even though
+ * set->ovfl_pmds[] may have bits set. When leaving set->num_ovfl_pmds
+ * must never be used to determine if there was a pending overflow.
+ */
+static void pfm_overflow_handler(struct pfm_context *ctx,
+				 struct pfm_event_set *set,
+				 unsigned long ip,
+				 struct pt_regs *regs)
+{
+	struct pfm_event_set *set_orig;
+	u64 now;
+	u32 ovfl_ctrl;
+	u16 max_intr, max_pmd;
+	u16 num_ovfls;
+	int ret, has_notify;
+
+	/*
+	 * take timestamp
+	 */
+	now = sched_clock();
+
+	max_pmd = ctx->regs.max_pmd;
+	max_intr = ctx->regs.max_intr_pmd;
+
+	set_orig = set;
+	ovfl_ctrl = 0;
+
+	/*
+	 * skip ZOMBIE case
+	 */
+	if (unlikely(ctx->state == PFM_CTX_ZOMBIE))
+		goto stop_monitoring;
+
+	PFM_DBG_ovfl("intr_pmds=0x%llx npend=%u ip=%p, blocking=%d "
+		     "u_pmds=0x%llx use_fmt=%u",
+		     (unsigned long long)set->povfl_pmds[0],
+		     set->npend_ovfls,
+		     (void *)ip,
+		     ctx->flags.block,
+		     (unsigned long long)set->used_pmds[0],
+		     !!ctx->smpl_fmt);
+
+	/*
+	 * return number of 64-bit overflows
+	 */
+	num_ovfls = pfm_intr_process_64bit_ovfls(ctx, set, &ovfl_ctrl);
+
+	/*
+	 * there were no 64-bit overflows
+	 * nothing else to do
+	 */
+	if (!num_ovfls)
+		return;
+
+	/*
+	 * tmp_ovfl_notify = ovfl_pmds & ovfl_notify
+	 * with:
+	 *   - ovfl_pmds: last 64-bit overflowed pmds
+	 *   - ovfl_notify: notify on overflow registers
+	 */
+	bitmap_and(cast_ulp(ctx->tmp_ovfl_notify),
+		   cast_ulp(set->ovfl_pmds),
+		   cast_ulp(set->ovfl_notify),
+		   max_intr);
+
+	has_notify = !bitmap_empty(cast_ulp(ctx->tmp_ovfl_notify), max_intr);
+
+	/*
+	 * check for sampling format and invoke fmt_handler
+	 */
+	if (likely(ctx->smpl_fmt)) {
+		pfm_intr_process_smpl_fmt(ctx, set, ip, now, num_ovfls,
+					  &ovfl_ctrl, regs);
+	} else {
+		/*
+		 * When no sampling format is used, the default
+		 * is:
+		 * 	- mask monitoring if not switching
+		 * 	- notify user if requested
+		 *
+		 * If notification is not requested, monitoring is masked
+		 * and overflowed registers are not reset (saturation).
+		 * This mimics the behavior of the default sampling format.
+		 */
+		ovfl_ctrl |= PFM_OVFL_CTRL_NOTIFY;
+		if (has_notify || !(ovfl_ctrl & PFM_OVFL_CTRL_SWITCH))
+			ovfl_ctrl |= PFM_OVFL_CTRL_MASK;
+	}
+
+	PFM_DBG_ovfl("set%u o_notify=0x%llx o_pmds=0x%llx "
+		     "r_pmds=0x%llx ovfl_ctrl=0x%x",
+		     set->id,
+		     (unsigned long long)ctx->tmp_ovfl_notify[0],
+		     (unsigned long long)set->ovfl_pmds[0],
+		     (unsigned long long)set->reset_pmds[0],
+		     ovfl_ctrl);
+
+	/*
+	 * execute the various controls
+	 *        ORDER MATTERS
+	 */
+
+
+	/*
+	 * mask monitoring
+	 */
+	if (ovfl_ctrl & PFM_OVFL_CTRL_MASK) {
+		pfm_mask_monitoring(ctx, set);
+		/*
+		 * when masking, reset is deferred until
+		 * pfm_restart()
+		 */
+		ovfl_ctrl &= ~PFM_OVFL_CTRL_RESET;
+
+		/*
+		 * when masking, switching is deferred until
+		 * pfm_restart and we need to remember it
+		 */
+		if (ovfl_ctrl & PFM_OVFL_CTRL_SWITCH) {
+			set->priv_flags |= PFM_SETFL_PRIV_SWITCH;
+			ovfl_ctrl &= ~PFM_OVFL_CTRL_SWITCH;
+		}
+	}
+
+	/*
+	 * switch event set
+	 */
+	if (ovfl_ctrl & PFM_OVFL_CTRL_SWITCH) {
+		pfm_switch_sets_from_intr(ctx);
+		/* update view of active set */
+		set = ctx->active_set;
+	}
+	/*
+	 * send overflow notification
+	 *
+	 * only necessary if at least one overflowed
+	 * register had the notify flag set
+	 */
+	if (has_notify && (ovfl_ctrl & PFM_OVFL_CTRL_NOTIFY)) {
+		/*
+		 * block on notify, not on masking
+		 */
+		if (ctx->flags.block)
+			pfm_post_work(current, ctx, PFM_WORK_BLOCK);
+
+		/*
+		 * send notification and passed original set id
+		 * if error, queue full, for instance, then default
+		 * to masking monitoring, i.e., saturate
+		 */
+		ret = pfm_ovfl_notify(ctx, set_orig, ip);
+		if (unlikely(ret)) {
+			if (ctx->state == PFM_CTX_LOADED) {
+				pfm_mask_monitoring(ctx, set);
+				ovfl_ctrl &= ~PFM_OVFL_CTRL_RESET;
+			}
+		} else {
+			ctx->flags.can_restart++;
+			PFM_DBG_ovfl("can_restart=%u", ctx->flags.can_restart);
+		}
+	}
+
+	/*
+	 * reset overflowed registers
+	 */
+	if (ovfl_ctrl & PFM_OVFL_CTRL_RESET) {
+		u16 nn;
+		nn = bitmap_weight(cast_ulp(set->reset_pmds), max_pmd);
+		if (nn)
+			pfm_reset_pmds(ctx, set, nn, PFM_PMD_RESET_SHORT);
+	}
+	return;
+
+stop_monitoring:
+	/*
+	 * Does not happen for a system-wide context nor for a
+	 * self-monitored context. We cannot attach to kernel-only
+	 * thread, thus it is safe to set TIF bits, i.e., the thread
+	 * will eventually leave the kernel or die and either we will
+	 * catch the context and clean it up in pfm_handler_work() or
+	 * pfm_exit_thread().
+	 *
+	 * Mask until we get to pfm_handle_work()
+	 */
+	pfm_mask_monitoring(ctx, set);
+
+	PFM_DBG_ovfl("ctx is zombie, converted to spurious");
+	pfm_post_work(current, ctx, PFM_WORK_ZOMBIE);
+}
+
+/**
+ * __pfm_interrupt_handler - 1st level interrupt handler
+ * @ip: interrupted instruction pointer
+ * @regs: machine state
+ *
+ * Function is static because we use a wrapper to easily capture timing infos.
+ *
+ *
+ * Context locking necessary to avoid concurrent accesses from other CPUs
+ * 	- For per-thread, we must prevent pfm_restart() which works when
+ * 	  context is LOADED or MASKED
+ */
+static void __pfm_interrupt_handler(unsigned long ip, struct pt_regs *regs)
+{
+	struct task_struct *task;
+	struct pfm_context *ctx;
+	struct pfm_event_set *set;
+
+
+	task = __get_cpu_var(pmu_owner);
+	ctx = __get_cpu_var(pmu_ctx);
+
+	/*
+	 * verify if there is a context on this CPU
+	 */
+	if (unlikely(ctx == NULL)) {
+		PFM_DBG_ovfl("no ctx");
+		goto spurious;
+	}
+
+	/*
+	 * we need to lock context because it could be accessed
+	 * from another CPU. Depending on the priority level of
+	 * the PMU interrupt or the arch, it may be necessary to
+	 * mask interrupts alltogether to avoid race condition with
+	 * the timer interrupt in case of time-based set switching,
+	 * for instance.
+	 */
+	spin_lock(&ctx->lock);
+
+	set = ctx->active_set;
+
+	/*
+	 * For SMP per-thread, it is not possible to have
+	 * owner != NULL && task != current.
+	 *
+	 * For UP per-thread, because of lazy save, it
+	 * is possible to receive an interrupt in another task
+	 * which is not using the PMU. This means
+	 * that the interrupt was in-flight at the
+	 * time of pfm_ctxswout_thread(). In that
+	 * case, it will be replayed when the task
+	 * is scheduled again. Hence we convert to spurious.
+	 *
+	 * The basic rule is that an overflow is always
+	 * processed in the context of the task that
+	 * generated it for all per-thread contexts.
+	 *
+	 * for system-wide, task is always NULL
+	 */
+#ifndef CONFIG_SMP
+	if (unlikely((task && current->pfm_context != ctx))) {
+		PFM_DBG_ovfl("spurious: not owned by current task");
+		goto spurious;
+	}
+#endif
+	if (unlikely(ctx->state == PFM_CTX_MASKED)) {
+		PFM_DBG_ovfl("spurious: monitoring masked");
+		goto spurious;
+	}
+
+	/*
+	 * check that monitoring is active, otherwise convert
+	 * to spurious
+	 */
+	if (unlikely(!pfm_arch_is_active(ctx))) {
+		PFM_DBG_ovfl("spurious: monitoring non active");
+		goto spurious;
+	}
+
+	/*
+	 * freeze PMU and collect overflowed PMD registers
+	 * into set->povfl_pmds. Number of overflowed PMDs
+	 * reported in set->npend_ovfls
+	 */
+	pfm_arch_intr_freeze_pmu(ctx, set);
+
+	/*
+	 * no overflow detected, interrupt may have come
+	 * from the previous thread running on this CPU
+	 */
+	if (unlikely(!set->npend_ovfls)) {
+		PFM_DBG_ovfl("no npend_ovfls");
+		goto spurious;
+	}
+
+	pfm_stats_inc(ovfl_intr_regular_count);
+
+	/*
+	 * invoke actual handler
+	 */
+	pfm_overflow_handler(ctx, set, ip, regs);
+
+	/*
+	 * unfreeze PMU, monitoring may not actual be restarted
+	 * if context is MASKED
+	 */
+	pfm_arch_intr_unfreeze_pmu(ctx);
+
+	spin_unlock(&ctx->lock);
+
+	return;
+
+spurious:
+	/* ctx may be NULL */
+	pfm_arch_intr_unfreeze_pmu(ctx);
+	if (ctx)
+		spin_unlock(&ctx->lock);
+
+	pfm_stats_inc(ovfl_intr_spurious_count);
+}
+
+
+/**
+ * pfm_interrupt_handler - 1st level interrupt handler
+ * @ip: interrupt instruction pointer
+ * @regs: machine state
+ *
+ * Function called from the low-level assembly code or arch-specific perfmon
+ * code. Simple wrapper used for timing purpose. Actual work done in
+ * __pfm_overflow_handler()
+ */
+void pfm_interrupt_handler(unsigned long ip, struct pt_regs *regs)
+{
+	u64 start;
+
+	pfm_stats_inc(ovfl_intr_all_count);
+
+	BUG_ON(!irqs_disabled());
+
+	start = sched_clock();
+
+	__pfm_interrupt_handler(ip, regs);
+
+	pfm_stats_add(ovfl_intr_ns, sched_clock() - start);
+}
+EXPORT_SYMBOL(pfm_interrupt_handler);
+
Index: linux-2.6.31-master/perfmon/perfmon_msg.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_msg.c
@@ -0,0 +1,229 @@
+/*
+ * perfmon_msg.c: perfmon2 notification message queue management
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/poll.h>
+#include <linux/perfmon_kern.h>
+
+/**
+ * pfm_get_new_msg - get a new message slot from the queue
+ * @ctx: context to operate on
+ *
+ * if queue if full NULL is returned
+ */
+static union pfarg_msg *pfm_get_new_msg(struct pfm_context *ctx)
+{
+	int next;
+
+	next = ctx->msgq_head & PFM_MSGQ_MASK;
+
+	if ((ctx->msgq_head - ctx->msgq_tail) == PFM_MSGS_COUNT)
+		return NULL;
+
+	/*
+	 * move to next possible slot
+	 */
+	ctx->msgq_head++;
+
+	PFM_DBG_ovfl("head=%d tail=%d msg=%d",
+		ctx->msgq_head & PFM_MSGQ_MASK,
+		ctx->msgq_tail & PFM_MSGQ_MASK,
+		next);
+
+	return ctx->msgq+next;
+}
+
+/**
+ * pfm_notify_user - wakeup any thread wiating on msg queue,  post SIGIO
+ * @ctx: context to operate on
+ *
+ * message is already enqueued
+ */
+static void pfm_notify_user(struct pfm_context *ctx)
+{
+	if (ctx->state == PFM_CTX_ZOMBIE) {
+		PFM_DBG("no notification, context is zombie");
+		return;
+	}
+
+	PFM_DBG_ovfl("waking up");
+
+	wake_up_interruptible(&ctx->msgq_wait);
+
+	/*
+	 * it is safe to call kill_fasync() from an interrupt
+	 * handler. kill_fasync()  grabs two RW locks (fasync_lock,
+	 * tasklist_lock) in read mode. There is conflict only in
+	 * case the PMU interrupt occurs during a write mode critical
+	 * section. This cannot happen because for both locks, the
+	 * write mode is always using interrupt masking (write_lock_irq).
+	 */
+	kill_fasync(&ctx->async_queue, SIGIO, POLL_IN);
+}
+
+/**
+ * pfm_ovfl_notify - send overflow notification
+ * @ctx: context to operate on
+ * @set: which set the overflow comes from
+ * @ip: overflow interrupt instruction address (IIP)
+ *
+ * Appends an overflow notification message to context queue.
+ * call pfm_notify() to wakeup any threads and/or send a signal
+ *
+ * Context is locked and interrupts are disabled (no preemption).
+ */
+int pfm_ovfl_notify(struct pfm_context *ctx,
+			struct pfm_event_set *set,
+			unsigned long ip)
+{
+	union pfarg_msg *msg = NULL;
+	u64 *ovfl_pmds;
+
+	if (!ctx->flags.no_msg) {
+		msg = pfm_get_new_msg(ctx);
+		if (msg == NULL) {
+			/*
+			 * when message queue fills up it is because the user
+			 * did not extract the message, yet issued
+			 * pfm_restart(). At this point, we stop sending
+			 * notification, thus the user will not be able to get
+			 * new samples when using the default format.
+			 */
+			PFM_DBG_ovfl("no more notification msgs");
+			return -1;
+		}
+
+		msg->pfm_ovfl_msg.msg_type = PFM_MSG_OVFL;
+		msg->pfm_ovfl_msg.msg_ovfl_pid = current->tgid;
+		msg->pfm_ovfl_msg.msg_active_set = set->id;
+
+		ovfl_pmds = msg->pfm_ovfl_msg.msg_ovfl_pmds;
+
+		/*
+		 * copy bitmask of all pmd that interrupted last
+		 */
+		bitmap_copy(cast_ulp(ovfl_pmds), cast_ulp(set->ovfl_pmds),
+			    ctx->regs.max_intr_pmd);
+
+		msg->pfm_ovfl_msg.msg_ovfl_cpu = smp_processor_id();
+		msg->pfm_ovfl_msg.msg_ovfl_tid = current->pid;
+		msg->pfm_ovfl_msg.msg_ovfl_ip = ip;
+
+		pfm_stats_inc(ovfl_notify_count);
+	}
+
+	PFM_DBG_ovfl("ip=0x%lx o_pmds=0x%llx",
+		     ip,
+		     (unsigned long long)set->ovfl_pmds[0]);
+
+	pfm_notify_user(ctx);
+	return 0;
+}
+
+/**
+ * pfm_end_notify_user - notify of thread termination
+ * @ctx: context to operate on
+ *
+ * In per-thread mode, when not self-monitoring, perfmon
+ * sends a 'end' notification message when the monitored
+ * thread where the context is attached is exiting.
+ *
+ * This helper message alleviates the need to track the activity
+ * of the thread/process when it is not directly related, i.e.,
+ * was attached. In other words, no needto keep the thread
+ * ptraced.
+ *
+ * The context must be locked and interrupts disabled.
+ */
+int pfm_end_notify(struct pfm_context *ctx)
+{
+	union pfarg_msg *msg;
+
+	msg = pfm_get_new_msg(ctx);
+	if (msg == NULL) {
+		PFM_ERR("%s no more msgs", __func__);
+		return -1;
+	}
+	/* no leak */
+	memset(msg, 0, sizeof(*msg));
+
+	msg->type = PFM_MSG_END;
+
+	PFM_DBG("end msg: msg=%p no_msg=%d",
+		msg,
+		ctx->flags.no_msg);
+
+	pfm_notify_user(ctx);
+	return 0;
+}
+
+/**
+ * pfm_get_next_msg - copy the oldest message from the queue and move tail
+ * @ctx: context to use
+ * @m: where to copy the message into
+ *
+ * The tail of the queue is moved as a consequence of this call
+ */
+void pfm_get_next_msg(struct pfm_context *ctx, union pfarg_msg *m)
+{
+	union pfarg_msg *next;
+
+	PFM_DBG_ovfl("in head=%d tail=%d",
+		ctx->msgq_head & PFM_MSGQ_MASK,
+		ctx->msgq_tail & PFM_MSGQ_MASK);
+
+	/*
+	 * get oldest message
+	 */
+	next = ctx->msgq + (ctx->msgq_tail & PFM_MSGQ_MASK);
+
+	/*
+	 * move tail forward
+	 */
+	ctx->msgq_tail++;
+
+	/*
+	 * copy message, we cannot simply point to it
+	 * as it may be re-used before we copy it out
+	 */
+	*m = *next;
+
+	PFM_DBG_ovfl("out head=%d tail=%d type=%d",
+		ctx->msgq_head & PFM_MSGQ_MASK,
+		ctx->msgq_tail & PFM_MSGQ_MASK,
+		m->type);
+}
Index: linux-2.6.31-master/perfmon/perfmon_pmu.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_pmu.c
@@ -0,0 +1,640 @@
+/*
+ * perfmon_pmu.c: perfmon2 PMU configuration management
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+#ifndef CONFIG_MODULE_UNLOAD
+#define module_refcount(n)	1
+#endif
+
+static __cacheline_aligned_in_smp int request_mod_in_progress;
+static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_pmu_conf_lock);
+
+static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_pmu_acq_lock);
+static u32 pfm_pmu_acquired;
+
+/*
+ * perfmon core must acces PMU information ONLY through pfm_pmu_conf
+ * if pfm_pmu_conf is NULL, then no description is registered
+ */
+struct pfm_pmu_config	*pfm_pmu_conf;
+EXPORT_SYMBOL(pfm_pmu_conf);
+
+static inline int pmu_is_module(struct pfm_pmu_config *c)
+{
+	return !(c->flags & PFM_PMUFL_IS_BUILTIN);
+}
+/**
+ * pfm_pmu_regdesc_init -- initialize regdesc structure from PMU table
+ * @regs: the regdesc structure to initialize
+ * @excl_type: the register type(s) to exclude from this regdesc
+ * @unvail_pmcs: unavailable PMC registers
+ * @unavail_pmds: unavailable PMD registers
+ *
+ * Return:
+ * 	0 success
+ * 	errno in case of error
+ */
+static int pfm_pmu_regdesc_init(struct pfm_regdesc *regs, int excl_type,
+				u64 *unavail_pmcs, u64 *unavail_pmds)
+{
+	struct pfm_regmap_desc *d;
+	u16 n, n2, n_counters, i;
+	int first_intr_pmd = -1, max1, max2, max3;
+
+	/*
+	 * compute the number of implemented PMC from the
+	 * description table
+	 */
+	n = 0;
+	max1 = max2 = -1;
+	d = pfm_pmu_conf->pmc_desc;
+	for (i = 0; i < pfm_pmu_conf->num_pmc_entries;  i++, d++) {
+		if (!(d->type & PFM_REG_I))
+			continue;
+
+		if (test_bit(i, cast_ulp(unavail_pmcs)))
+			continue;
+
+		if (d->type & excl_type)
+			continue;
+
+		__set_bit(i, cast_ulp(regs->pmcs));
+
+		max1 = i;
+		n++;
+	}
+
+	if (!n) {
+		PFM_INFO("%s PMU description has no PMC registers",
+			 pfm_pmu_conf->pmu_name);
+		return -EINVAL;
+	}
+
+	regs->max_pmc = max1 + 1;
+	regs->num_pmcs = n;
+
+	n = n_counters = n2 = 0;
+	max1 = max2 = max3 = -1;
+	d = pfm_pmu_conf->pmd_desc;
+	for (i = 0; i < pfm_pmu_conf->num_pmd_entries;  i++, d++) {
+		if (!(d->type & PFM_REG_I))
+			continue;
+
+		if (test_bit(i, cast_ulp(unavail_pmds)))
+			continue;
+
+		if (d->type & excl_type)
+			continue;
+
+		__set_bit(i, cast_ulp(regs->pmds));
+		max1 = i;
+		n++;
+
+		/*
+		 * read-write registers
+		 */
+		if (!(d->type & PFM_REG_RO)) {
+			__set_bit(i, cast_ulp(regs->rw_pmds));
+			max3 = i;
+			n2++;
+		}
+
+		/*
+		 * counter registers
+		 */
+		if (d->type & PFM_REG_C64) {
+			__set_bit(i, cast_ulp(regs->cnt_pmds));
+			n_counters++;
+		}
+
+		/*
+		 * PMD with intr capabilities
+		 */
+		if (d->type & PFM_REG_INTR) {
+			__set_bit(i, cast_ulp(regs->intr_pmds));
+			if (first_intr_pmd == -1)
+				first_intr_pmd = i;
+			max2 = i;
+		}
+	}
+
+	if (!n) {
+		PFM_INFO("%s PMU description has no PMD registers",
+			 pfm_pmu_conf->pmu_name);
+		return -EINVAL;
+	}
+
+	regs->max_pmd = max1 + 1;
+	regs->first_intr_pmd = first_intr_pmd;
+	regs->max_intr_pmd  = max2 + 1;
+
+	regs->num_counters = n_counters;
+	regs->num_pmds = n;
+	regs->max_rw_pmd = max3 + 1;
+	regs->num_rw_pmd = n2;
+
+	PFM_DBG("intr_pmds=0x%llx cnt_pmds=0x%llx rw_pmds=0x%llx",
+		(unsigned long long)regs->intr_pmds[0],
+		(unsigned long long)regs->cnt_pmds[0],
+		(unsigned long long)regs->rw_pmds[0]);
+
+	return 0;
+}
+
+/**
+ * pfm_pmu_regdesc_init_all -- initialize all regdesc structures
+ * @una_pmcs : unavailable PMC registers
+ * @una_pmds : unavailable PMD registers
+ *
+ * Return:
+ * 	0 sucess
+ * 	errno if error
+ *
+ * We maintain 3 regdesc:
+ * 	regs_all: all available registers
+ * 	regs_sys: registers available to system-wide contexts only
+ * 	regs_thr: registers available to per-thread contexts only
+ */
+static int pfm_pmu_regdesc_init_all(u64 *una_pmcs, u64 *una_pmds)
+{
+	int ret;
+
+	memset(&pfm_pmu_conf->regs_all, 0, sizeof(struct pfm_regdesc));
+	memset(&pfm_pmu_conf->regs_thr, 0, sizeof(struct pfm_regdesc));
+	memset(&pfm_pmu_conf->regs_sys, 0, sizeof(struct pfm_regdesc));
+
+	ret = pfm_pmu_regdesc_init(&pfm_pmu_conf->regs_all,
+				   0,
+				   una_pmcs, una_pmds);
+	if (ret)
+		return ret;
+
+	PFM_DBG("regs_all.pmcs=0x%llx",
+		(unsigned long long)pfm_pmu_conf->regs_all.pmcs[0]);
+
+	ret = pfm_pmu_regdesc_init(&pfm_pmu_conf->regs_thr,
+				   PFM_REG_SYS,
+				   una_pmcs, una_pmds);
+	if (ret)
+		return ret;
+	PFM_DBG("regs.thr.pmcs=0x%llx",
+		(unsigned long long)pfm_pmu_conf->regs_thr.pmcs[0]);
+
+	ret = pfm_pmu_regdesc_init(&pfm_pmu_conf->regs_sys,
+				    PFM_REG_THR,
+				    una_pmcs, una_pmds);
+
+	PFM_DBG("regs_sys.pmcs=0x%llx",
+		(unsigned long long)pfm_pmu_conf->regs_sys.pmcs[0]);
+
+	return ret;
+}
+
+int pfm_pmu_register(struct pfm_pmu_config *cfg)
+{
+	u16 i, nspec, nspec_ro, num_pmcs, num_pmds, num_wc = 0;
+	int type, ret = -EBUSY;
+
+	if (perfmon_disabled) {
+		PFM_INFO("perfmon disabled, cannot add PMU description");
+		return -ENOSYS;
+	}
+
+	nspec = nspec_ro = num_pmds = num_pmcs = 0;
+
+	/* some sanity checks */
+	if (cfg == NULL || cfg->pmu_name == NULL) {
+		PFM_INFO("PMU config descriptor is invalid");
+		return -EINVAL;
+	}
+
+	/* must have a probe */
+	if (cfg->probe_pmu == NULL) {
+		PFM_INFO("PMU config has no probe routine");
+		return -EINVAL;
+	}
+
+	/*
+	 * execute probe routine before anything else as it
+	 * may update configuration tables
+	 */
+	if ((*cfg->probe_pmu)() == -1) {
+		PFM_INFO("%s PMU detection failed", cfg->pmu_name);
+		return -EINVAL;
+	}
+
+	if (!(cfg->flags & PFM_PMUFL_IS_BUILTIN) && cfg->owner == NULL) {
+		PFM_INFO("PMU config %s is missing owner", cfg->pmu_name);
+		return -EINVAL;
+	}
+
+	if (!cfg->num_pmd_entries) {
+		PFM_INFO("%s needs to define num_pmd_entries", cfg->pmu_name);
+		return -EINVAL;
+	}
+
+	if (!cfg->num_pmc_entries) {
+		PFM_INFO("%s needs to define num_pmc_entries", cfg->pmu_name);
+		return -EINVAL;
+	}
+
+	if (!cfg->counter_width) {
+		PFM_INFO("PMU config %s, zero width counters", cfg->pmu_name);
+		return -EINVAL;
+	}
+
+	/*
+	 * REG_RO, REG_V not supported on PMC registers
+	 */
+	for (i = 0; i < cfg->num_pmc_entries;  i++) {
+
+		type = cfg->pmc_desc[i].type;
+
+		if (type & PFM_REG_I)
+			num_pmcs++;
+
+		if (type & PFM_REG_WC)
+			num_wc++;
+
+		if (type & PFM_REG_V) {
+			PFM_INFO("PFM_REG_V is not supported on "
+				 "PMCs (PMC%d)", i);
+			return -EINVAL;
+		}
+		if (type & PFM_REG_RO) {
+			PFM_INFO("PFM_REG_RO meaningless on "
+				 "PMCs (PMC%u)", i);
+			return -EINVAL;
+		}
+	}
+
+	if (num_wc && cfg->pmc_write_check == NULL) {
+		PFM_INFO("some PMCs have write-checker but no callback provided\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * check virtual PMD registers
+	 */
+	num_wc = 0;
+	for (i = 0; i < cfg->num_pmd_entries;  i++) {
+
+		type = cfg->pmd_desc[i].type;
+
+		if (type & PFM_REG_I)
+			num_pmds++;
+
+		if (type & PFM_REG_V) {
+			nspec++;
+			if (type & PFM_REG_RO)
+				nspec_ro++;
+		}
+
+		if (type & PFM_REG_WC)
+			num_wc++;
+	}
+
+	if (num_wc && cfg->pmd_write_check == NULL) {
+		PFM_INFO("PMD have write-checker but no callback provided\n");
+		return -EINVAL;
+	}
+
+	if (nspec && cfg->pmd_sread == NULL) {
+		PFM_INFO("PMU config is missing pmd_sread()");
+		return -EINVAL;
+	}
+
+	nspec = nspec - nspec_ro;
+	if (nspec && cfg->pmd_swrite == NULL) {
+		PFM_INFO("PMU config is missing pmd_swrite()");
+		return -EINVAL;
+	}
+
+	if (num_pmcs >= PFM_MAX_PMCS) {
+		PFM_INFO("%s PMCS registers exceed name space [0-%u]",
+			 cfg->pmu_name,
+			 PFM_MAX_PMCS);
+		return -EINVAL;
+	}
+	if (num_pmds >= PFM_MAX_PMDS) {
+		PFM_INFO("%s PMDS registers exceed name space [0-%u]",
+			 cfg->pmu_name,
+			 PFM_MAX_PMDS);
+		return -EINVAL;
+	}
+	spin_lock(&pfm_pmu_conf_lock);
+
+	if (pfm_pmu_conf)
+		goto unlock;
+
+	if (!cfg->version)
+		cfg->version = "0.0";
+
+	pfm_pmu_conf = cfg;
+	pfm_pmu_conf->ovfl_mask = (1ULL << cfg->counter_width) - 1;
+
+	ret = pfm_arch_pmu_config_init(cfg);
+	if (ret)
+		goto unlock;
+
+	ret = pfm_sysfs_add_pmu(pfm_pmu_conf);
+	if (ret)
+		pfm_pmu_conf = NULL;
+
+unlock:
+	spin_unlock(&pfm_pmu_conf_lock);
+
+	if (ret) {
+		PFM_INFO("register %s PMU error %d", cfg->pmu_name, ret);
+	} else {
+		PFM_INFO("%s PMU installed", cfg->pmu_name);
+		/*
+		 * (re)initialize PMU on each PMU now that we have a description
+		 */
+		on_each_cpu(__pfm_init_percpu, cfg, 1);
+	}
+	return ret;
+}
+EXPORT_SYMBOL(pfm_pmu_register);
+
+/*
+ * remove PMU description. Caller must pass address of current
+ * configuration. This is mostly for sanity checking as only
+ * one config can exist at any time.
+ *
+ * We are using the module refcount mechanism to protect against
+ * removal while the configuration is being used. As long as there is
+ * one context, a PMU configuration cannot be removed. The protection is
+ * managed in module logic.
+ */
+void pfm_pmu_unregister(struct pfm_pmu_config *cfg)
+{
+	if (!(cfg || pfm_pmu_conf))
+		return;
+
+	spin_lock(&pfm_pmu_conf_lock);
+
+	BUG_ON(module_refcount(pfm_pmu_conf->owner));
+
+	if (cfg->owner == pfm_pmu_conf->owner) {
+		pfm_sysfs_remove_pmu(pfm_pmu_conf);
+		pfm_pmu_conf = NULL;
+	}
+
+	spin_unlock(&pfm_pmu_conf_lock);
+}
+EXPORT_SYMBOL(pfm_pmu_unregister);
+
+static int pfm_pmu_request_module(void)
+{
+	char *mod_name;
+	int ret;
+
+	mod_name = pfm_arch_get_pmu_module_name();
+	if (!mod_name)
+		return -ENOSYS;
+
+	ret = request_module("%s", mod_name);
+
+	PFM_DBG("mod=%s ret=%d", mod_name, ret);
+	return ret;
+}
+
+/*
+ * autoload:
+ * 	0     : do not try to autoload the PMU description module
+ * 	not 0 : try to autoload the PMU description module
+ */
+int pfm_pmu_conf_get(int autoload)
+{
+	int ret;
+
+	spin_lock(&pfm_pmu_conf_lock);
+
+	if (request_mod_in_progress) {
+		ret = -ENOSYS;
+		goto skip;
+	}
+
+	if (autoload && pfm_pmu_conf == NULL) {
+
+		request_mod_in_progress = 1;
+
+		spin_unlock(&pfm_pmu_conf_lock);
+
+		pfm_pmu_request_module();
+
+		spin_lock(&pfm_pmu_conf_lock);
+
+		request_mod_in_progress = 0;
+
+		/*
+		 * request_module() may succeed but the module
+		 * may not have registered properly so we need
+		 * to check
+		 */
+	}
+
+	ret = pfm_pmu_conf == NULL ? -ENOSYS : 0;
+	if (!ret && pmu_is_module(pfm_pmu_conf)
+	    && !try_module_get(pfm_pmu_conf->owner))
+		ret = -ENOSYS;
+
+skip:
+	spin_unlock(&pfm_pmu_conf_lock);
+
+	return ret;
+}
+
+void pfm_pmu_conf_put(void)
+{
+	if (pfm_pmu_conf == NULL || !pmu_is_module(pfm_pmu_conf))
+		return;
+
+	spin_lock(&pfm_pmu_conf_lock);
+	module_put(pfm_pmu_conf->owner);
+	spin_unlock(&pfm_pmu_conf_lock);
+}
+
+/*
+ * quiesce the PMU on one CPU
+ */
+static void __pfm_pmu_quiesce_percpu(void *dummy)
+{
+	u64 *mask, val;
+	u16 num, i;
+
+	mask = pfm_pmu_conf->regs_all.pmcs;
+	num = pfm_pmu_conf->regs_all.num_pmcs;
+
+	for (i = 0; num; i++) {
+		if (test_bit(i, cast_ulp(mask))) {
+			val = pfm_pmu_conf->pmc_desc[i].dfl_val;
+			pfm_arch_write_pmc(NULL, i, val);
+			num--;
+		}
+	}
+}
+
+/*
+ * Quiesce the PMU on all CPUs
+ * This is necessary as we have no guarantee the PMU
+ * is actually stopped when perfmon gets control
+ */
+static void pfm_pmu_quiesce(void)
+{
+	on_each_cpu(__pfm_pmu_quiesce_percpu, NULL, 1);
+}
+
+/*
+ * acquire PMU resource from lower-level PMU register allocator
+ * (currently perfctr-watchdog.c)
+ *
+ * acquisition is done when the first context is created (and not
+ * when it is loaded). We grab all that is defined in the description
+ * module and then we make adjustments at the arch-specific level.
+ *
+ * The PMU resource is released when the last perfmon context is
+ * destroyed.
+ *
+ * interrupts are not masked
+ */
+int pfm_pmu_acquire(struct pfm_context *ctx)
+{
+	u64 unavail_pmcs[PFM_PMC_BV];
+	u64 unavail_pmds[PFM_PMD_BV];
+	int ret = 0;
+
+	spin_lock(&pfm_pmu_acq_lock);
+
+	PFM_DBG("pmu_acquired=%u", pfm_pmu_acquired);
+
+	pfm_pmu_acquired++;
+
+	/*
+	 * we need to initialize regdesc each  time we re-acquire
+	 * the PMU for the first time as there may have been changes
+	 * in the list of available registers, e.g., NMI may have
+	 * been disabled. Checking on PMU module insert is not
+	 * enough
+	 */
+	if (pfm_pmu_acquired == 1) {
+
+		memset(unavail_pmcs, 0, sizeof(unavail_pmcs));
+		memset(unavail_pmds, 0, sizeof(unavail_pmds));
+
+		/*
+ 		 * gather unavailable registers
+ 		 *
+ 		 * cannot use pfm_pmu_conf->regs_all as it
+ 		 * is not yet initialized
+ 		 */
+		ret = pfm_arch_reserve_regs(unavail_pmcs, unavail_pmds);
+		if (ret) {
+			pfm_pmu_acquired = 0;
+		} else {
+			pfm_pmu_regdesc_init_all(unavail_pmcs, unavail_pmds);
+
+			/* available PMU ressources */
+			PFM_DBG("PMU acquired: %u PMCs, %u PMDs, %u counters",
+				pfm_pmu_conf->regs_all.num_pmcs,
+				pfm_pmu_conf->regs_all.num_pmds,
+				pfm_pmu_conf->regs_all.num_counters);
+
+			ret = pfm_arch_acquire_pmu();
+			if (ret) {
+				pfm_arch_release_regs();
+				pfm_pmu_acquired = 0;
+			} else
+				pfm_pmu_quiesce();
+		}
+	}
+	spin_unlock(&pfm_pmu_acq_lock);
+
+	/*
+	 * copy the regdesc that corresponds to the context
+	 * we copy and not just point because it helps with
+	 * memory locality. the regdesc structure is accessed
+	 * very frequently in performance critical code such
+	 * as context switch and interrupt handling. By using
+	 * a local copy, we increase memory footprint, but
+	 * increase chance to have local memory access,
+	 * especially for system-wide contexts.
+	 */
+	if (!ret) {
+		if (ctx->flags.system)
+			ctx->regs = pfm_pmu_conf->regs_sys;
+		else
+			ctx->regs = pfm_pmu_conf->regs_thr;
+	}
+	return ret;
+}
+
+/*
+ * release the PMU resource
+ *
+ * actual release happens when last context is destroyed
+ *
+ * interrupts are not masked
+ */
+void pfm_pmu_release(void)
+{
+	BUG_ON(irqs_disabled());
+
+	/*
+	 * we need to use a spinlock because release takes some time
+	 * and we may have a race with pfm_pmu_acquire()
+	 */
+	spin_lock(&pfm_pmu_acq_lock);
+
+	PFM_DBG("pmu_acquired=%d", pfm_pmu_acquired);
+
+	/*
+	 * we decouple test and decrement because if we had errors
+	 * in pfm_pmu_acquire(), we still come here on pfm_context_free()
+	 * but with pfm_pmu_acquire=0
+	 */
+	if (pfm_pmu_acquired > 0 && --pfm_pmu_acquired == 0) {
+		pfm_arch_release_regs();
+		pfm_arch_release_pmu();
+		PFM_DBG("PMU released");
+	}
+	spin_unlock(&pfm_pmu_acq_lock);
+}
Index: linux-2.6.31-master/perfmon/perfmon_priv.h
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_priv.h
@@ -0,0 +1,178 @@
+/*
+ * Copyright (c) 2001-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+
+#ifndef __PERFMON_PRIV_H__
+#define __PERFMON_PRIV_H__
+/*
+ * This file contains all the definitions of data structures, variables, macros
+ * that are to private to the generic code, i.e., not shared with any code that
+ * lives under arch/ or include/asm-XX
+ *
+ * For shared definitions, use include/linux/perfmon_kern.h
+ */
+
+#ifdef CONFIG_PERFMON
+
+/*
+ * type of PMD reset for pfm_reset_pmds() or pfm_switch_sets*()
+ */
+#define PFM_PMD_RESET_SHORT	1	/* use short reset value */
+#define PFM_PMD_RESET_LONG	2	/* use long reset value  */
+
+/*
+ * context lazy save/restore activation count
+ */
+#define PFM_INVALID_ACTIVATION	((u64)~0)
+
+DECLARE_PER_CPU(u64, pmu_activation_number);
+DECLARE_PER_CPU(struct hrtimer, pfm_hrtimer);
+
+static inline void pfm_set_pmu_owner(struct task_struct *task,
+				     struct pfm_context *ctx)
+{
+	__get_cpu_var(pmu_owner) = task;
+	__get_cpu_var(pmu_ctx) = ctx;
+}
+
+static inline int pfm_msgq_is_empty(struct pfm_context *ctx)
+{
+	return ctx->msgq_head == ctx->msgq_tail;
+}
+
+void pfm_get_next_msg(struct pfm_context *ctx, union pfarg_msg *m);
+int pfm_end_notify(struct pfm_context *ctx);
+int pfm_ovfl_notify(struct pfm_context *ctx, struct pfm_event_set *set,
+		    unsigned long ip);
+
+int pfm_alloc_fd(struct pfm_context *ctx);
+
+int __pfm_delete_evtsets(struct pfm_context *ctx, void *arg, int count);
+int __pfm_getinfo_evtsets(struct pfm_context *ctx, struct pfarg_setinfo *req,
+			  int count);
+int __pfm_create_evtsets(struct pfm_context *ctx, struct pfarg_setdesc *req,
+			int count);
+
+
+int pfm_init_ctx(void);
+
+int pfm_pmu_acquire(struct pfm_context *ctx);
+void pfm_pmu_release(void);
+
+int pfm_session_acquire(int is_system, u32 cpu);
+void pfm_session_release(int is_system, u32 cpu);
+
+int pfm_smpl_buf_space_acquire(struct pfm_context *ctx, size_t size);
+int pfm_smpl_buf_load_context(struct pfm_context *ctx);
+void pfm_smpl_buf_unload_context(struct pfm_context *ctx);
+
+int  pfm_init_sysfs(void);
+
+#ifdef CONFIG_PERFMON_DEBUG_FS
+int  pfm_init_debugfs(void);
+int pfm_debugfs_add_cpu(int mycpu);
+void pfm_debugfs_del_cpu(int mycpu);
+#else
+static inline int pfm_init_debugfs(void)
+{
+	return 0;
+}
+static inline int pfm_debugfs_add_cpu(int mycpu)
+{
+	return 0;
+}
+
+static inline void pfm_debugfs_del_cpu(int mycpu)
+{}
+#endif
+
+
+void pfm_reset_pmds(struct pfm_context *ctx, struct pfm_event_set *set,
+		    int num_pmds,
+		    int reset_mode);
+
+struct pfm_event_set *pfm_prepare_sets(struct pfm_context *ctx, u16 load_set);
+int pfm_init_sets(void);
+
+ssize_t pfm_sysfs_res_show(char *buf, size_t sz, int what);
+
+void pfm_free_sets(struct pfm_context *ctx);
+int pfm_create_initial_set(struct pfm_context *ctx);
+void pfm_switch_sets_from_intr(struct pfm_context *ctx);
+void pfm_restart_timer(struct pfm_context *ctx, struct pfm_event_set *set);
+enum hrtimer_restart pfm_handle_switch_timeout(struct hrtimer *t);
+
+enum hrtimer_restart pfm_switch_sets(struct pfm_context *ctx,
+		    struct pfm_event_set *new_set,
+		    int reset_mode,
+		    int no_restart);
+
+/**
+ * pfm_save_prev_ctx - check if previous context exists and save state
+ *
+ * called from pfm_load_ctx_thread() and __pfm_ctxsin_thread() to
+ * check if previous context exists. If so saved its PMU state. This is used
+ * only for UP kernels.
+ *
+ * PMU ownership is not cleared because the function is always called while
+ * trying to install a new owner.
+ */
+static inline void pfm_check_save_prev_ctx(void)
+{
+#ifndef CONFIG_SMP
+	struct pfm_event_set *set;
+	struct pfm_context *ctxp;
+
+	ctxp = __get_cpu_var(pmu_ctx);
+	if (!ctxp)
+		return;
+	/*
+	 * in UP per-thread, due to lazy save
+	 * there could be a context from another
+	 * task. We need to push it first before
+	 * installing our new state
+	 */
+	set = ctxp->active_set;
+	pfm_save_pmds(ctxp, set);
+	/*
+	 * do not clear ownership because we rewrite
+	 * right away
+	 */
+#endif
+}
+
+int pfm_init_hotplug(void);
+
+void pfm_mask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set);
+void pfm_resume_after_ovfl(struct pfm_context *ctx);
+int pfm_setup_smpl_fmt(struct pfm_context *ctx, u32 ctx_flags, void *fmt_arg, int fd);
+
+static inline void pfm_post_work(struct task_struct *task,
+				 struct pfm_context *ctx, int type)
+{
+	ctx->flags.work_type = type;
+	set_tsk_thread_flag(task, TIF_PERFMON_WORK);
+	pfm_arch_arm_handle_work(task);
+}
+
+#define PFM_PMC_STK_ARG	PFM_ARCH_PMC_STK_ARG
+#define PFM_PMD_STK_ARG	PFM_ARCH_PMD_STK_ARG
+
+#endif /* CONFIG_PERFMON */
+
+#endif /* __PERFMON_PRIV_H__ */
Index: linux-2.6.31-master/perfmon/perfmon_res.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_res.c
@@ -0,0 +1,425 @@
+/*
+ * perfmon_res.c:  perfmon2 resource allocations
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+/*
+ * global information about all sessions
+ * mostly used to synchronize between system wide and per-process
+ */
+struct pfm_resources {
+	size_t		smpl_buf_mem_cur;/* current smpl buf mem usage */
+	cpumask_t	sys_cpumask;     /* bitmask of used cpus */
+	u32		thread_sessions; /* #num loaded per-thread sessions */
+};
+
+static struct pfm_resources pfm_res;
+
+static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pfm_res_lock);
+
+/**
+ * pfm_smpl_buf_space_acquire - check memory resource usage for sampling buffer
+ * @ctx: context of interest
+ * @size: size fo requested buffer
+ *
+ * sampling buffer allocated by perfmon must be
+ * checked against max locked memory usage thresholds
+ * for security reasons.
+ *
+ * The first level check is against the system wide limit
+ * as indicated by the system administrator in /sys/kernel/perfmon
+ *
+ * The second level check is on a per-process basis using
+ * RLIMIT_MEMLOCK limit.
+ *
+ * Operating on the current task only.
+ */
+int pfm_smpl_buf_space_acquire(struct pfm_context *ctx, size_t size)
+{
+	struct mm_struct *mm;
+	unsigned long locked;
+	unsigned long buf_mem, buf_mem_max;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pfm_res_lock, flags);
+
+	/*
+	 * check against global buffer limit
+	 */
+	buf_mem_max = pfm_controls.smpl_buffer_mem_max;
+	buf_mem = pfm_res.smpl_buf_mem_cur + size;
+
+	if (buf_mem <= buf_mem_max) {
+		pfm_res.smpl_buf_mem_cur = buf_mem;
+
+		PFM_DBG("buf_mem_max=%lu current_buf_mem=%lu",
+			buf_mem_max,
+			buf_mem);
+	}
+
+	spin_unlock_irqrestore(&pfm_res_lock, flags);
+
+	if (buf_mem > buf_mem_max) {
+		PFM_DBG("smpl buffer memory threshold reached");
+		return -ENOMEM;
+	}
+
+	/*
+	 * check against per-process RLIMIT_MEMLOCK
+	 */
+	mm = get_task_mm(current);
+
+	down_write(&mm->mmap_sem);
+
+	locked  = mm->locked_vm << PAGE_SHIFT;
+	locked += size;
+
+	if (locked > current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur) {
+
+		PFM_DBG("RLIMIT_MEMLOCK reached ask_locked=%lu rlim_cur=%lu",
+			locked,
+			current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur);
+
+		up_write(&mm->mmap_sem);
+		mmput(mm);
+		goto unres;
+	}
+
+	mm->locked_vm = locked >> PAGE_SHIFT;
+
+	up_write(&mm->mmap_sem);
+
+	mmput(mm);
+
+	return 0;
+
+unres:
+	/*
+	 * remove global buffer memory allocation
+	 */
+	spin_lock_irqsave(&pfm_res_lock, flags);
+
+	pfm_res.smpl_buf_mem_cur -= size;
+
+	spin_unlock_irqrestore(&pfm_res_lock, flags);
+
+	return -ENOMEM;
+}
+/**
+ * pfm_smpl_buf_space_release - release resource usage for sampling buffer
+ * @ctx: perfmon context of interest
+ *
+ * There exist multiple paths leading to this function. We need to
+ * be very careful withlokcing on the mmap_sem as it may already be
+ * held by the time we come here.
+ * The following paths exist:
+ *
+ * exit path:
+ * sys_exit_group
+ *    do_group_exit
+ *     do_exit
+ *      exit_mm
+ *       mmput
+ *        exit_mmap
+ *         remove_vma
+ *          fput
+ *           __fput
+ *            pfm_close
+ *             __pfm_close
+ *              pfm_context_free
+ * 	         pfm_release_buf_space
+ * munmap path:
+ * sys_munmap
+ *  do_munmap
+ *   remove_vma
+ *    fput
+ *     __fput
+ *      pfm_close
+ *       __pfm_close
+ *        pfm_context_free
+ *         pfm_release_buf_space
+ *
+ * close path:
+ * sys_close
+ *  filp_close
+ *   fput
+ *    __fput
+ *     pfm_close
+ *      __pfm_close
+ *       pfm_context_free
+ *        pfm_release_buf_space
+ *
+ * The issue is that on the munmap() path, the mmap_sem is already held
+ * in write-mode by the time we come here. To avoid the deadlock, we need
+ * to know where we are coming from and skip down_write(). If is fairly
+ * difficult to know this because of the lack of good hooks and
+ * the fact that, there may not have been any mmap() of the sampling buffer
+ * (i.e. create_context() followed by close() or exit()).
+ *
+ * We use a set flag ctx->flags.mmap_nlock which is toggled in the vm_ops
+ * callback in remove_vma() which is called systematically for the call, so
+ * on all but the pure close() path. The exit path does not already hold
+ * the lock but this is exit so there is no task->mm by the time we come here.
+ *
+ * The mmap_nlock is set only when unmapping and this is the LAST reference
+ * to the file (i.e., close() followed by munmap()).
+ */
+void pfm_smpl_buf_space_release(struct pfm_context *ctx, size_t size)
+{
+	unsigned long flags;
+	struct mm_struct *mm;
+
+	mm = get_task_mm(current);
+	if (mm) {
+		if (ctx->flags.mmap_nlock == 0) {
+			PFM_DBG("doing down_write");
+			down_write(&mm->mmap_sem);
+		}
+
+		mm->locked_vm -= size >> PAGE_SHIFT;
+
+		PFM_DBG("size=%zu locked_vm=%lu", size, mm->locked_vm);
+
+		if (ctx->flags.mmap_nlock == 0)
+			up_write(&mm->mmap_sem);
+
+		mmput(mm);
+	}
+
+	spin_lock_irqsave(&pfm_res_lock, flags);
+
+	pfm_res.smpl_buf_mem_cur -= size;
+
+	spin_unlock_irqrestore(&pfm_res_lock, flags);
+}
+
+/**
+ * pfm_session_acquire - reserve a per-thread or per-cpu session
+ * @is_system: true if per-cpu session
+ * @cpu: cpu number for per-cpu session
+ *
+ * return:
+ * 	 0    : success
+ * 	-EBUSY: if conflicting session exist
+ */
+int pfm_session_acquire(int is_system, u32 cpu)
+{
+	unsigned long flags;
+	u32 nsys_cpus;
+	int ret = 0;
+
+	/*
+	 * validy checks on cpu_mask have been done upstream
+	 */
+	spin_lock_irqsave(&pfm_res_lock, flags);
+
+	nsys_cpus = cpus_weight(pfm_res.sys_cpumask);
+
+	PFM_DBG("in  sys=%u task=%u is_sys=%d cpu=%u",
+		nsys_cpus,
+		pfm_res.thread_sessions,
+		is_system,
+		cpu);
+
+	if (is_system) {
+		/*
+		 * cannot mix system wide and per-task sessions
+		 */
+		if (pfm_res.thread_sessions > 0) {
+			PFM_DBG("%u conflicting thread_sessions",
+				pfm_res.thread_sessions);
+			ret = -EBUSY;
+			goto abort;
+		}
+
+		if (cpu_isset(cpu, pfm_res.sys_cpumask)) {
+			PFM_DBG("conflicting session on CPU%u", cpu);
+			ret = -EBUSY;
+			goto abort;
+		}
+
+		PFM_DBG("reserved session on CPU%u", cpu);
+
+		cpu_set(cpu, pfm_res.sys_cpumask);
+		nsys_cpus++;
+	} else {
+		if (nsys_cpus) {
+			ret = -EBUSY;
+			goto abort;
+		}
+		pfm_res.thread_sessions++;
+	}
+
+	PFM_DBG("out sys=%u task=%u is_sys=%d cpu=%u",
+		nsys_cpus,
+		pfm_res.thread_sessions,
+		is_system,
+		cpu);
+
+abort:
+	spin_unlock_irqrestore(&pfm_res_lock, flags);
+
+	return ret;
+}
+
+/**
+ * pfm_session_release - release a per-cpu or per-thread session
+ * @is_system: true if per-cpu session
+ * @cpu: cpu number for per-cpu session
+ *
+ * called from __pfm_unload_context()
+ */
+void pfm_session_release(int is_system, u32 cpu)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&pfm_res_lock, flags);
+
+	PFM_DBG("in sys_sessions=%u thread_sessions=%u syswide=%d cpu=%u",
+		cpus_weight(pfm_res.sys_cpumask),
+		pfm_res.thread_sessions,
+		is_system, cpu);
+
+	if (is_system)
+		cpu_clear(cpu, pfm_res.sys_cpumask);
+	else
+		pfm_res.thread_sessions--;
+
+	PFM_DBG("out sys_sessions=%u thread_sessions=%u syswide=%d cpu=%u",
+		cpus_weight(pfm_res.sys_cpumask),
+		pfm_res.thread_sessions,
+		is_system, cpu);
+
+	spin_unlock_irqrestore(&pfm_res_lock, flags);
+}
+
+/**
+ * pfm_session_allcpus_acquire - acquire per-cpu sessions on all available cpus
+ *
+ * currently used by Oprofile on X86
+ */
+int pfm_session_allcpus_acquire(void)
+{
+	unsigned long flags;
+	int ret = -EBUSY;
+
+	spin_lock_irqsave(&pfm_res_lock, flags);
+
+	if (!cpus_empty(pfm_res.sys_cpumask)) {
+		PFM_DBG("already some system-wide sessions");
+		goto abort;
+	}
+	/*
+	 * cannot mix system wide and per-task sessions
+	 */
+	if (pfm_res.thread_sessions) {
+		PFM_DBG("%u conflicting thread_sessions",
+			pfm_res.thread_sessions);
+		goto abort;
+	}
+	/*
+	 * we need to set all bits to avoid issues
+	 * with HOTPLUG, and cpus showing up while
+	 * there is already an allcpu session
+	 */
+	cpus_setall(pfm_res.sys_cpumask);
+
+	ret = 0;
+abort:
+	spin_unlock_irqrestore(&pfm_res_lock, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL(pfm_session_allcpus_acquire);
+
+/**
+ * pfm_session_allcpus_release - relase per-cpu sessions on all cpus
+ *
+ * currently used by Oprofile code
+ */
+void pfm_session_allcpus_release(void)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&pfm_res_lock, flags);
+
+	cpus_clear(pfm_res.sys_cpumask);
+
+	spin_unlock_irqrestore(&pfm_res_lock, flags);
+}
+EXPORT_SYMBOL(pfm_session_allcpus_release);
+
+/**
+ * pfm_sysfs_res_show - return currnt resourcde usage for sysfs
+ * @buf: buffer to hold string in return
+ * @sz: size of buf
+ * @what: what to produce
+ *        what=0 : thread_sessions
+ *        what=1 : cpus_weight(sys_cpumask)
+ *        what=2 : smpl_buf_mem_cur
+ *        what=3 : pmu model name
+ *
+ * called from perfmon_sysfs.c
+ * return number of bytes written into buf (up to sz)
+ */
+ssize_t pfm_sysfs_res_show(char *buf, size_t sz, int what)
+{
+	unsigned long flags;
+	cpumask_t mask;
+
+	spin_lock_irqsave(&pfm_res_lock, flags);
+
+	switch (what) {
+	case 0: snprintf(buf, sz, "%u\n", pfm_res.thread_sessions);
+		break;
+	case 1:
+		cpus_and(mask, pfm_res.sys_cpumask, cpu_online_map);
+		snprintf(buf, sz, "%d\n", cpus_weight(mask));
+		break;
+	case 2: snprintf(buf, sz, "%zu\n", pfm_res.smpl_buf_mem_cur);
+		break;
+	case 3:
+		snprintf(buf, sz, "%s\n",
+			pfm_pmu_conf ?	pfm_pmu_conf->pmu_name
+				     :	"unknown\n");
+	}
+	spin_unlock_irqrestore(&pfm_res_lock, flags);
+	return strlen(buf);
+}
Index: linux-2.6.31-master/perfmon/perfmon_rw.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_rw.c
@@ -0,0 +1,643 @@
+/*
+ * perfmon.c: perfmon2 PMC/PMD read/write system calls
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net/
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+#define PFM_REGFL_PMC_ALL	(PFM_REGFL_NO_EMUL64)
+#define PFM_REGFL_PMD_ALL	(PFM_REGFL_RANDOM|PFM_REGFL_OVFL_NOTIFY)
+
+/**
+ * handle_dep_pmcs - update used_pmds based on dep_pmcs for the pmd
+ * @ctx: context to use
+ * @set: set to use
+ * @cnum: PMD to use
+ *
+ * return:
+ * 	0 : success
+ * 	<0: on error (errno)
+ */
+static int handle_dep_pmcs(struct pfm_context *ctx, struct pfm_event_set *set,
+			   u16 cnum)
+{
+	struct pfarg_pmc req;
+	u64 *dep_pmcs;
+	int n, p, q, ret = 0;
+
+	dep_pmcs = pfm_pmu_conf->pmd_desc[cnum].dep_pmcs;
+	n = bitmap_weight(cast_ulp(dep_pmcs), ctx->regs.max_pmc);
+
+	memset(&req, 0, sizeof(req));
+
+	for(p = 0; n; n--, p = q+1) {
+		q = find_next_bit(cast_ulp(dep_pmcs), ctx->regs.max_pmc, p);
+
+		if (test_bit(q, cast_ulp(set->used_pmcs)))
+			continue;
+
+		req.reg_num = q;
+		req.reg_value = set->pmcs[q]; /* default value */
+
+		ret = __pfm_write_pmcs(ctx, &req, 1);
+		if (ret)
+			break;
+	}
+	return ret;
+}
+
+/**
+ * handle_smpl_bv - checks sampling bitmasks for new PMDs
+ * @ctx: context to use
+ * @set: set to use
+ * @bv: sampling bitmask
+ *
+ * scans the smpl bitmask looking for new PMDs (not yet used), if found
+ * invoke pfm_write_pmds() on them to get them initialized and marked used
+ *
+ * return:
+ * 	0 : success
+ * 	<0: error (errno)
+ */
+static int handle_smpl_bv(struct pfm_context *ctx, struct pfm_event_set *set,
+			  unsigned long *bv)
+{
+	struct pfarg_pmd req;
+	int p, q, n, ret = 0;
+	u16 max_pmd;
+
+	memset(&req, 0, sizeof(req));
+
+	max_pmd = ctx->regs.max_pmd;
+
+	n = bitmap_weight(cast_ulp(bv), max_pmd);
+
+	for(p = 0; n; n--, p = q+1) {
+		q = find_next_bit(cast_ulp(bv), max_pmd, p);
+
+		if (test_bit(q, cast_ulp(set->used_pmds)))
+			continue;
+
+		req.reg_num = q;
+		req.reg_value = 0;
+
+		ret = __pfm_write_pmds(ctx, &req, 1, 0);
+		if (ret)
+			break;
+	}
+	return ret;
+}
+
+/**
+ * is_invalid -- check if register index is within limits
+ * @cnum: register index
+ * @impl: bitmask of implemented registers
+ * @max: highest implemented registers + 1
+ *
+ * return:
+ *    0 is register index is valid
+ *    1 if invalid
+ */
+static inline int is_invalid(u16 cnum, unsigned long *impl, u16 max)
+{
+	return cnum >= max || !test_bit(cnum, impl);
+}
+
+/**
+ * __pfm_write_pmds - modified data registers
+ * @ctx: context to operate on
+ * @req: pfarg_pmd_t request from user
+ * @count: number of element in the pfarg_pmd_t vector
+ * @compat: used only on IA-64 to maintain backward compatibility with v2.0
+ *
+ * The function succeeds whether the context is attached or not.
+ * When attached to another thread, that thread must be stopped.
+ *
+ * The context is locked and interrupts are disabled.
+ */
+int __pfm_write_pmds(struct pfm_context *ctx, struct pfarg_pmd *req, int count,
+		     int compat)
+{
+	struct pfm_event_set *set, *active_set;
+	unsigned long *smpl_pmds, *reset_pmds, *impl_pmds, *impl_rw_pmds;
+	u32 req_flags;
+	u16 cnum, pmd_type, max_pmd;
+	u16 set_id;
+	int i, can_access_pmu;
+	int ret;
+	pfm_pmd_check_t	wr_func;
+
+	active_set = ctx->active_set;
+	max_pmd	= ctx->regs.max_pmd;
+	impl_pmds = cast_ulp(ctx->regs.pmds);
+	impl_rw_pmds = cast_ulp(ctx->regs.rw_pmds);
+	wr_func = pfm_pmu_conf->pmd_write_check;
+	set = list_first_entry(&ctx->set_list, struct pfm_event_set, list);
+
+	can_access_pmu = 0;
+
+	/*
+	 * we cannot access the actual PMD registers when monitoring is masked
+	 */
+	if (unlikely(ctx->state == PFM_CTX_LOADED))
+		can_access_pmu = __get_cpu_var(pmu_owner) == ctx->task
+			|| ctx->flags.system;
+
+	ret = -EINVAL;
+	for (i = 0; i < count; i++, req++) {
+
+		cnum = req->reg_num;
+		set_id = req->reg_set;
+		req_flags = req->reg_flags;
+		smpl_pmds = cast_ulp(req->reg_smpl_pmds);
+		reset_pmds = cast_ulp(req->reg_reset_pmds);
+
+		/*
+		 * cannot write to unexisting
+		 * writes to read-only register are ignored
+		 */
+		if (unlikely(is_invalid(cnum, impl_pmds, max_pmd))) {
+			PFM_DBG("pmd%u is not available", cnum);
+			goto error;
+		}
+
+		pmd_type = pfm_pmu_conf->pmd_desc[cnum].type;
+
+		/*
+		 * ensure only valid flags are set
+		 */
+		if (req_flags & ~PFM_REGFL_PMD_ALL) {
+			PFM_DBG("pmd%u: invalid flags=0x%x",
+				cnum, req_flags);
+			goto error;
+		}
+
+		/*
+		 * verify validity of smpl_pmds
+		 */
+		if (unlikely(!bitmap_subset(smpl_pmds, impl_pmds, PFM_MAX_PMDS))) {
+			PFM_DBG("invalid smpl_pmds=0x%llx for pmd%u",
+				(unsigned long long)req->reg_smpl_pmds[0],
+				cnum);
+			goto error;
+		}
+
+		/*
+		 * verify validity of reset_pmds
+		 * check against impl_rw_pmds because it is not
+		 * possible to reset read-only PMDs
+		 */
+		if (unlikely(!bitmap_subset(reset_pmds, impl_rw_pmds, PFM_MAX_PMDS))) {
+			PFM_DBG("invalid reset_pmds=0x%llx for pmd%u",
+				(unsigned long long)req->reg_reset_pmds[0],
+				cnum);
+			goto error;
+		}
+
+		/*
+		 * locate event set
+		 */
+		if (set_id != set->id) {
+			set = pfm_find_set(ctx, set_id, 0);
+			if (set == NULL) {
+				PFM_DBG("event set%u does not exist",
+						set_id);
+				goto error;
+			}
+		}
+
+		/*
+		 * execute write checker, if any
+		 */
+		if (unlikely(wr_func && (pmd_type & PFM_REG_WC))) {
+			ret = (*wr_func)(ctx, set, req);
+			if (ret)
+				goto error;
+
+		}
+
+		ret = handle_dep_pmcs(ctx, set, cnum);
+		if (ret)
+			goto error;
+
+		if (unlikely(compat))
+			goto skip_set;
+
+		if (bitmap_weight(smpl_pmds, max_pmd)) {
+			ret = handle_smpl_bv(ctx, set, smpl_pmds);
+			if (ret)
+				goto error;
+		}
+
+		if (bitmap_weight(reset_pmds, max_pmd)) {
+			ret = handle_smpl_bv(ctx, set, reset_pmds);
+			if (ret)
+				goto error;
+		}
+
+		/*
+		 * now commit changes to software state
+		 */
+		bitmap_copy(cast_ulp(set->pmds[cnum].smpl_pmds),
+			    smpl_pmds,
+			    max_pmd);
+
+		bitmap_copy(cast_ulp(set->pmds[cnum].reset_pmds),
+			   reset_pmds,
+			   max_pmd);
+
+		set->pmds[cnum].flags = req_flags;
+
+		__set_bit(cnum, cast_ulp(set->used_pmds));
+
+		/*
+		 * we reprogram the PMD hence, we clear any pending
+		 * ovfl. Does affect ovfl switch on restart but new
+		 * value has already been established here
+		 */
+		if (test_bit(cnum, cast_ulp(set->povfl_pmds))) {
+			set->npend_ovfls--;
+			__clear_bit(cnum, cast_ulp(set->povfl_pmds));
+		}
+		__clear_bit(cnum, cast_ulp(set->ovfl_pmds));
+
+		/*
+		 * update ovfl_notify
+		 */
+		if (req_flags & PFM_REGFL_OVFL_NOTIFY)
+			__set_bit(cnum, cast_ulp(set->ovfl_notify));
+		else
+			__clear_bit(cnum, cast_ulp(set->ovfl_notify));
+
+		/*
+		 * establish new switch count
+		 */
+		set->pmds[cnum].ovflsw_thres = req->reg_ovfl_switch_cnt;
+		set->pmds[cnum].ovflsw_ref_thres = req->reg_ovfl_switch_cnt;
+skip_set:
+
+		/*
+		 * set last value to new value for all types of PMD
+		 */
+		set->pmds[cnum].lval = req->reg_value;
+		set->pmds[cnum].value = req->reg_value;
+
+		/*
+		 * update reset values (not just for counters)
+		 */
+		set->pmds[cnum].long_reset = req->reg_long_reset;
+		set->pmds[cnum].short_reset = req->reg_short_reset;
+
+		/*
+		 * update randomization mask
+		 */
+		set->pmds[cnum].mask = req->reg_random_mask;
+
+		set->pmds[cnum].eventid = req->reg_smpl_eventid;
+
+		if (set == active_set) {
+			set->priv_flags |= PFM_SETFL_PRIV_MOD_PMDS;
+			if (can_access_pmu)
+				pfm_write_pmd(ctx, cnum, req->reg_value);
+		}
+
+
+		PFM_DBG("set%u pmd%u=0x%llx flags=0x%x a_pmu=%d "
+			"ctx_pmd=0x%llx s_reset=0x%llx "
+			"l_reset=0x%llx s_pmds=0x%llx "
+			"r_pmds=0x%llx o_pmds=0x%llx "
+			"o_thres=%llu compat=%d eventid=%llx",
+			set->id,
+			cnum,
+			(unsigned long long)req->reg_value,
+			set->pmds[cnum].flags,
+			can_access_pmu,
+			(unsigned long long)set->pmds[cnum].value,
+			(unsigned long long)set->pmds[cnum].short_reset,
+			(unsigned long long)set->pmds[cnum].long_reset,
+			(unsigned long long)set->pmds[cnum].smpl_pmds[0],
+			(unsigned long long)set->pmds[cnum].reset_pmds[0],
+			(unsigned long long)set->ovfl_pmds[0],
+			(unsigned long long)set->pmds[cnum].ovflsw_thres,
+			compat,
+			(unsigned long long)set->pmds[cnum].eventid);
+	}
+	ret = 0;
+error:
+	/*
+	 * make changes visible
+	 */
+	if (can_access_pmu)
+		pfm_arch_serialize();
+
+	return ret;
+}
+
+/**
+ * __pfm_write_pmcs - modified config registers
+ * @ctx: context to operate on
+ * @req: pfarg_pmc_t request from user
+ * @count: number of element in the pfarg_pmc_t vector
+ *
+ *
+ * The function succeeds whether the context is * attached or not.
+ * When attached to another thread, that thread must be stopped.
+ *
+ * The context is locked and interrupts are disabled.
+ */
+int __pfm_write_pmcs(struct pfm_context *ctx, struct pfarg_pmc *req, int count)
+{
+	struct pfm_event_set *set, *active_set;
+	u64 value, dfl_val, rsvd_msk;
+	unsigned long *impl_pmcs;
+	int i, can_access_pmu;
+	int ret;
+	u16 set_id;
+	u16 cnum, pmc_type, max_pmc;
+	u32 flags, expert;
+	pfm_pmc_check_t	wr_func;
+
+	active_set = ctx->active_set;
+
+	wr_func = pfm_pmu_conf->pmc_write_check;
+	max_pmc = ctx->regs.max_pmc;
+	impl_pmcs = cast_ulp(ctx->regs.pmcs);
+	set = list_first_entry(&ctx->set_list, struct pfm_event_set, list);
+
+	expert = pfm_controls.flags & PFM_CTRL_FL_RW_EXPERT;
+
+	can_access_pmu = 0;
+
+	/*
+	 * we cannot access the actual PMC registers when monitoring is masked
+	 */
+	if (unlikely(ctx->state == PFM_CTX_LOADED))
+		can_access_pmu = __get_cpu_var(pmu_owner) == ctx->task
+			|| ctx->flags.system;
+
+	ret = -EINVAL;
+
+	for (i = 0; i < count; i++, req++) {
+
+		cnum = req->reg_num;
+		set_id = req->reg_set;
+		value = req->reg_value;
+		flags = req->reg_flags;
+
+		/*
+		 * no access to unavailable PMC register
+		 */
+		if (unlikely(is_invalid(cnum, impl_pmcs, max_pmc))) {
+			PFM_DBG("pmc%u is not available", cnum);
+			goto error;
+		}
+
+		pmc_type = pfm_pmu_conf->pmc_desc[cnum].type;
+		dfl_val = pfm_pmu_conf->pmc_desc[cnum].dfl_val;
+		rsvd_msk = pfm_pmu_conf->pmc_desc[cnum].rsvd_msk;
+
+		/*
+		 * ensure only valid flags are set
+		 */
+		if (flags & ~PFM_REGFL_PMC_ALL) {
+			PFM_DBG("pmc%u: invalid flags=0x%x", cnum, flags);
+			goto error;
+		}
+
+		/*
+		 * locate event set
+		 */
+		if (set_id != set->id) {
+			set = pfm_find_set(ctx, set_id, 0);
+			if (set == NULL) {
+				PFM_DBG("event set%u does not exist",
+					set_id);
+				goto error;
+			}
+		}
+
+		/*
+		 * set reserved bits to default values
+		 * (reserved bits must be 1 in rsvd_msk)
+		 *
+		 * bypass via /sys/kernel/perfmon/mode = 1
+		 */
+		if (likely(!expert))
+			value = (value & ~rsvd_msk) | (dfl_val & rsvd_msk);
+
+		if (flags & PFM_REGFL_NO_EMUL64) {
+			if (!(pmc_type & PFM_REG_NO64)) {
+				PFM_DBG("pmc%u no support for "
+					"PFM_REGFL_NO_EMUL64", cnum);
+				goto error;
+			}
+			value &= ~pfm_pmu_conf->pmc_desc[cnum].no_emul64_msk;
+		}
+
+		/*
+		 * execute write checker, if any
+		 */
+		if (likely(wr_func && (pmc_type & PFM_REG_WC))) {
+			req->reg_value = value;
+			ret = (*wr_func)(ctx, set, req);
+			if (ret)
+				goto error;
+			value = req->reg_value;
+		}
+
+		/*
+		 * Now we commit the changes
+		 */
+
+		/*
+		 * mark PMC register as used
+		 * We do not track associated PMC register based on
+		 * the fact that they will likely need to be written
+		 * in order to become useful at which point the statement
+		 * below will catch that.
+		 *
+		 * The used_pmcs bitmask is only useful on architectures where
+		 * the PMC needs to be modified for particular bits, especially
+		 * on overflow or to stop/start.
+		 */
+		if (!test_bit(cnum, cast_ulp(set->used_pmcs)))
+			__set_bit(cnum, cast_ulp(set->used_pmcs));
+
+		set->pmcs[cnum] = value;
+
+		if (set == active_set) {
+			set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
+			if (can_access_pmu)
+				pfm_arch_write_pmc(ctx, cnum, value);
+		}
+
+		PFM_DBG("set%u pmc%u=0x%llx a_pmu=%d "
+			"u_pmcs=0x%llx",
+			set->id,
+			cnum,
+			(unsigned long long)value,
+			can_access_pmu,
+			(unsigned long long)set->used_pmcs[0]);
+	}
+	ret = 0;
+error:
+	/*
+	 * make sure the changes are visible
+	 */
+	if (can_access_pmu)
+		pfm_arch_serialize();
+
+	return ret;
+}
+
+/**
+ * __pfm_read_pmds - read data registers
+ * @ctx: context to operate on
+ * @req: pfarg_pmd_t request from user
+ * @count: number of element in the pfarg_pmd_t vector
+ *
+ *
+ * The function succeeds whether the context is attached or not.
+ * When attached to another thread, that thread must be stopped.
+ *
+ * The context is locked and interrupts are disabled.
+ */
+int __pfm_read_pmds(struct pfm_context *ctx, struct pfarg_pmd *req, int count)
+{
+	u64 val = 0, lval, ovfl_mask, hw_val;
+	u64 sw_cnt;
+	unsigned long *impl_pmds;
+	struct pfm_event_set *set, *active_set;
+	int i, ret, can_access_pmu = 0;
+	u16 cnum, pmd_type, set_id, max_pmd;
+
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+	impl_pmds = cast_ulp(ctx->regs.pmds);
+	max_pmd   = ctx->regs.max_pmd;
+	active_set = ctx->active_set;
+	set = list_first_entry(&ctx->set_list, struct pfm_event_set, list);
+
+	if (likely(ctx->state == PFM_CTX_LOADED)) {
+		can_access_pmu = __get_cpu_var(pmu_owner) == ctx->task
+			|| ctx->flags.system;
+
+		if (can_access_pmu)
+			pfm_arch_serialize();
+	}
+
+	/*
+	 * on both UP and SMP, we can only read the PMD from the hardware
+	 * register when the task is the owner of the local PMU.
+	 */
+	ret = -EINVAL;
+	for (i = 0; i < count; i++, req++) {
+
+		cnum = req->reg_num;
+		set_id = req->reg_set;
+
+		if (unlikely(is_invalid(cnum, impl_pmds, max_pmd))) {
+			PFM_DBG("pmd%u is not implemented/unaccessible", cnum);
+			goto error;
+		}
+
+		pmd_type = pfm_pmu_conf->pmd_desc[cnum].type;
+
+		/*
+		 * locate event set
+		 */
+		if (set_id != set->id) {
+			set = pfm_find_set(ctx, set_id, 0);
+			if (set == NULL) {
+				PFM_DBG("event set%u does not exist",
+					set_id);
+				goto error;
+			}
+		}
+		/*
+		 * it is not possible to read a PMD which was not requested:
+		 * 	- explicitly written via pfm_write_pmds()
+		 * 	- provided as a reg_smpl_pmds[] to another PMD during
+		 * 	  pfm_write_pmds()
+		 *
+		 * This is motivated by security and for optimization purposes:
+		 * 	- on context switch restore, we can restore only what
+		 * 	  we use (except when regs directly readable at user
+		 * 	  level, e.g., IA-64 self-monitoring, I386 RDPMC).
+		 * 	- do not need to maintain PMC -> PMD dependencies
+		 */
+		if (unlikely(!test_bit(cnum, cast_ulp(set->used_pmds)))) {
+			PFM_DBG("pmd%u cannot read, because not used", cnum);
+			goto error;
+		}
+
+		val = set->pmds[cnum].value;
+		lval = set->pmds[cnum].lval;
+
+		/*
+		 * extract remaining ovfl to switch
+		 */
+		sw_cnt = set->pmds[cnum].ovflsw_thres;
+
+		/*
+		 * If the task is not the current one, then we check if the
+		 * PMU state is still in the local live register due to lazy
+		 * ctxsw. If true, then we read directly from the registers.
+		 */
+		if (set == active_set && can_access_pmu) {
+			hw_val = pfm_read_pmd(ctx, cnum);
+			if (pmd_type & PFM_REG_C64)
+				val = (val & ~ovfl_mask) | (hw_val & ovfl_mask);
+			else
+				val = hw_val;
+		}
+
+		PFM_DBG("set%u pmd%u=0x%llx sw_thr=%llu lval=0x%llx",
+			set->id,
+			cnum,
+			(unsigned long long)val,
+			(unsigned long long)sw_cnt,
+			(unsigned long long)lval);
+
+		req->reg_value = val;
+		req->reg_last_reset_val = lval;
+		req->reg_ovfl_switch_cnt = sw_cnt;
+	}
+	ret = 0;
+error:
+	return ret;
+}
Index: linux-2.6.31-master/perfmon/perfmon_sets.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_sets.c
@@ -0,0 +1,873 @@
+/*
+ * perfmon_sets.c: perfmon2 event sets and multiplexing functions
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+static struct kmem_cache	*pfm_set_cachep;
+
+/**
+ * pfm_reload_switch_thresholds - reload overflow-based switch thresholds per set
+ * @set: the set for which to reload thresholds
+ *
+ */
+static void pfm_reload_switch_thresholds(struct pfm_context *ctx,
+					 struct pfm_event_set *set)
+{
+	u64 *used_pmds;
+	u16 i, max, first;
+
+	used_pmds = set->used_pmds;
+	first = ctx->regs.first_intr_pmd;
+	max = ctx->regs.max_intr_pmd;
+
+	for (i = first; i < max; i++) {
+		if (test_bit(i, cast_ulp(used_pmds))) {
+			set->pmds[i].ovflsw_thres = set->pmds[i].ovflsw_ref_thres;
+
+			PFM_DBG("set%u pmd%u ovflsw_thres=%llu",
+				set->id,
+				i,
+				(unsigned long long)set->pmds[i].ovflsw_thres);
+		}
+	}
+}
+
+/**
+ * pfm_prepare_sets - initialize sets on pfm_load_context
+ * @ctx : context to operate on
+ * @load_set: set to activate first
+ *
+ * connect all sets, reset internal fields
+ */
+struct pfm_event_set *pfm_prepare_sets(struct pfm_context *ctx, u16 load_set)
+{
+	struct pfm_event_set *set, *p;
+	u16 max;
+
+	/*
+	 * locate first set to activate
+	 */
+	set = pfm_find_set(ctx, load_set, 0);
+	if (!set)
+		return NULL;
+
+	if (set->flags & PFM_SETFL_OVFL_SWITCH)
+		pfm_reload_switch_thresholds(ctx, set);
+
+	max = ctx->regs.max_intr_pmd;
+
+	list_for_each_entry(p, &ctx->set_list, list) {
+		/*
+		 * cleanup bitvectors
+		 */
+		bitmap_zero(cast_ulp(p->ovfl_pmds), max);
+		bitmap_zero(cast_ulp(p->povfl_pmds), max);
+
+		p->npend_ovfls = 0;
+
+		/*
+		 * we cannot just use plain clear because of arch-specific flags
+		 */
+		p->priv_flags &= ~(PFM_SETFL_PRIV_MOD_BOTH|PFM_SETFL_PRIV_SWITCH);
+		/*
+		 * neither duration nor runs are reset because typically loading/unloading
+		 * does not mean counts are reset. To reset, the set must be modified
+		 */
+	}
+	return set;
+}
+
+/*
+ * called by hrtimer_interrupt()
+ *
+ * This is the only function where we come with
+ * cpu_base->lock held before ctx->lock
+ *
+ * interrupts are disabled
+ */
+enum hrtimer_restart pfm_handle_switch_timeout(struct hrtimer *t)
+{
+	struct pfm_event_set *set;
+	struct pfm_context *ctx;
+	unsigned long flags;
+	enum hrtimer_restart ret = HRTIMER_NORESTART;
+
+	/*
+	 * prevent against race with unload
+	 */
+	ctx  = __get_cpu_var(pmu_ctx);
+	if (!ctx)
+		return HRTIMER_NORESTART;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	set = ctx->active_set;
+
+	/*
+	 * switching occurs only when context is attached
+	 */
+	if (ctx->state != PFM_CTX_LOADED)
+		goto done;
+	/*
+	 * timer does not run while monitoring is inactive (not started)
+	 */
+	if (!pfm_arch_is_active(ctx))
+		goto done;
+
+	pfm_stats_inc(handle_timeout_count);
+
+	ret  = pfm_switch_sets(ctx, NULL, PFM_PMD_RESET_SHORT, 0);
+done:
+	spin_unlock_irqrestore(&ctx->lock, flags);
+	return ret;
+}
+
+/*
+ *
+ * always operating on the current task
+ * interrupts are masked
+ *
+ * input:
+ * 	- new_set: new set to switch to, if NULL follow normal chain
+ */
+enum hrtimer_restart pfm_switch_sets(struct pfm_context *ctx,
+				     struct pfm_event_set *new_set,
+				     int reset_mode,
+				     int no_restart)
+{
+	struct pfm_event_set *set;
+	u64 now, end;
+	u32 new_flags;
+	int is_system, is_active, nn;
+	enum hrtimer_restart ret = HRTIMER_NORESTART;
+
+	now = sched_clock();
+	set = ctx->active_set;
+	is_active = pfm_arch_is_active(ctx);
+
+	/*
+	 * if no set is explicitly requested,
+	 * use the set_switch_next field
+	 */
+	if (!new_set) {
+		/*
+		 * we use round-robin unless the user specified
+		 * a particular set to go to.
+		 */
+		new_set = list_first_entry(&set->list, struct pfm_event_set, list);
+		if (&new_set->list == &ctx->set_list)
+			new_set = list_first_entry(&ctx->set_list, struct pfm_event_set, list);
+	}
+
+	PFM_DBG_ovfl("state=%d act=%d cur_set=%u cur_runs=%llu cur_npend=%d next_set=%u "
+		  "next_runs=%llu new_npend=%d reset_mode=%d reset_pmds=%llx",
+		  ctx->state,
+		  is_active,
+		  set->id,
+		  (unsigned long long)set->runs,
+		  set->npend_ovfls,
+		  new_set->id,
+		  (unsigned long long)new_set->runs,
+		  new_set->npend_ovfls,
+		  reset_mode,
+		  (unsigned long long)new_set->reset_pmds[0]);
+
+	is_system = ctx->flags.system;
+	new_flags = new_set->flags;
+
+	/*
+	 * nothing more to do
+	 */
+	if (new_set == set)
+		goto skip_same_set;
+
+	if (is_active) {
+		pfm_arch_stop(current, ctx);
+		pfm_save_pmds(ctx, set);
+		/*
+		 * compute elapsed ns for active set
+		 */
+		set->duration += now - set->duration_start;
+	}
+
+	pfm_arch_restore_pmds(ctx, new_set);
+	/*
+	 * if masked, we must restore the pmcs such that they
+	 * do not capture anything.
+	 */
+	pfm_arch_restore_pmcs(ctx, new_set);
+
+	if (new_set->npend_ovfls) {
+		pfm_arch_resend_irq(ctx);
+		pfm_stats_inc(ovfl_intr_replay_count);
+	}
+
+	new_set->priv_flags &= ~PFM_SETFL_PRIV_MOD_BOTH;
+
+skip_same_set:
+	new_set->runs++;
+	/*
+	 * reset switch threshold
+	 */
+	if (new_flags & PFM_SETFL_OVFL_SWITCH)
+		pfm_reload_switch_thresholds(ctx, new_set);
+
+	/*
+	 * reset overflowed PMD registers in new set
+	 */
+	nn = bitmap_weight(cast_ulp(new_set->reset_pmds), ctx->regs.max_pmd);
+	if (nn)
+		pfm_reset_pmds(ctx, new_set, nn, reset_mode);
+
+
+	/*
+	 * This is needed when coming from pfm_start()
+	 *
+	 * When switching to the same set, there is no
+	 * need to restart
+	 */
+	if (no_restart)
+		goto skip_restart;
+
+	if (is_active) {
+		/*
+		 * do not need to restart when same set
+		 */
+		if (new_set != set) {
+			ctx->active_set = new_set;
+			new_set->duration_start = now;
+			pfm_arch_start(current, ctx);
+		}
+		/*
+		 * install new timeout if necessary
+		 */
+		if (new_flags & PFM_SETFL_TIME_SWITCH) {
+			struct hrtimer *h;
+			h = &__get_cpu_var(pfm_hrtimer);
+			hrtimer_forward(h, h->base->get_time(), new_set->hrtimer_exp);
+			new_set->hrtimer_rem = new_set->hrtimer_exp;
+			ret = HRTIMER_RESTART;
+		}
+	}
+
+skip_restart:
+	ctx->active_set = new_set;
+
+	end = sched_clock();
+
+	pfm_stats_inc(set_switch_count);
+	pfm_stats_add(set_switch_ns, end - now);
+
+	return ret;
+}
+
+/*
+ * called from __pfm_overflow_handler() to switch event sets.
+ * monitoring is stopped, task is current, interrupts are masked.
+ * compared to pfm_switch_sets(), this version is simplified because
+ * it knows about the call path. There is no need to stop monitoring
+ * because it is already frozen by PMU handler.
+ */
+void pfm_switch_sets_from_intr(struct pfm_context *ctx)
+{
+	struct pfm_event_set *set, *new_set;
+	u64 now, end;
+	u32 new_flags;
+	int is_system, n;
+
+	now = sched_clock();
+	set = ctx->active_set;
+	new_set = list_first_entry(&set->list, struct pfm_event_set, list);
+	if (&new_set->list == &ctx->set_list)
+		new_set = list_first_entry(&ctx->set_list, struct pfm_event_set, list);
+
+	PFM_DBG_ovfl("state=%d cur_set=%u cur_runs=%llu cur_npend=%d next_set=%u "
+		  "next_runs=%llu new_npend=%d new_r_pmds=%llx",
+		  ctx->state,
+		  set->id,
+		  (unsigned long long)set->runs,
+		  set->npend_ovfls,
+		  new_set->id,
+		  (unsigned long long)new_set->runs,
+		  new_set->npend_ovfls,
+		  (unsigned long long)new_set->reset_pmds[0]);
+
+	is_system = ctx->flags.system;
+	new_flags = new_set->flags;
+
+	/*
+	 * nothing more to do
+	 */
+	if (new_set == set)
+		goto skip_same_set;
+
+	/*
+	 * switch on intr only when set has OVFL_SWITCH
+	 */
+	BUG_ON(set->flags & PFM_SETFL_TIME_SWITCH);
+
+	/*
+	 * when called from PMU intr handler, monitoring
+	 * is already stopped
+	 *
+	 * save current PMD registers, we use a special
+	 * form for performance reason. On some architectures,
+	 * such as x86, the pmds are already saved when entering
+	 * the PMU interrupt handler via pfm-arch_intr_freeze()
+	 * so we don't need to save them again. On the contrary,
+	 * on IA-64, they are not saved by freeze, thus we have to
+	 * to it here.
+	 */
+	pfm_arch_save_pmds_from_intr(ctx, set);
+
+	/*
+	 * compute elapsed ns for active set
+	 */
+	set->duration += now - set->duration_start;
+
+	pfm_arch_restore_pmds(ctx, new_set);
+
+	/*
+	 * must not be restored active as we are still executing in the
+	 * PMU interrupt handler. activation is deferred to unfreeze PMU
+	 */
+	pfm_arch_restore_pmcs(ctx, new_set);
+
+	/*
+	 * check for pending interrupt on incoming set.
+	 * interrupts are masked so handler call deferred
+	 */
+	if (new_set->npend_ovfls) {
+		pfm_arch_resend_irq(ctx);
+		pfm_stats_inc(ovfl_intr_replay_count);
+	}
+	/*
+	 * no need to restore anything, that is already done
+	 */
+	new_set->priv_flags &= ~PFM_SETFL_PRIV_MOD_BOTH;
+	/*
+	 * reset duration counter
+	 */
+	new_set->duration_start = now;
+
+skip_same_set:
+	new_set->runs++;
+
+	/*
+	 * reset switch threshold
+	 */
+	if (new_flags & PFM_SETFL_OVFL_SWITCH)
+		pfm_reload_switch_thresholds(ctx, new_set);
+
+	/*
+	 * reset overflowed PMD registers
+	 */
+	n = bitmap_weight(cast_ulp(new_set->reset_pmds), ctx->regs.max_pmd);
+	if (n)
+		pfm_reset_pmds(ctx, new_set, n, PFM_PMD_RESET_SHORT);
+
+	/*
+	 * XXX: isactive?
+	 *
+	 * Came here following a interrupt which triggered a switch, i.e.,
+	 * previous set was using OVFL_SWITCH, thus we just need to arm
+	 * check if the next set is using timeout, and if so arm the timer.
+	 *
+	 * Timeout is always at least one tick away. No risk of having to
+	 * invoke the timeout handler right now. In any case, cb_mode is
+	 * set to HRTIMER_CB_IRQSAFE_NO_SOFTIRQ such that hrtimer_start
+	 * will not try to wakeup the softirqd which could cause a locking
+	 * problem.
+	 */
+	if (new_flags & PFM_SETFL_TIME_SWITCH) {
+		hrtimer_start(&__get_cpu_var(pfm_hrtimer), set->hrtimer_exp, HRTIMER_MODE_REL);
+		PFM_DBG("armed new timeout for set%u", new_set->id);
+	}
+
+	ctx->active_set = new_set;
+
+	end = sched_clock();
+
+	pfm_stats_inc(set_switch_count);
+	pfm_stats_add(set_switch_ns, end - now);
+}
+
+
+static int pfm_setfl_sane(struct pfm_context *ctx, u32 flags)
+{
+#define PFM_SETFL_BOTH_SWITCH	(PFM_SETFL_OVFL_SWITCH|PFM_SETFL_TIME_SWITCH)
+	int ret;
+
+	ret = pfm_arch_setfl_sane(ctx, flags);
+	if (ret)
+		return ret;
+
+	if ((flags & PFM_SETFL_BOTH_SWITCH) == PFM_SETFL_BOTH_SWITCH) {
+		PFM_DBG("both switch ovfl and switch time are set");
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * it is never possible to change the identification of an existing set
+ */
+static int pfm_change_evtset(struct pfm_context *ctx,
+			       struct pfm_event_set *set,
+			       struct pfarg_setdesc *req)
+{
+	struct timeval tv;
+	struct timespec ts;
+	ktime_t kt;
+	long d, res_ns;
+	s32 rem;
+	u32 flags;
+	int ret;
+	u16 set_id;
+
+	BUG_ON(ctx->state == PFM_CTX_LOADED);
+
+	set_id = req->set_id;
+	flags = req->set_flags;
+
+	ret = pfm_setfl_sane(ctx, flags);
+	if (ret) {
+		PFM_DBG("invalid flags 0x%x set %u", flags, set_id);
+		return -EINVAL;
+	}
+
+	/*
+	 * compute timeout value
+	 */
+	if (flags & PFM_SETFL_TIME_SWITCH) {
+		/*
+		 * timeout value of zero is illegal
+		 */
+		if (req->set_timeout == 0) {
+			PFM_DBG("invalid timeout 0");
+			return -EINVAL;
+		}
+
+		hrtimer_get_res(CLOCK_MONOTONIC, &ts);
+		res_ns = (long)ktime_to_ns(timespec_to_ktime(ts));
+
+		/*
+		 * round-up to multiple of clock resolution
+		 * timeout = ((req->set_timeout+res_ns-1)/res_ns)*res_ns;
+		 *
+		 * u64 division missing on 32-bit arch, so use div_s64_rem
+		 */
+		d = div_s64_rem(req->set_timeout, res_ns, &rem);
+
+		PFM_DBG("set%u flags=0x%x req_timeout=%lluns "
+				"HZ=%u TICK_NSEC=%lu clock_res=%ldns rem=%dns",
+				set_id,
+				flags,
+				(unsigned long long)req->set_timeout,
+				HZ, TICK_NSEC,
+				res_ns,
+				rem);
+
+		/*
+		 * Only accept timeout, we can actually achieve.
+		 * users can invoke clock_getres(CLOCK_MONOTONIC)
+		 * to figure out resolution and adjust timeout
+		 */
+		if (rem) {
+			PFM_DBG("set%u invalid timeout=%llu",
+				set_id,
+				(unsigned long long)req->set_timeout);
+			return -EINVAL;
+		}
+
+		tv = ns_to_timeval(req->set_timeout);
+		kt = timeval_to_ktime(tv);
+		set->hrtimer_exp = kt;
+	} else {
+		set->hrtimer_exp = ktime_set(0, 0);
+	}
+
+	/*
+	 * commit changes
+	 */
+	set->id = set_id;
+	set->flags = flags;
+	set->priv_flags = 0;
+
+	/*
+	 * activation and duration counters are reset as
+	 * most likely major things will change in the set
+	 */
+	set->runs = 0;
+	set->duration = 0;
+
+	return 0;
+}
+
+/*
+ * this function does not modify the next field
+ */
+static void pfm_initialize_set(struct pfm_context *ctx,
+			       struct pfm_event_set *set)
+{
+	u64 *impl_pmcs;
+	u16 i, max_pmc;
+
+	max_pmc = ctx->regs.max_pmc;
+	impl_pmcs =  ctx->regs.pmcs;
+
+	/*
+	 * install default values for all PMC  registers
+	 */
+	for (i = 0; i < max_pmc; i++) {
+		if (test_bit(i, cast_ulp(impl_pmcs))) {
+			set->pmcs[i] = pfm_pmu_conf->pmc_desc[i].dfl_val;
+			PFM_DBG("set%u pmc%u=0x%llx",
+				set->id,
+				i,
+				(unsigned long long)set->pmcs[i]);
+		}
+	}
+
+	/*
+	 * PMD registers are set to 0 when the event set is allocated,
+	 * hence we do not need to explicitly initialize them.
+	 *
+	 * For virtual PMD registers (i.e., those tied to a SW resource)
+	 * their value becomes meaningful once the context is attached.
+	 */
+}
+
+/*
+ * look for an event set using its identification. If the set does not
+ * exist:
+ * 	- if alloc == 0 then return error
+ * 	- if alloc == 1  then allocate set
+ *
+ * alloc is one ONLY when coming from pfm_create_evtsets() which can only
+ * be called when the context is detached, i.e. monitoring is stopped.
+ */
+struct pfm_event_set *pfm_find_set(struct pfm_context *ctx, u16 set_id, int alloc)
+{
+	struct pfm_event_set *set = NULL, *prev, *new_set;
+
+	PFM_DBG("looking for set=%u", set_id);
+
+	prev = NULL;
+	list_for_each_entry(set, &ctx->set_list, list) {
+		if (set->id == set_id)
+			return set;
+		if (set->id > set_id)
+			break;
+		prev = set;
+	}
+
+	if (!alloc)
+		return NULL;
+
+	/*
+	 * we are holding the context spinlock and interrupts
+	 * are unmasked. We must use GFP_ATOMIC as we cannot
+	 * sleep while holding a spin lock.
+	 */
+	new_set = kmem_cache_zalloc(pfm_set_cachep, GFP_ATOMIC);
+	if (!new_set)
+		return NULL;
+
+	new_set->id = set_id;
+
+	INIT_LIST_HEAD(&new_set->list);
+
+	if (prev == NULL) {
+		list_add(&(new_set->list), &ctx->set_list);
+	} else {
+		PFM_DBG("add after set=%u", prev->id);
+		list_add(&(new_set->list), &prev->list);
+	}
+	return new_set;
+}
+
+/**
+ * pfm_create_initial_set - create initial set from __pfm_c reate_context
+ * @ctx: context to atatched the set to
+ */
+int pfm_create_initial_set(struct pfm_context *ctx)
+{
+	struct pfm_event_set *set;
+
+	/*
+	 * create initial set0
+	 */
+	if (!pfm_find_set(ctx, 0, 1))
+		return -ENOMEM;
+
+	set = list_first_entry(&ctx->set_list, struct pfm_event_set, list);
+
+	pfm_initialize_set(ctx, set);
+
+	return 0;
+}
+
+/*
+ * context is unloaded for this command. Interrupts are enabled
+ */
+int __pfm_create_evtsets(struct pfm_context *ctx, struct pfarg_setdesc *req,
+			int count)
+{
+	struct pfm_event_set *set;
+	u16 set_id;
+	int i, ret;
+
+	for (i = 0; i < count; i++, req++) {
+		set_id = req->set_id;
+
+		PFM_DBG("set_id=%u", set_id);
+
+		set = pfm_find_set(ctx, set_id, 1);
+		if (set == NULL)
+			goto error_mem;
+
+		ret = pfm_change_evtset(ctx, set, req);
+		if (ret)
+			goto error_params;
+
+		pfm_initialize_set(ctx, set);
+	}
+	return 0;
+error_mem:
+	PFM_DBG("cannot allocate set %u", set_id);
+	return -ENOMEM;
+error_params:
+	return ret;
+}
+
+int __pfm_getinfo_evtsets(struct pfm_context *ctx, struct pfarg_setinfo *req,
+				 int count)
+{
+	struct pfm_event_set *set;
+	int i, is_system, is_loaded, is_self, ret;
+	u16 set_id;
+	u64 end;
+
+	end = sched_clock();
+
+	is_system = ctx->flags.system;
+	is_loaded = ctx->state == PFM_CTX_LOADED;
+	is_self   = ctx->task == current || is_system;
+
+	ret = -EINVAL;
+	for (i = 0; i < count; i++, req++) {
+
+		set_id = req->set_id;
+
+		list_for_each_entry(set, &ctx->set_list, list) {
+			if (set->id == set_id)
+				goto found;
+			if (set->id > set_id)
+				goto error;
+		}
+found:
+		req->set_flags = set->flags;
+
+		/*
+		 * compute leftover timeout
+		 *
+		 * lockdep may complain about lock inversion
+		 * because of get_remaining() however, this
+		 * applies to self-montoring only, thus the
+		 * thread cannot be in the timeout handler
+		 * and here at the same time given that we
+		 * run with interrupts disabled
+		 */
+		if (is_loaded && is_self) {
+			struct hrtimer *h;
+			h = &__get_cpu_var(pfm_hrtimer);
+			req->set_timeout = ktime_to_ns(hrtimer_get_remaining(h));
+		} else {
+			/*
+			 * hrtimer_rem zero when not using
+			 * timeout-based switching
+			 */
+			req->set_timeout = ktime_to_ns(set->hrtimer_rem);
+		}
+
+		req->set_runs = set->runs;
+		req->set_act_duration = set->duration;
+
+		/*
+		 * adjust for active set if needed
+		 */
+		if (is_system && is_loaded && ctx->flags.started
+		    && set == ctx->active_set)
+			req->set_act_duration  += end - set->duration_start;
+
+		/*
+		 * copy the list of pmds which last overflowed
+		 */
+		bitmap_copy(cast_ulp(req->set_ovfl_pmds),
+			    cast_ulp(set->ovfl_pmds),
+			    PFM_MAX_PMDS);
+
+		/*
+		 * copy bitmask of available PMU registers
+		 *
+		 * must copy over the entire vector to avoid
+		 * returning bogus upper bits pass by user
+		 */
+		bitmap_copy(cast_ulp(req->set_avail_pmcs),
+			    cast_ulp(ctx->regs.pmcs),
+			    PFM_MAX_PMCS);
+
+		bitmap_copy(cast_ulp(req->set_avail_pmds),
+			    cast_ulp(ctx->regs.pmds),
+			    PFM_MAX_PMDS);
+
+		PFM_DBG("set%u flags=0x%x eff_usec=%llu runs=%llu "
+			"a_pmcs=0x%llx a_pmds=0x%llx",
+			set_id,
+			set->flags,
+			(unsigned long long)req->set_timeout,
+			(unsigned long long)set->runs,
+			(unsigned long long)ctx->regs.pmcs[0],
+			(unsigned long long)ctx->regs.pmds[0]);
+	}
+	ret = 0;
+error:
+	return ret;
+}
+
+/*
+ * context is unloaded for this command. Interrupts are enabled
+ */
+int __pfm_delete_evtsets(struct pfm_context *ctx, void *arg, int count)
+{
+	struct pfarg_setdesc *req = arg;
+	struct pfm_event_set *set;
+	u16 set_id;
+	int i, ret;
+
+	ret = -EINVAL;
+	for (i = 0; i < count; i++, req++) {
+		set_id = req->set_id;
+
+		list_for_each_entry(set, &ctx->set_list, list) {
+			if (set->id == set_id)
+				goto found;
+			if (set->id > set_id)
+				goto error;
+		}
+		goto error;
+found:
+		/*
+		 * clear active set if necessary.
+		 * will be updated when context is loaded
+		 */
+		if (set == ctx->active_set)
+			ctx->active_set = NULL;
+
+		list_del(&set->list);
+
+		kmem_cache_free(pfm_set_cachep, set);
+
+		PFM_DBG("set%u deleted", set_id);
+	}
+	ret = 0;
+error:
+	return ret;
+}
+
+/*
+ * called from pfm_context_free() to free all sets
+ */
+void pfm_free_sets(struct pfm_context *ctx)
+{
+	struct pfm_event_set *set, *tmp;
+
+	list_for_each_entry_safe(set, tmp, &ctx->set_list, list) {
+		list_del(&set->list);
+		kmem_cache_free(pfm_set_cachep, set);
+	}
+}
+
+/**
+ * pfm_restart_timer - restart hrtimer taking care of expired timeout
+ * @ctx : context to work with
+ * @set : current active set
+ *
+ * Must be called on the processor on which the timer is to be armed.
+ * Assumes context is locked and interrupts are masked
+ *
+ * Upon return the active set for the context may have changed
+ */
+void pfm_restart_timer(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	struct hrtimer *h;
+	enum hrtimer_restart ret;
+
+	h = &__get_cpu_var(pfm_hrtimer);
+
+	PFM_DBG_ovfl("hrtimer=%lld", (long long)ktime_to_ns(set->hrtimer_rem));
+
+	if (ktime_to_ns(set->hrtimer_rem) > 0) {
+		hrtimer_start(h, set->hrtimer_rem, HRTIMER_MODE_REL);
+	} else {
+		/*
+		 * timer was not re-armed because it has already expired
+		 * timer was not enqueued, we need to switch set now
+		 */
+		pfm_stats_inc(set_switch_exp);
+
+		ret = pfm_switch_sets(ctx, NULL, 1, 0);
+		set = ctx->active_set;
+		if (ret == HRTIMER_RESTART)
+			hrtimer_start(h, set->hrtimer_rem, HRTIMER_MODE_REL);
+	}
+}
+
+int __init pfm_init_sets(void)
+{
+	pfm_set_cachep = kmem_cache_create("pfm_event_set",
+					   sizeof(struct pfm_event_set),
+					   SLAB_HWCACHE_ALIGN, 0, NULL);
+	if (!pfm_set_cachep) {
+		PFM_ERR("cannot initialize event set slab");
+		return -ENOMEM;
+	}
+	return 0;
+}
Index: linux-2.6.31-master/perfmon/perfmon_smpl.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_smpl.c
@@ -0,0 +1,888 @@
+/*
+ * perfmon_smpl.c: perfmon2 sampling management
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/vmalloc.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/random.h>
+#include <linux/uaccess.h>
+#include <linux/perfmon_kern.h>
+
+#include "perfmon_priv.h"
+
+/**
+ * pfm_smpl_buf_alloc - allocate memory for sampling buffer
+ * @ctx: context to operate on
+ * @rsize: requested size
+ *
+ * called from pfm_smpl_buffer_alloc_old() (IA64-COMPAT)
+ * and pfm_setup_smpl_fmt()
+ *
+ * interrupts are enabled, context is not locked.
+ *
+ * function is not static because it is called from the IA-64
+ * compatibility module (perfmon_compat.c)
+ */
+int pfm_smpl_buf_alloc(struct pfm_context *ctx, size_t rsize)
+{
+#if PFM_ARCH_SMPL_ALIGN_SIZE > 0
+#define PFM_ALIGN_SMPL(a, f) (void *)((((unsigned long)(a))+(f-1)) & ~(f-1))
+#else
+#define PFM_ALIGN_SMPL(a, f) (a)
+#endif
+	void *addr, *real_addr;
+	size_t size, real_size;
+	int ret;
+
+	might_sleep();
+
+	/*
+	 * align page boundary
+	 */
+	size = PAGE_ALIGN(rsize);
+
+	/*
+	 * On some arch, it may be necessary to get an alignment greater
+	 * than page size to avoid certain cache effects (e.g., MIPS).
+	 * This is the reason for PFM_ARCH_SMPL_ALIGN_SIZE.
+	 */
+	real_size = size + PFM_ARCH_SMPL_ALIGN_SIZE;
+
+	PFM_DBG("req_size=%zu size=%zu real_size=%zu",
+		rsize,
+		size,
+		real_size);
+
+	ret = pfm_smpl_buf_space_acquire(ctx, real_size);
+	if (ret)
+		return ret;
+
+	/*
+	 * vmalloc can sleep. we do not hold
+	 * any spinlock and interrupts are enabled
+	 */
+	real_addr = addr = vmalloc(real_size);
+	if (!real_addr) {
+		PFM_DBG("cannot allocate sampling buffer");
+		goto unres;
+	}
+
+	/*
+	 * align the useable sampling buffer address to the arch requirement
+	 * This is a nop on most architectures
+	 */
+	addr = PFM_ALIGN_SMPL(real_addr, PFM_ARCH_SMPL_ALIGN_SIZE);
+
+	memset(addr, 0, real_size);
+
+	/*
+	 * due to cache aliasing, it may be necessary to flush the pages
+	 * on certain architectures (e.g., MIPS)
+	 */
+	pfm_cacheflush(addr, real_size);
+
+	/*
+	 * what needs to be freed
+	 */
+	ctx->smpl_real_addr = real_addr;
+	ctx->smpl_real_size = real_size;
+
+	/*
+	 * what is actually available to user
+	 */
+	ctx->smpl_addr = addr;
+	ctx->smpl_size = size;
+
+	PFM_DBG("addr=%p real_addr=%p", addr, real_addr);
+
+	return 0;
+unres:
+	/*
+	 * smpl_addr is NULL, no double freeing possible in pfm_context_free()
+	 */
+	pfm_smpl_buf_space_release(ctx, real_size);
+
+	return -ENOMEM;
+}
+
+/**
+ * pfm_smpl_buf_free - free resources associated with sampling
+ * @ctx: context to operate on
+ */
+void pfm_smpl_buf_free(struct pfm_context *ctx)
+{
+	struct pfm_smpl_fmt *fmt;
+
+	fmt = ctx->smpl_fmt;
+
+	/*
+	 * some formats may not use a buffer, yet they may
+	 * need to be called on exit
+	 */
+	if (fmt) {
+		if (fmt->fmt_exit)
+			(*fmt->fmt_exit)(ctx->smpl_addr);
+		/*
+		 * decrease refcount of sampling format
+		 */
+		pfm_smpl_fmt_put(fmt);
+	}
+
+	if (ctx->smpl_addr) {
+		pfm_smpl_buf_space_release(ctx, ctx->smpl_real_size);
+
+		PFM_DBG("free buffer real_addr=0x%p real_size=%zu",
+			ctx->smpl_real_addr,
+			ctx->smpl_real_size);
+
+		vfree(ctx->smpl_real_addr);
+	}
+}
+
+/**
+ * pfm_setup_smpl_fmt - initialization of sampling format and buffer
+ * @ctx: context to operate on
+ * @fmt_arg: smapling format arguments
+ * @ctx_flags: context flags as passed by user
+ * @filp: file descriptor associated with context
+ *
+ * called from __pfm_create_context()
+ */
+int pfm_setup_smpl_fmt(struct pfm_context *ctx, u32 ctx_flags, void *fmt_arg, int fd)
+{
+	struct pfm_smpl_fmt *fmt;
+	size_t size = 0;
+	int ret = 0;
+
+	fmt = ctx->smpl_fmt;
+
+	/*
+	 * validate parameters
+	 */
+	if (fmt->fmt_validate) {
+		ret = (*fmt->fmt_validate)(ctx_flags,
+					   ctx->regs.num_pmds,
+					   fmt_arg);
+		PFM_DBG("validate(0x%x,%p)=%d", ctx_flags, fmt_arg, ret);
+		if (ret)
+			goto error;
+	}
+
+	/*
+	 * check if buffer format needs buffer allocation
+	 */
+	size = 0;
+	if (fmt->fmt_getsize) {
+		ret = (*fmt->fmt_getsize)(ctx_flags, fmt_arg, &size);
+		if (ret) {
+			PFM_DBG("cannot get size ret=%d", ret);
+			goto error;
+		}
+	}
+
+	/*
+	 * allocate buffer
+	 * v20_compat is for IA-64 backward compatibility with perfmon v2.0
+	 */
+	if (size) {
+#ifdef CONFIG_IA64_PERFMON_COMPAT
+		/*
+		 * backward compatibility with perfmon v2.0 on Ia-64
+		 */
+		if (ctx->flags.ia64_v20_compat)
+			ret = pfm_smpl_buf_alloc_compat(ctx, size, fd);
+		else
+#endif
+			ret = pfm_smpl_buf_alloc(ctx, size);
+
+		if (ret)
+			goto error;
+
+	}
+
+	if (fmt->fmt_init) {
+		ret = (*fmt->fmt_init)(ctx, ctx->smpl_addr, ctx_flags,
+				       ctx->regs.num_pmds,
+				       fmt_arg);
+	}
+	/*
+	 * if there was an error, the buffer/resource will be freed by
+	 * via pfm_context_free()
+	 */
+error:
+	return ret;
+}
+
+void pfm_mask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	u64 now;
+
+	now = sched_clock();
+
+	/*
+	 * we save the PMD values such that we can read them while
+	 * MASKED without having the thread stopped
+	 * because monitoring is stopped
+	 *
+	 * pfm_save_pmds() could be avoided if we knew
+	 * that pfm_arch_intr_freeze() had saved them already
+	 */
+	pfm_save_pmds(ctx, set);
+	pfm_arch_mask_monitoring(ctx, set);
+	/*
+	 * accumulate the set duration up to this point
+	 */
+	set->duration += now - set->duration_start;
+
+	ctx->state = PFM_CTX_MASKED;
+
+	/*
+	 * need to stop timer and remember remaining time
+	 * will be reloaded in pfm_unmask_monitoring
+	 * hrtimer is cancelled in the tail of the interrupt
+	 * handler once the context is unlocked
+	 */
+	if (set->flags & PFM_SETFL_TIME_SWITCH) {
+		struct hrtimer *h = &__get_cpu_var(pfm_hrtimer);
+		hrtimer_cancel(h);
+		set->hrtimer_rem = hrtimer_get_remaining(h);
+	}
+	PFM_DBG_ovfl("can_restart=%u", ctx->flags.can_restart);
+}
+
+/**
+ * pfm_unmask_monitoring - unmask monitoring
+ * @ctx: context to work with
+ * @set: current active set
+ *
+ * interrupts are masked when entering this function.
+ * context must be in MASKED state when calling.
+ *
+ * Upon return, the active set may have changed when using timeout
+ * based switching.
+ */
+static void pfm_unmask_monitoring(struct pfm_context *ctx, struct pfm_event_set *set)
+{
+	if (ctx->state != PFM_CTX_MASKED)
+		return;
+
+	PFM_DBG_ovfl("unmasking monitoring");
+
+	/*
+	 * must be done before calling
+	 * pfm_arch_unmask_monitoring()
+	 */
+	ctx->state = PFM_CTX_LOADED;
+
+	/*
+	 * we need to restore the PMDs because they
+	 * may have been modified by user while MASKED in
+	 * which case the actual registers have no yet
+	 * been updated
+	 */
+	pfm_arch_restore_pmds(ctx, set);
+
+	/*
+	 * call arch specific handler
+	 */
+	pfm_arch_unmask_monitoring(ctx, set);
+
+	/*
+	 * clear force reload flag. May have been set
+	 * in pfm_write_pmcs or pfm_write_pmds
+	 */
+	set->priv_flags &= ~PFM_SETFL_PRIV_MOD_BOTH;
+
+	/*
+	 * reset set duration timer
+	 */
+	set->duration_start = sched_clock();
+
+	/*
+	 * restart hrtimer if needed
+	 */
+	if (set->flags & PFM_SETFL_TIME_SWITCH) {
+		pfm_restart_timer(ctx, set);
+		/* careful here as pfm_restart_timer may switch sets */
+	}
+}
+
+void pfm_reset_pmds(struct pfm_context *ctx,
+		    struct pfm_event_set *set,
+		    int num_pmds,
+		    int reset_mode)
+{
+	u64 val, mask, new_seed;
+	struct pfm_pmd *reg;
+	unsigned int i, not_masked;
+
+	not_masked = ctx->state != PFM_CTX_MASKED;
+
+	PFM_DBG_ovfl("%s r_pmds=0x%llx not_masked=%d",
+		reset_mode == PFM_PMD_RESET_LONG ? "long" : "short",
+		(unsigned long long)set->reset_pmds[0],
+		not_masked);
+
+	pfm_stats_inc(reset_pmds_count);
+
+	for (i = 0; num_pmds; i++) {
+		if (test_bit(i, cast_ulp(set->reset_pmds))) {
+			num_pmds--;
+
+			reg = set->pmds + i;
+
+			val = reset_mode == PFM_PMD_RESET_LONG ?
+			       reg->long_reset : reg->short_reset;
+
+			if (reg->flags & PFM_REGFL_RANDOM) {
+				mask = reg->mask;
+				new_seed = random32();
+
+				/* construct a full 64-bit random value: */
+				if ((unlikely(mask >> 32) != 0))
+					new_seed |= (u64)random32() << 32;
+
+				/* counter values are negative numbers! */
+				val -= (new_seed & mask);
+			}
+
+			set->pmds[i].value = val;
+			reg->lval = val;
+
+			/*
+			 * not all PMD to reset are necessarily
+			 * counters
+			 */
+			if (not_masked)
+				pfm_write_pmd(ctx, i, val);
+
+			PFM_DBG_ovfl("set%u pmd%u sval=0x%llx",
+					set->id,
+					i,
+					(unsigned long long)val);
+		}
+	}
+
+	/*
+	 * done with reset
+	 */
+	bitmap_zero(cast_ulp(set->reset_pmds), i);
+
+	/*
+	 * make changes visible
+	 */
+	if (not_masked)
+		pfm_arch_serialize();
+}
+
+/*
+ * called from pfm_handle_work() and __pfm_restart()
+ * for system-wide and per-thread context to resume
+ * monitoring after a user level notification.
+ *
+ * In both cases, the context is locked and interrupts
+ * are disabled.
+ */
+void pfm_resume_after_ovfl(struct pfm_context *ctx)
+{
+	struct pfm_smpl_fmt *fmt;
+	u32 rst_ctrl;
+	struct pfm_event_set *set;
+	u64 *reset_pmds;
+	void *hdr;
+	int state, ret;
+
+	hdr = ctx->smpl_addr;
+	fmt = ctx->smpl_fmt;
+	state = ctx->state;
+	set = ctx->active_set;
+	ret = 0;
+
+	if (hdr) {
+		rst_ctrl = 0;
+		prefetch(hdr);
+	} else {
+		rst_ctrl = PFM_OVFL_CTRL_RESET;
+	}
+
+	/*
+	 * if using a sampling buffer format and it has a restart callback,
+	 * then invoke it. hdr may be NULL, if the format does not use a
+	 * perfmon buffer
+	 */
+	if (fmt && fmt->fmt_restart)
+		ret = (*fmt->fmt_restart)(ctx, &rst_ctrl);
+
+	reset_pmds = set->reset_pmds;
+
+	PFM_DBG("fmt_restart=%d reset_count=%d set=%u r_pmds=0x%llx switch=%d "
+		"ctx_state=%d",
+		ret,
+		ctx->flags.reset_count,
+		set->id,
+		(unsigned long long)reset_pmds[0],
+		(set->priv_flags & PFM_SETFL_PRIV_SWITCH),
+		state);
+
+	if (!ret) {
+		/*
+		 * switch set if needed
+		 */
+		if (set->priv_flags & PFM_SETFL_PRIV_SWITCH) {
+			set->priv_flags &= ~PFM_SETFL_PRIV_SWITCH;
+			pfm_switch_sets(ctx, NULL, PFM_PMD_RESET_LONG, 0);
+			set = ctx->active_set;
+		} else if (rst_ctrl & PFM_OVFL_CTRL_RESET) {
+			int nn;
+			nn = bitmap_weight(cast_ulp(set->reset_pmds),
+					   ctx->regs.max_pmd);
+			if (nn)
+				pfm_reset_pmds(ctx, set, nn, PFM_PMD_RESET_LONG);
+		}
+
+		if (!(rst_ctrl & PFM_OVFL_CTRL_MASK))
+			pfm_unmask_monitoring(ctx, set);
+		else
+			PFM_DBG("stopping monitoring?");
+		ctx->state = PFM_CTX_LOADED;
+	}
+}
+
+/*
+ * This function is called when we need to perform asynchronous
+ * work on a context. This function is called ONLY when about to
+ * return to user mode (very much like with signal handling).
+ *
+ * There are several reasons why we come here:
+ *
+ *  - per-thread mode, not self-monitoring, to reset the counters
+ *    after a pfm_restart()
+ *
+ *  - we are zombie and we need to cleanup our state
+ *
+ *  - we need to block after an overflow notification
+ *    on a context with the PFM_OVFL_NOTIFY_BLOCK flag
+ *
+ * This function is never called for a system-wide context.
+ *
+ * pfm_handle_work() can be called with interrupts enabled
+ * (TIF_NEED_RESCHED) or disabled. The down_interruptible
+ * call may sleep, therefore we must re-enable interrupts
+ * to avoid deadlocks. It is safe to do so because this function
+ * is called ONLY when returning to user level, in which case
+ * there is no risk of kernel stack overflow due to deep
+ * interrupt nesting.
+ */
+void pfm_handle_work(struct pt_regs *regs)
+{
+	struct pfm_context *ctx;
+	unsigned long flags, dummy_flags;
+	int type, ret, info;
+
+#ifdef CONFIG_PPC
+	/*
+	 * This is just a temporary fix. Obviously we'd like to fix the powerpc
+	 * code to make that check before calling __pfm_handle_work() to
+	 * prevent the function call overhead, but the call is made from
+	 * assembly code, so it will take a little while to figure out how to
+	 * perform the check correctly.
+	 */
+	if (!test_thread_flag(TIF_PERFMON_WORK))
+		return;
+#endif
+
+	if (!user_mode(regs))
+		return;
+
+	clear_thread_flag(TIF_PERFMON_WORK);
+
+	pfm_stats_inc(handle_work_count);
+
+	ctx = current->pfm_context;
+	if (ctx == NULL) {
+		PFM_DBG("[%d] has no ctx", current->pid);
+		return;
+	}
+
+	BUG_ON(ctx->flags.system);
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	type = ctx->flags.work_type;
+	ctx->flags.work_type = PFM_WORK_NONE;
+
+	PFM_DBG("work_type=%d reset_count=%d",
+		type,
+		ctx->flags.reset_count);
+
+	switch (type) {
+	case PFM_WORK_ZOMBIE:
+		goto do_zombie;
+	case PFM_WORK_RESET:
+		/* simply reset, no blocking */
+		goto skip_blocking;
+	case PFM_WORK_NONE:
+		PFM_DBG("unexpected PFM_WORK_NONE");
+		goto nothing_todo;
+	case PFM_WORK_BLOCK:
+		break;
+	default:
+		PFM_DBG("unkown type=%d", type);
+		goto nothing_todo;
+	}
+
+	/*
+	 * restore interrupt mask to what it was on entry.
+	 * Could be enabled/disabled.
+	 */
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	/*
+	 * force interrupt enable because of down_interruptible()
+	 */
+	local_irq_enable();
+
+	PFM_DBG("before block sleeping");
+
+	/*
+	 * may go through without blocking on SMP systems
+	 * if restart has been received already by the time we call down()
+	 */
+	ret = wait_for_completion_interruptible(&ctx->restart_complete);
+
+	PFM_DBG("after block sleeping ret=%d", ret);
+
+	/*
+	 * lock context and mask interrupts again
+	 * We save flags into a dummy because we may have
+	 * altered interrupts mask compared to entry in this
+	 * function.
+	 */
+	spin_lock_irqsave(&ctx->lock, dummy_flags);
+
+	if (ctx->state == PFM_CTX_ZOMBIE)
+		goto do_zombie;
+
+	/*
+	 * in case of interruption of down() we don't restart anything
+	 */
+	if (ret < 0)
+		goto nothing_todo;
+
+skip_blocking:
+	/*
+	 * iterate over the number of pending resets
+	 * There are certain situations where there may be
+	 * multiple notifications sent before a pfm_restart().
+	 * As such, it may be that multiple pfm_restart() are
+	 * issued before the monitored thread gets to
+	 * pfm_handle_work(). To avoid losing restarts, pfm_restart()
+	 * increments a counter (reset_counts). Here, we take this
+	 * into account by potentially calling pfm_resume_after_ovfl()
+	 * multiple times. It is up to the sampling format to take the
+	 * appropriate actions.
+	 */
+	while (ctx->flags.reset_count) {
+		pfm_resume_after_ovfl(ctx);
+		/* careful as active set may have changed */
+		ctx->flags.reset_count--;
+	}
+
+nothing_todo:
+	/*
+	 * restore flags as they were upon entry
+	 */
+	spin_unlock_irqrestore(&ctx->lock, flags);
+	return;
+
+do_zombie:
+	PFM_DBG("context is zombie, bailing out");
+
+	__pfm_unload_context(ctx, &info);
+
+	/*
+	 * keep the spinlock check happy
+	 */
+	spin_unlock(&ctx->lock);
+
+	/*
+	 * enable interrupt for vfree()
+	 */
+	local_irq_enable();
+
+	/*
+	 * cancel timer now that context is unlocked
+	 */
+	if (info & 0x2) {
+		ret = hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
+		PFM_DBG("timeout cancel=%d", ret);
+	}
+
+	/*
+	 * actual context free
+	 */
+	pfm_free_context(ctx);
+
+	/*
+	 * restore interrupts as they were upon entry
+	 */
+	local_irq_restore(flags);
+
+	/* always true */
+	if (info & 0x1)
+		pfm_session_release(0, 0);
+}
+
+/**
+ * __pfm_restart - resume monitoring after user-level notification
+ * @ctx: context to operate on
+ * @info: return information used to free resource once unlocked
+ *
+ * function called from sys_pfm_restart(). It is used when overflow
+ * notification is requested. For each notification received, the user
+ * must call pfm_restart() to indicate to the kernel that it is done
+ * processing the notification.
+ *
+ * When the caller is doing user level sampling, this function resets
+ * the overflowed counters and resumes monitoring which is normally stopped
+ * during notification (always the consequence of a counter overflow).
+ *
+ * When using a sampling format, the format restart() callback is invoked,
+ * overflowed PMDS may be reset based upon decision from sampling format.
+ *
+ * When operating in per-thread mode, and when not self-monitoring, the
+ * monitored thread DOES NOT need to be stopped, unlike for many other calls.
+ *
+ * This means that the effect of the restart may not necessarily be observed
+ * right when returning from the call. For instance, counters may not already
+ * be reset in the other thread.
+ *
+ * When operating in system-wide, the caller must be running on the monitored
+ * CPU.
+ *
+ * The context is locked and interrupts are disabled.
+ *
+ * info value upon return:
+ * 	- bit 0: when set, mudt issue complete() on restart semaphore
+ */
+int __pfm_restart(struct pfm_context *ctx, int *info)
+{
+	int state;
+
+	state = ctx->state;
+
+	PFM_DBG("state=%d can_restart=%d reset_count=%d",
+		state,
+		ctx->flags.can_restart,
+		ctx->flags.reset_count);
+
+	*info = 0;
+
+	switch (state) {
+	case PFM_CTX_MASKED:
+		break;
+	case PFM_CTX_LOADED:
+		if (ctx->smpl_addr && ctx->smpl_fmt->fmt_restart)
+			break;
+	default:
+		PFM_DBG("invalid state=%d", state);
+		return -EBUSY;
+	}
+
+	/*
+	 * first check if allowed to restart, i.e., notifications received
+	 */
+	if (!ctx->flags.can_restart) {
+		PFM_DBG("no restart can_restart=0");
+		return -EBUSY;
+	}
+
+	pfm_stats_inc(pfm_restart_count);
+
+	/*
+	 * at this point, the context is either LOADED or MASKED
+	 */
+	ctx->flags.can_restart--;
+
+	/*
+	 * handle self-monitoring case and system-wide
+	 */
+	if (ctx->task == current || ctx->flags.system) {
+		pfm_resume_after_ovfl(ctx);
+		return 0;
+	}
+
+	/*
+	 * restart another task
+	 */
+
+	/*
+	 * if blocking, then post the semaphore if PFM_CTX_MASKED, i.e.
+	 * the task is blocked or on its way to block. That's the normal
+	 * restart path. If the monitoring is not masked, then the task
+	 * can be actively monitoring and we cannot directly intervene.
+	 * Therefore we use the trap mechanism to catch the task and
+	 * force it to reset the buffer/reset PMDs.
+	 *
+	 * if non-blocking, then we ensure that the task will go into
+	 * pfm_handle_work() before returning to user mode.
+	 *
+	 * We cannot explicitly reset another task, it MUST always
+	 * be done by the task itself. This works for system wide because
+	 * the tool that is controlling the session is logically doing
+	 * "self-monitoring".
+	 */
+	if (ctx->flags.block && state == PFM_CTX_MASKED) {
+		PFM_DBG("unblocking [%d]", ctx->task->pid);
+		/*
+		 * It is not possible to call complete() with the context locked
+		 * otherwise we have a potential deadlock with the PMU context
+		 * switch code due to a lock inversion between task_rq_lock()
+		 * and the context lock.
+		 * Instead we mark whether or not we need to issue the complete
+		 * and we invoke the function once the context lock is released
+		 * in sys_pfm_restart()
+		 */
+		*info = 1;
+	} else {
+		PFM_DBG("[%d] armed exit trap", ctx->task->pid);
+		pfm_post_work(ctx->task, ctx, PFM_WORK_RESET);
+	}
+	ctx->flags.reset_count++;
+	return 0;
+}
+
+/**
+ * pfm_get_smpl_arg -- copy user arguments to pfm_create_context() related to sampling format
+ * @name: format name as passed by user
+ * @fmt_arg: format optional argument as passed by user
+ * @uszie: size of structure pass in fmt_arg
+ * @arg: kernel copy of fmt_arg
+ * @fmt: pointer to sampling format upon success
+ *
+ * arg is kmalloc'ed, thus it needs a kfree by caller
+ */
+int pfm_get_smpl_arg(char __user *fmt_uname, void __user *fmt_uarg, size_t usize, void **arg,
+		     struct pfm_smpl_fmt **fmt)
+{
+	struct pfm_smpl_fmt *f;
+	char *fmt_name;
+	void *addr = NULL;
+	size_t sz;
+	int ret;
+
+	fmt_name = getname(fmt_uname);
+	ret = PTR_ERR(fmt_name);
+	if (IS_ERR(fmt_name)) {
+		PFM_DBG("getname failed: %d", ret);
+		return ret;
+	}
+
+	/*
+	 * find fmt and increase refcount
+	 */
+	f = pfm_smpl_fmt_get(fmt_name);
+
+	putname(fmt_name);
+
+	if (f == NULL) {
+		PFM_DBG("buffer format not found");
+		return -EINVAL;
+	}
+
+	/*
+	 * expected format argument size
+	 */
+	sz = f->fmt_arg_size;
+
+	/*
+	 * check user size matches expected size
+	 * usize = -1 is for IA-64 backward compatibility
+	 */
+	ret = -EINVAL;
+	if (sz != usize && usize != -1) {
+		PFM_DBG("invalid arg size %zu, format expects %zu",
+				usize, sz);
+		goto error;
+	}
+
+	if (sz) {
+		ret = -ENOMEM;
+		addr = kmalloc(sz, GFP_KERNEL);
+		if (addr == NULL)
+			goto error;
+
+		ret = -EFAULT;
+		if (copy_from_user(addr, fmt_uarg, sz))
+			goto error;
+	}
+	*arg = addr;
+	*fmt = f;
+	return 0;
+
+error:
+	kfree(addr);
+	pfm_smpl_fmt_put(f);
+	return ret;
+}
+
+int pfm_smpl_buf_load_context(struct pfm_context *ctx)
+{
+	struct pfm_smpl_fmt *f;
+
+	f = ctx->smpl_fmt;
+
+	if (!(f && f->fmt_load))
+		return 0;
+
+	return f->fmt_load(ctx);
+}
+
+void pfm_smpl_buf_unload_context(struct pfm_context *ctx)
+{
+	struct pfm_smpl_fmt *f;
+
+	f = ctx->smpl_fmt;
+
+	if (!(f && f->fmt_unload))
+		return;
+
+	f->fmt_unload(ctx);
+}
Index: linux-2.6.31-master/perfmon/perfmon_syscalls.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_syscalls.c
@@ -0,0 +1,1067 @@
+/*
+ * perfmon_syscalls.c: perfmon2 system call interface
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/ptrace.h>
+#include <linux/perfmon_kern.h>
+#include <linux/uaccess.h>
+#include "perfmon_priv.h"
+
+/*
+ * Context locking rules:
+ * ---------------------
+ * 	- any thread with access to the file descriptor of a context can
+ * 	  potentially issue perfmon calls
+ *
+ * 	- calls must be serialized to guarantee correctness
+ *
+ * 	- as soon as a context is attached to a thread or CPU, it may be
+ * 	  actively monitoring. On some architectures, such as IA-64, this
+ * 	  is true even though the pfm_start() call has not been made. This
+ * 	  comes from the fact that on some architectures, it is possible to
+ * 	  start/stop monitoring from userland.
+ *
+ *	- If monitoring is active, then there can PMU interrupts. Because
+ *	  context accesses must be serialized, the perfmon system calls
+ *	  must mask interrupts as soon as the context is attached.
+ *
+ *	- perfmon system calls that operate with the context unloaded cannot
+ *	  assume it is actually unloaded when they are called. They first need
+ *	  to check and for that they need interrupts masked. Then, if the
+ *	  context is actually unloaded, they can unmask interrupts.
+ *
+ *	- interrupt masking holds true for other internal perfmon functions as
+ *	  well. Except for PMU interrupt handler because those interrupts
+ *	  cannot be nested.
+ *
+ * 	- we mask ALL interrupts instead of just the PMU interrupt because we
+ * 	  also need to protect against timer interrupts which could trigger
+ * 	  a set switch.
+ */
+#ifdef CONFIG_UTRACE
+#include <linux/utrace.h>
+
+static u32
+stopper_quiesce(struct utrace_attached_engine *engine, struct task_struct *tsk)
+{
+	PFM_DBG("quiesced [%d]", tsk->pid);
+	complete(engine->data);
+	return UTRACE_ACTION_RESUME;
+}
+
+void
+pfm_resume_task(struct task_struct *t, void *data)
+{
+	PFM_DBG("utrace detach [%d]", t->pid);
+	(void) utrace_detach(t, data);
+}
+
+static const struct utrace_engine_ops utrace_ops =
+{
+	.report_quiesce = stopper_quiesce,
+};
+
+static int pfm_wait_task_stopped(struct task_struct *task, void **data)
+{
+	DECLARE_COMPLETION_ONSTACK(done);
+	struct utrace_attached_engine *eng;
+	int ret;
+
+	eng = utrace_attach(task, UTRACE_ATTACH_CREATE, &utrace_ops, &done);
+	if (IS_ERR(eng))
+		return PTR_ERR(eng);
+
+	ret = utrace_set_flags(task, eng,
+			       UTRACE_ACTION_QUIESCE | UTRACE_EVENT(QUIESCE));
+	PFM_DBG("wait quiesce [%d]", task->pid);
+	if (!ret)
+		ret = wait_for_completion_interruptible(&done);
+
+	if (ret)
+		(void) utrace_detach(task, eng);
+	else
+		*data = eng;
+	return 0;
+}
+#else /* !CONFIG_UTRACE */
+static int pfm_wait_task_stopped(struct task_struct *task, void **data)
+{
+	int ret;
+
+	*data = NULL;
+
+	/*
+	 * returns 0 if cannot attach
+	 */
+	ret = ptrace_may_access(task, PTRACE_MODE_ATTACH);
+	PFM_DBG("may_attach=%d", ret);
+	if (!ret)
+		return -EPERM;
+
+	ret = ptrace_check_attach(task, 0);
+	PFM_DBG("check_attach=%d", ret);
+	return ret;
+}
+void pfm_resume_task(struct task_struct *t, void *data)
+{}
+#endif
+
+struct pfm_syscall_cookie {
+	struct file *filp;
+	int fput_needed;
+};
+
+/*
+ * cannot attach if :
+ * 	- kernel task
+ * 	- task not owned by caller (checked by ptrace_may_attach())
+ * 	- task is dead or zombie
+ * 	- cannot use blocking notification when self-monitoring
+ */
+static int pfm_task_incompatible(struct pfm_context *ctx,
+				 struct task_struct *task)
+{
+	/*
+	 * cannot attach to a kernel thread
+	 */
+	if (!task->mm) {
+		PFM_DBG("cannot attach to kernel thread [%d]", task->pid);
+		return -EPERM;
+	}
+
+	/*
+	 * cannot use block on notification when
+	 * self-monitoring.
+	 */
+	if (ctx->flags.block && task == current) {
+		PFM_DBG("cannot use block on notification when self-monitoring"
+			"[%d]", task->pid);
+		return -EINVAL;
+	}
+	/*
+	 * cannot attach to a zombie task
+	 */
+	if (task->exit_state == EXIT_ZOMBIE || task->exit_state == EXIT_DEAD) {
+		PFM_DBG("cannot attach to zombie/dead task [%d]", task->pid);
+		return -EBUSY;
+	}
+	return 0;
+}
+
+/**
+ * pfm_get_task -- check permission and acquire task to monitor
+ * @ctx: perfmon context
+ * @pid: identification of the task to check
+ * @task: upon return, a pointer to the task to monitor
+ *
+ * This function  is used in per-thread mode only AND when not
+ * self-monitoring. It finds the task to monitor and checks
+ * that the caller has permissions to attach. It also checks
+ * that the task is stopped via ptrace so that we can safely
+ * modify its state.
+ *
+ * task refcount is incremented when succesful.
+ */
+static int pfm_get_task(struct pfm_context *ctx, pid_t pid,
+			struct task_struct **task, void **data)
+{
+	struct task_struct *p;
+	int ret = 0, ret1 = 0;
+
+	*data = NULL;
+
+	/*
+	 * When attaching to another thread we must ensure
+	 * that the thread is actually stopped.
+	 *
+	 * As a consequence, only the ptracing parent can actually
+	 * attach a context to a thread. Obviously, this constraint
+	 * does not exist for self-monitoring threads.
+	 *
+	 * We use ptrace_may_attach() to check for permission.
+	 */
+	read_lock(&tasklist_lock);
+
+	p = find_task_by_vpid(pid);
+	if (p)
+		get_task_struct(p);
+
+	read_unlock(&tasklist_lock);
+
+	if (!p) {
+		PFM_DBG("task not found %d", pid);
+		return -ESRCH;
+	}
+
+	ret = pfm_task_incompatible(ctx, p);
+	if (ret)
+		goto error;
+
+	ret = pfm_wait_task_stopped(p, data);
+	if (ret)
+		goto error;
+
+	*task = p;
+
+	return 0;
+error:
+	if (!(ret1 || ret))
+		ret = -EPERM;
+
+	put_task_struct(p);
+
+	return ret;
+}
+
+/*
+ * context must be locked when calling this function
+ */
+int pfm_check_task_state(struct pfm_context *ctx, int check_mask,
+			 unsigned long *flags, void **resume)
+{
+	struct task_struct *task;
+	unsigned long local_flags, new_flags;
+	int state, ret;
+
+	*resume = NULL;
+
+recheck:
+	/*
+	 * task is NULL for system-wide context
+	 */
+	task = ctx->task;
+	state = ctx->state;
+	local_flags = *flags;
+
+	PFM_DBG("state=%d check_mask=0x%x", state, check_mask);
+	/*
+	 * if the context is detached, then we do not touch
+	 * hardware, therefore there is not restriction on when we can
+	 * access it.
+	 */
+	if (state == PFM_CTX_UNLOADED)
+		return 0;
+	/*
+	 * no command can operate on a zombie context.
+	 * A context becomes zombie when the file that identifies
+	 * it is closed while the context is still attached to the
+	 * thread it monitors.
+	 */
+	if (state == PFM_CTX_ZOMBIE)
+		return -EINVAL;
+
+	/*
+	 * at this point, state is PFM_CTX_LOADED or PFM_CTX_MASKED
+	 */
+
+	/*
+	 * some commands require the context to be unloaded to operate
+	 */
+	if (check_mask & PFM_CMD_UNLOADED)  {
+		PFM_DBG("state=%d, cmd needs context unloaded", state);
+		return -EBUSY;
+	}
+
+	/*
+	 * self-monitoring always ok.
+	 */
+	if (task == current)
+		return 0;
+
+	/*
+	 * for syswide, the calling thread must be running on the cpu
+	 * the context is bound to.
+	 */
+	if (ctx->flags.system) {
+		if (ctx->cpu != smp_processor_id())
+			return -EBUSY;
+		return 0;
+	}
+
+	/*
+	 * at this point, monitoring another thread
+	 */
+
+	/*
+	 * the pfm_unload_context() command is allowed on masked context
+	 */
+	if (state == PFM_CTX_MASKED && !(check_mask & PFM_CMD_UNLOAD))
+		return 0;
+
+	/*
+	 * When we operate on another thread, we must wait for it to be
+	 * stopped and completely off any CPU as we need to access the
+	 * PMU state (or machine state).
+	 *
+	 * A thread can be put in the STOPPED state in various ways
+	 * including PTRACE_ATTACH, or when it receives a SIGSTOP signal.
+	 * We enforce that the thread must be ptraced, so it is stopped
+	 * AND it CANNOT wake up while we operate on it because this
+	 * would require an action from the ptracing parent which is the
+	 * thread that is calling this function.
+	 *
+	 * The dependency on ptrace, imposes that only the ptracing
+	 * parent can issue command on a thread. This is unfortunate
+	 * but we do not know of a better way of doing this.
+	 */
+	if (check_mask & PFM_CMD_STOPPED) {
+
+		spin_unlock_irqrestore(&ctx->lock, local_flags);
+
+		/*
+		 * check that the thread is ptraced AND STOPPED
+		 */
+		ret = pfm_wait_task_stopped(task, resume);
+
+		spin_lock_irqsave(&ctx->lock, new_flags);
+
+		/*
+		 * flags may be different than when we released the lock
+		 */
+		*flags = new_flags;
+
+		if (ret)
+			return ret;
+		/*
+		 * we must recheck to verify if state has changed
+		 */
+		if (unlikely(ctx->state != state)) {
+			PFM_DBG("old_state=%d new_state=%d",
+				state,
+				ctx->state);
+			goto recheck;
+		}
+	}
+	return 0;
+}
+
+/*
+ * pfm_get_args - Function used to copy the syscall argument into kernel memory.
+ * @ureq: user argument
+ * @sz: user argument size
+ * @lsz: size of stack buffer
+ * @laddr: stack buffer address
+ * @req: point to start of kernel copy of the argument
+ * @ptr_free: address of kernel copy to free
+ *
+ * There are two options:
+ * 	- use a stack buffer described by laddr (addresses) and lsz (size)
+ * 	- allocate memory
+ *
+ * return:
+ * 	< 0 : in case of error (ptr_free may not be updated)
+ * 	  0 : success
+ *      - req: points to base of kernel copy of arguments
+ *	- ptr_free: address of buffer to free by caller on exit.
+ *		    NULL if using the stack buffer
+ *
+ * when ptr_free is not NULL upon return, the caller must kfree()
+ */
+int pfm_get_args(void __user *ureq, size_t sz, size_t lsz, void *laddr,
+		 void **req, void **ptr_free)
+{
+	void *addr;
+
+	/*
+	 * check syadmin argument limit
+	 */
+	if (unlikely(sz > pfm_controls.arg_mem_max)) {
+		PFM_DBG("argument too big %zu max=%zu",
+			sz,
+			pfm_controls.arg_mem_max);
+		return -E2BIG;
+	}
+
+	/*
+	 * check if vector fits on stack buffer
+	 */
+	if (sz > lsz) {
+		addr = kmalloc(sz, GFP_KERNEL);
+		if (unlikely(addr == NULL))
+			return -ENOMEM;
+		*ptr_free = addr;
+	} else {
+		addr = laddr;
+		*req = laddr;
+		*ptr_free = NULL;
+	}
+
+	/*
+	 * bring the data in
+	 */
+	if (unlikely(copy_from_user(addr, ureq, sz))) {
+		if (addr != laddr)
+			kfree(addr);
+		return -EFAULT;
+	}
+
+	/*
+	 * base address of kernel buffer
+	 */
+	*req = addr;
+
+	return 0;
+}
+
+/**
+ * pfm_acquire_ctx_from_fd -- get ctx from file descriptor
+ * @fd: file descriptor
+ * @ctx: pointer to pointer of context updated on return
+ * @cookie: opaque structure to use for release
+ *
+ * This helper function extracts the ctx from the file descriptor.
+ * It also increments the refcount of the file structure. Thus
+ * it updates the cookie so the refcount can be decreased when
+ * leaving the perfmon syscall via pfm_release_ctx_from_fd
+ */
+static int pfm_acquire_ctx_from_fd(int fd, struct pfm_context **ctx,
+				   struct pfm_syscall_cookie *cookie)
+{
+	struct file *filp;
+	int fput_needed;
+
+	filp = fget_light(fd, &fput_needed);
+	if (unlikely(filp == NULL)) {
+		PFM_DBG("invalid fd %d", fd);
+		return -EBADF;
+	}
+
+	*ctx = filp->private_data;
+
+	if (unlikely(!*ctx || filp->f_op != &pfm_file_ops)) {
+		PFM_DBG("fd %d not related to perfmon", fd);
+		return -EBADF;
+	}
+	cookie->filp = filp;
+	cookie->fput_needed = fput_needed;
+
+	return 0;
+}
+
+/**
+ * pfm_release_ctx_from_fd -- decrease refcount of file associated with context
+ * @cookie: the cookie structure initialized by pfm_acquire_ctx_from_fd
+ */
+static inline void pfm_release_ctx_from_fd(struct pfm_syscall_cookie *cookie)
+{
+	fput_light(cookie->filp, cookie->fput_needed);
+}
+
+/*
+ * unlike the other perfmon system calls, this one returns a file descriptor
+ * or a value < 0 in case of error, very much like open() or socket()
+ */
+asmlinkage long sys_pfm_create_context(struct pfarg_ctx __user *ureq,
+				       char __user *fmt_name,
+				       void __user *fmt_uarg, size_t fmt_size)
+{
+	struct pfarg_ctx req;
+	struct pfm_smpl_fmt *fmt = NULL;
+	void *fmt_arg = NULL;
+	int ret;
+
+	PFM_DBG("req=%p fmt=%p fmt_arg=%p size=%zu",
+		ureq, fmt_name, fmt_uarg, fmt_size);
+
+	if (perfmon_disabled)
+		return -ENOSYS;
+
+	if (copy_from_user(&req, ureq, sizeof(req)))
+		return -EFAULT;
+
+	if (fmt_name) {
+		ret = pfm_get_smpl_arg(fmt_name, fmt_uarg, fmt_size, &fmt_arg, &fmt);
+		if (ret)
+			goto abort;
+	}
+
+	ret = __pfm_create_context(&req, fmt, fmt_arg, PFM_NORMAL, NULL);
+
+	kfree(fmt_arg);
+abort:
+	return ret;
+}
+
+asmlinkage long sys_pfm_write_pmcs(int fd, struct pfarg_pmc __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct pfm_syscall_cookie cookie;
+	struct pfarg_pmc pmcs[PFM_PMC_STK_ARG];
+	struct pfarg_pmc *req;
+	void *fptr, *resume;
+	unsigned long flags;
+	size_t sz;
+	int ret;
+
+	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
+
+	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq)) {
+		PFM_DBG("invalid arg count %d", count);
+		return -EINVAL;
+	}
+
+	sz = count*sizeof(*ureq);
+
+	ret = pfm_acquire_ctx_from_fd(fd, &ctx, &cookie);
+	if (ret)
+		return ret;
+
+	ret = pfm_get_args(ureq, sz, sizeof(pmcs), pmcs, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags, &resume);
+	if (!ret)
+		ret = __pfm_write_pmcs(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	/*
+	 * This function may be on the critical path.
+	 * We want to avoid the branch if unecessary.
+	 */
+	if (fptr)
+		kfree(fptr);
+error:
+	pfm_release_ctx_from_fd(&cookie);
+	return ret;
+}
+
+asmlinkage long sys_pfm_write_pmds(int fd, struct pfarg_pmd __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct pfm_syscall_cookie cookie;
+	struct pfarg_pmd pmds[PFM_PMD_STK_ARG];
+	struct pfarg_pmd *req;
+	void *fptr, *resume;
+	unsigned long flags;
+	size_t sz;
+	int ret;
+
+	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
+
+	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq)) {
+		PFM_DBG("invalid arg count %d", count);
+		return -EINVAL;
+	}
+
+	sz = count*sizeof(*ureq);
+
+	ret = pfm_acquire_ctx_from_fd(fd, &ctx, &cookie);
+	if (ret)
+		return ret;
+
+	ret = pfm_get_args(ureq, sz, sizeof(pmds), pmds, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags, &resume);
+	if (!ret)
+		ret = __pfm_write_pmds(ctx, req, count, 0);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	if (fptr)
+		kfree(fptr);
+error:
+	pfm_release_ctx_from_fd(&cookie);
+	return ret;
+}
+
+asmlinkage long sys_pfm_read_pmds(int fd, struct pfarg_pmd __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct pfm_syscall_cookie cookie;
+	struct pfarg_pmd pmds[PFM_PMD_STK_ARG];
+	struct pfarg_pmd *req;
+	void *fptr, *resume;
+	unsigned long flags;
+	size_t sz;
+	int ret;
+
+	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
+
+	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq))
+		return -EINVAL;
+
+	sz = count*sizeof(*ureq);
+
+	ret = pfm_acquire_ctx_from_fd(fd, &ctx, &cookie);
+	if (ret)
+		return ret;
+
+	ret = pfm_get_args(ureq, sz, sizeof(pmds), pmds, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags, &resume);
+	if (!ret)
+		ret = __pfm_read_pmds(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	if (fptr)
+		kfree(fptr);
+error:
+	pfm_release_ctx_from_fd(&cookie);
+	return ret;
+}
+
+asmlinkage long sys_pfm_restart(int fd)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct pfm_syscall_cookie cookie;
+	void *resume;
+	unsigned long flags;
+	int ret, info;
+
+	PFM_DBG("fd=%d", fd);
+
+	ret = pfm_acquire_ctx_from_fd(fd, &ctx, &cookie);
+	if (ret)
+		return ret;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, 0, &flags, &resume);
+	if (!ret)
+		ret = __pfm_restart(ctx, &info);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (resume)
+		pfm_resume_task(task, resume);
+	/*
+	 * In per-thread mode with blocking notification, i.e.
+	 * ctx->flags.blocking=1, we need to defer issuing the
+	 * complete to unblock the blocked monitored thread.
+	 * Otherwise we have a potential deadlock due to a lock
+	 * inversion between the context lock and the task_rq_lock()
+	 * which can happen if one thread is in this call and the other
+	 * (the monitored thread) is in the context switch code.
+	 *
+	 * It is safe to access the context outside the critical section
+	 * because:
+	 * 	- we are protected by the fget_light(), thus the context
+	 * 	  cannot disappear
+	 */
+	if (ret == 0 && info == 1)
+		complete(&ctx->restart_complete);
+
+	pfm_release_ctx_from_fd(&cookie);
+	return ret;
+}
+
+asmlinkage long sys_pfm_stop(int fd)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct pfm_syscall_cookie cookie;
+	void *resume;
+	unsigned long flags;
+	int ret;
+	int release_info;
+
+	PFM_DBG("fd=%d", fd);
+
+	ret = pfm_acquire_ctx_from_fd(fd, &ctx, &cookie);
+	if (ret)
+		return ret;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags, &resume);
+	if (!ret)
+		ret = __pfm_stop(ctx, &release_info);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	/*
+	 * defer cancellation of timer to avoid race
+	 * with pfm_handle_switch_timeout()
+	 *
+	 * applies only when self-monitoring
+	 */
+	if (release_info & 0x2)
+		hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
+
+	pfm_release_ctx_from_fd(&cookie);
+	return ret;
+}
+
+asmlinkage long sys_pfm_start(int fd, struct pfarg_start __user *ureq)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct pfm_syscall_cookie cookie;
+	void *resume;
+	struct pfarg_start req;
+	unsigned long flags;
+	int ret;
+
+	PFM_DBG("fd=%d req=%p", fd, ureq);
+
+	ret = pfm_acquire_ctx_from_fd(fd, &ctx, &cookie);
+	if (ret)
+		return ret;
+
+	/*
+	 * the one argument is actually optional
+	 */
+	if (ureq && copy_from_user(&req, ureq, sizeof(req)))
+		return -EFAULT;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED, &flags, &resume);
+	if (!ret)
+		ret = __pfm_start(ctx, ureq ? &req : NULL);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	pfm_release_ctx_from_fd(&cookie);
+	return ret;
+}
+
+asmlinkage long sys_pfm_load_context(int fd, struct pfarg_load __user *ureq)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct pfm_syscall_cookie cookie;
+	void *resume, *dummy_resume;
+	unsigned long flags;
+	struct pfarg_load req;
+	int ret;
+
+	PFM_DBG("fd=%d req=%p", fd, ureq);
+
+	if (copy_from_user(&req, ureq, sizeof(req)))
+		return -EFAULT;
+
+	ret = pfm_acquire_ctx_from_fd(fd, &ctx, &cookie);
+	if (ret)
+		return ret;
+
+	task = current;
+
+	/*
+	 * in per-thread mode (not self-monitoring), get a reference
+	 * on task to monitor. This must be done with interrupts enabled
+	 * Upon succesful return, refcount on task is increased.
+	 *
+	 * fget_light() is protecting the context.
+	 */
+	if (!ctx->flags.system && req.load_pid != current->pid) {
+		ret = pfm_get_task(ctx, req.load_pid, &task, &resume);
+		if (ret)
+			goto error;
+	}
+
+	/*
+	 * irqsave is required to avoid race in case context is already
+	 * loaded or with switch timeout in the case of self-monitoring
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_UNLOADED, &flags, &dummy_resume);
+	if (!ret) {
+		/*
+		 * we can safely unmask interrupts because we know monitoring
+		 * is stopped
+		 */
+		local_irq_restore(flags);
+		ret = __pfm_load_context(ctx, &req, task);
+		spin_unlock(&ctx->lock);
+	} else {
+		spin_unlock_irqrestore(&ctx->lock, flags);
+	}
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	/*
+	 * in per-thread mode (not self-monitoring), we need
+	 * to decrease refcount on task to monitor:
+	 *   - load successful: we have a reference to the task in ctx->task
+	 *   - load failed    : undo the effect of pfm_get_task()
+	 */
+	if (task != current)
+		put_task_struct(task);
+error:
+	pfm_release_ctx_from_fd(&cookie);
+	return ret;
+}
+
+asmlinkage long sys_pfm_unload_context(int fd)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct pfm_syscall_cookie cookie;
+	void *resume;
+	unsigned long flags;
+	int ret;
+	int is_system, release_info = 0;
+	u32 cpu;
+
+	PFM_DBG("fd=%d", fd);
+
+	ret = pfm_acquire_ctx_from_fd(fd, &ctx, &cookie);
+	if (ret)
+		return ret;
+
+	is_system = ctx->flags.system;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	cpu = ctx->cpu;
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_STOPPED|PFM_CMD_UNLOAD,
+				   &flags, &resume);
+	if (!ret)
+		ret = __pfm_unload_context(ctx, &release_info);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	/*
+	 * cancel time now that context is unlocked
+	 * avoid race with pfm_handle_switch_timeout()
+	 */
+	if (release_info & 0x2) {
+		int r;
+		r = hrtimer_cancel(&__get_cpu_var(pfm_hrtimer));
+		PFM_DBG("timeout cancel=%d", r);
+	}
+
+	if (release_info & 0x1)
+		pfm_session_release(is_system, cpu);
+
+	pfm_release_ctx_from_fd(&cookie);
+	return ret;
+}
+
+asmlinkage long sys_pfm_create_evtsets(int fd, struct pfarg_setdesc __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct pfm_syscall_cookie cookie;
+	struct pfarg_setdesc *req;
+	void *fptr, *resume;
+	unsigned long flags;
+	size_t sz;
+	int ret;
+
+	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
+
+	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq))
+		return -EINVAL;
+
+	sz = count*sizeof(*ureq);
+
+	ret = pfm_acquire_ctx_from_fd(fd, &ctx, &cookie);
+	if (ret)
+		return ret;
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	/*
+	 * must mask interrupts because we do not know the state of context,
+	 * could be attached and we could be getting PMU interrupts. So
+	 * we mask and lock context and we check and possibly relax masking
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_UNLOADED, &flags, &resume);
+	if (!ret)
+		ret = __pfm_create_evtsets(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+	/*
+	 * context must be unloaded for this command. The resume pointer
+	 * is necessarily NULL, thus no need to call pfm_resume_task()
+	 */
+	kfree(fptr);
+
+error:
+	pfm_release_ctx_from_fd(&cookie);
+	return ret;
+}
+
+asmlinkage long  sys_pfm_getinfo_evtsets(int fd, struct pfarg_setinfo __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct task_struct *task;
+	struct pfm_syscall_cookie cookie;
+	struct pfarg_setinfo *req;
+	void *fptr, *resume;
+	unsigned long flags;
+	size_t sz;
+	int ret;
+
+	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
+
+	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq))
+		return -EINVAL;
+
+	sz = count*sizeof(*ureq);
+
+	ret = pfm_acquire_ctx_from_fd(fd, &ctx, &cookie);
+	if (ret)
+		return ret;
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	/*
+	 * this command operates even when context is loaded, so we need
+	 * to keep interrupts masked to avoid a race with PMU interrupt
+	 * which may switch the active set
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	task = ctx->task;
+
+	ret = pfm_check_task_state(ctx, 0, &flags, &resume);
+	if (!ret)
+		ret = __pfm_getinfo_evtsets(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+
+	if (resume)
+		pfm_resume_task(task, resume);
+
+	if (copy_to_user(ureq, req, sz))
+		ret = -EFAULT;
+
+	kfree(fptr);
+error:
+	pfm_release_ctx_from_fd(&cookie);
+	return ret;
+}
+
+asmlinkage long sys_pfm_delete_evtsets(int fd, struct pfarg_setinfo __user *ureq, int count)
+{
+	struct pfm_context *ctx;
+	struct pfm_syscall_cookie cookie;
+	struct pfarg_setinfo *req;
+	void *fptr, *resume;
+	unsigned long flags;
+	size_t sz;
+	int ret;
+
+	PFM_DBG("fd=%d req=%p count=%d", fd, ureq, count);
+
+	if (count < 0 || count >= PFM_MAX_ARG_COUNT(ureq))
+		return -EINVAL;
+
+	sz = count*sizeof(*ureq);
+
+	ret = pfm_acquire_ctx_from_fd(fd, &ctx, &cookie);
+	if (ret)
+		return ret;
+
+	ret = pfm_get_args(ureq, sz, 0, NULL, (void **)&req, &fptr);
+	if (ret)
+		goto error;
+
+	/*
+	 * must mask interrupts because we do not know the state of context,
+	 * could be attached and we could be getting PMU interrupts
+	 */
+	spin_lock_irqsave(&ctx->lock, flags);
+
+	ret = pfm_check_task_state(ctx, PFM_CMD_UNLOADED, &flags, &resume);
+	if (!ret)
+		ret = __pfm_delete_evtsets(ctx, req, count);
+
+	spin_unlock_irqrestore(&ctx->lock, flags);
+	/*
+	 * context must be unloaded for this command. The resume pointer
+	 * is necessarily NULL, thus no need to call pfm_resume_task()
+	 */
+	kfree(fptr);
+
+error:
+	pfm_release_ctx_from_fd(&cookie);
+	return ret;
+}
Index: linux-2.6.31-master/perfmon/perfmon_sysfs.c
===================================================================
--- /dev/null
+++ linux-2.6.31-master/perfmon/perfmon_sysfs.c
@@ -0,0 +1,525 @@
+/*
+ * perfmon_sysfs.c: perfmon2 sysfs interface
+ *
+ * This file implements the perfmon2 interface which
+ * provides access to the hardware performance counters
+ * of the host processor.
+ *
+ * The initial version of perfmon.c was written by
+ * Ganesh Venkitachalam, IBM Corp.
+ *
+ * Then it was modified for perfmon-1.x by Stephane Eranian and
+ * David Mosberger, Hewlett Packard Co.
+ *
+ * Version Perfmon-2.x is a complete rewrite of perfmon-1.x
+ * by Stephane Eranian, Hewlett Packard Co.
+ *
+ * Copyright (c) 1999-2006 Hewlett-Packard Development Company, L.P.
+ * Contributed by Stephane Eranian <eranian@hpl.hp.com>
+ *                David Mosberger-Tang <davidm@hpl.hp.com>
+ *
+ * More information about perfmon available at:
+ * 	http://perfmon2.sf.net
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+ * 02111-1307 USA
+ */
+#include <linux/kernel.h>
+#include <linux/module.h> /* for EXPORT_SYMBOL */
+#include <linux/perfmon_kern.h>
+#include "perfmon_priv.h"
+
+struct pfm_attribute {
+	struct attribute attr;
+	ssize_t (*show)(void *, struct pfm_attribute *attr, char *);
+	ssize_t (*store)(void *, const char *, size_t);
+};
+#define to_attr(n) container_of(n, struct pfm_attribute, attr);
+
+#define PFM_RO_ATTR(_name, _show) \
+	struct kobj_attribute attr_##_name = __ATTR(_name, 0444, _show, NULL)
+
+#define PFM_RW_ATTR(_name, _show, _store) 			\
+	struct kobj_attribute attr_##_name = __ATTR(_name, 0644, _show, _store)
+
+#define PFM_ROS_ATTR(_name, _show) \
+	struct pfm_attribute attr_##_name = __ATTR(_name, 0444, _show, NULL)
+
+#define is_attr_name(a, n) (!strcmp((a)->attr.name, n))
+int pfm_sysfs_add_pmu(struct pfm_pmu_config *pmu);
+
+static struct kobject *pfm_kernel_kobj, *pfm_fmt_kobj;
+static struct kobject *pfm_pmu_kobj;
+
+static ssize_t pfm_regs_attr_show(struct kobject *kobj,
+		struct attribute *attr, char *buf)
+{
+	struct pfm_regmap_desc *reg = to_reg(kobj);
+	struct pfm_attribute *attribute = to_attr(attr);
+	return attribute->show ? attribute->show(reg, attribute, buf) : -EIO;
+}
+
+static ssize_t pfm_fmt_attr_show(struct kobject *kobj,
+		struct attribute *attr, char *buf)
+{
+	struct pfm_smpl_fmt *fmt = to_smpl_fmt(kobj);
+	struct pfm_attribute *attribute = to_attr(attr);
+	return attribute->show ? attribute->show(fmt, attribute, buf) : -EIO;
+}
+
+static struct sysfs_ops pfm_regs_sysfs_ops = {
+	.show  = pfm_regs_attr_show
+};
+
+static struct sysfs_ops pfm_fmt_sysfs_ops = {
+	.show = pfm_fmt_attr_show
+};
+
+static struct kobj_type pfm_regs_ktype = {
+	.sysfs_ops = &pfm_regs_sysfs_ops,
+};
+
+static struct kobj_type pfm_fmt_ktype = {
+	.sysfs_ops = &pfm_fmt_sysfs_ops,
+};
+
+static ssize_t pfm_controls_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
+{
+	int base;
+
+	if (is_attr_name(attr, "version"))
+		return snprintf(buf, PAGE_SIZE, "%u.%u\n",  PFM_VERSION_MAJ, PFM_VERSION_MIN);
+
+	if (is_attr_name(attr, "task_sessions_count"))
+		return pfm_sysfs_res_show(buf, PAGE_SIZE, 0);
+
+	if (is_attr_name(attr, "debug"))
+		return snprintf(buf, PAGE_SIZE, "%d\n", pfm_controls.debug);
+
+	if (is_attr_name(attr, "task_group"))
+		return snprintf(buf, PAGE_SIZE, "%d\n", pfm_controls.task_group);
+
+	if (is_attr_name(attr, "mode"))
+		return snprintf(buf, PAGE_SIZE, "%d\n", pfm_controls.flags);
+
+	if (is_attr_name(attr, "arg_mem_max"))
+		return snprintf(buf, PAGE_SIZE, "%zu\n", pfm_controls.arg_mem_max);
+
+	if (is_attr_name(attr, "syscall")) {
+		base = pfm_arch_get_base_syscall();
+		return snprintf(buf, PAGE_SIZE, "%d\n",  base);
+	}
+
+	if (is_attr_name(attr, "sys_sessions_count"))
+		return pfm_sysfs_res_show(buf, PAGE_SIZE, 1);
+
+	if (is_attr_name(attr, "smpl_buffer_mem_max"))
+		return snprintf(buf, PAGE_SIZE, "%zu\n", pfm_controls.smpl_buffer_mem_max);
+
+	if (is_attr_name(attr, "smpl_buffer_mem_cur"))
+		return pfm_sysfs_res_show(buf, PAGE_SIZE, 2);
+
+	if (is_attr_name(attr, "sys_group"))
+		return snprintf(buf, PAGE_SIZE, "%d\n", pfm_controls.sys_group);
+
+	/* XXX: could be set to write-only */
+	if (is_attr_name(attr, "reset_stats")) {
+		buf[0] = '0';
+		buf[1] = '\0';
+		return strnlen(buf, PAGE_SIZE);
+	}
+	return 0;
+}
+
+static ssize_t pfm_controls_store(struct kobject *kobj, struct kobj_attribute *attr,
+				  const char *buf, size_t count)
+{
+	int i;
+	size_t d;
+
+	if (sscanf(buf, "%zu", &d) != 1)
+		goto skip;
+
+	if (is_attr_name(attr, "debug"))
+		pfm_controls.debug = d;
+
+	if (is_attr_name(attr, "task_group"))
+		pfm_controls.task_group = d;
+
+	if (is_attr_name(attr, "sys_group"))
+		pfm_controls.sys_group = d;
+
+	if (is_attr_name(attr, "mode"))
+                pfm_controls.flags = d;
+
+	if (is_attr_name(attr, "arg_mem_max")) {
+		/*
+		 * we impose a page as the minimum.
+		 *
+		 * This limit may be smaller than the stack buffer
+		 * available and that is fine.
+		 */
+		if (d >= PAGE_SIZE)
+			pfm_controls.arg_mem_max = d;
+	}
+	if (is_attr_name(attr, "reset_stats")) {
+		for_each_online_cpu(i) {
+			pfm_reset_stats(i);
+		}
+	}
+
+	if (is_attr_name(attr, "smpl_buffer_mem_max")) {
+		if (d >= PAGE_SIZE)
+			pfm_controls.smpl_buffer_mem_max = d;
+	}
+skip:
+	return count;
+}
+
+/*
+ * /sys/kernel/perfmon attributes
+ */
+static PFM_RO_ATTR(version, pfm_controls_show);
+static PFM_RO_ATTR(task_sessions_count, pfm_controls_show);
+static PFM_RO_ATTR(syscall, pfm_controls_show);
+static PFM_RO_ATTR(sys_sessions_count, pfm_controls_show);
+static PFM_RO_ATTR(smpl_buffer_mem_cur, pfm_controls_show);
+
+static PFM_RW_ATTR(debug, pfm_controls_show, pfm_controls_store);
+static PFM_RW_ATTR(task_group, pfm_controls_show, pfm_controls_store);
+static PFM_RW_ATTR(mode, pfm_controls_show, pfm_controls_store);
+static PFM_RW_ATTR(sys_group, pfm_controls_show, pfm_controls_store);
+static PFM_RW_ATTR(arg_mem_max, pfm_controls_show, pfm_controls_store);
+static PFM_RW_ATTR(smpl_buffer_mem_max, pfm_controls_show, pfm_controls_store);
+static PFM_RW_ATTR(reset_stats, pfm_controls_show, pfm_controls_store);
+
+static struct attribute *pfm_kernel_attrs[] = {
+	&attr_version.attr,
+	&attr_syscall.attr,
+	&attr_task_sessions_count.attr,
+	&attr_sys_sessions_count.attr,
+	&attr_smpl_buffer_mem_cur.attr,
+	&attr_debug.attr,
+	&attr_reset_stats.attr,
+	&attr_sys_group.attr,
+	&attr_task_group.attr,
+        &attr_mode.attr,
+	&attr_smpl_buffer_mem_max.attr,
+	&attr_arg_mem_max.attr,
+	NULL
+};
+
+static struct attribute_group pfm_kernel_attr_group = {
+	.attrs = pfm_kernel_attrs,
+};
+
+/*
+ * per-reg attributes
+ */
+static ssize_t pfm_reg_show(void *data, struct pfm_attribute *attr, char *buf)
+{
+	struct pfm_regmap_desc *reg;
+	int w;
+
+	reg = data;
+
+	if (is_attr_name(attr, "name"))
+		return snprintf(buf, PAGE_SIZE, "%s\n", reg->desc);
+
+	if (is_attr_name(attr, "dfl_val"))
+		return snprintf(buf, PAGE_SIZE, "0x%llx\n",
+				(unsigned long long)reg->dfl_val);
+
+	if (is_attr_name(attr, "width")) {
+		w = (reg->type & PFM_REG_C64) ?
+		    pfm_pmu_conf->counter_width : 64;
+		return snprintf(buf, PAGE_SIZE, "%d\n", w);
+	}
+
+	if (is_attr_name(attr, "rsvd_msk"))
+		return snprintf(buf, PAGE_SIZE, "0x%llx\n",
+				(unsigned long long)reg->rsvd_msk);
+
+	if (is_attr_name(attr, "addr"))
+		return snprintf(buf, PAGE_SIZE, "0x%lx\n", reg->hw_addr);
+
+	return 0;
+}
+
+static PFM_ROS_ATTR(name, pfm_reg_show);
+static PFM_ROS_ATTR(dfl_val, pfm_reg_show);
+static PFM_ROS_ATTR(rsvd_msk, pfm_reg_show);
+static PFM_ROS_ATTR(width, pfm_reg_show);
+static PFM_ROS_ATTR(addr, pfm_reg_show);
+
+static struct attribute *pfm_reg_attrs[] = {
+	&attr_name.attr,
+	&attr_dfl_val.attr,
+	&attr_rsvd_msk.attr,
+	&attr_width.attr,
+	&attr_addr.attr,
+	NULL
+};
+
+static struct attribute_group pfm_reg_attr_group = {
+	.attrs = pfm_reg_attrs,
+};
+
+static ssize_t pfm_pmu_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
+{
+	if (is_attr_name(attr, "model"))
+		return snprintf(buf, PAGE_SIZE, "%s\n", pfm_pmu_conf->pmu_name);
+	return 0;
+}
+static PFM_RO_ATTR(model, pfm_pmu_show);
+
+static struct attribute *pfm_pmu_desc_attrs[] = {
+	&attr_model.attr,
+	NULL
+};
+
+static struct attribute_group pfm_pmu_desc_attr_group = {
+	.attrs = pfm_pmu_desc_attrs,
+};
+
+static int pfm_sysfs_add_pmu_regs(struct pfm_pmu_config *pmu)
+{
+	struct pfm_regmap_desc *reg;
+	unsigned int i, k;
+	int ret;
+
+	reg = pmu->pmc_desc;
+	for (i = 0; i < pmu->num_pmc_entries; i++, reg++) {
+
+		if (!(reg->type & PFM_REG_I))
+			continue;
+
+		ret = kobject_init_and_add(&reg->kobj, &pfm_regs_ktype,
+					   pfm_pmu_kobj, "pmc%u", i);
+		if (ret)
+			goto undo_pmcs;
+
+		ret = sysfs_create_group(&reg->kobj, &pfm_reg_attr_group);
+		if (ret) {
+			kobject_del(&reg->kobj);
+			goto undo_pmcs;
+		}
+	}
+
+	reg = pmu->pmd_desc;
+	for (i = 0; i < pmu->num_pmd_entries; i++, reg++) {
+
+		if (!(reg->type & PFM_REG_I))
+			continue;
+
+		ret = kobject_init_and_add(&reg->kobj, &pfm_regs_ktype,
+					   pfm_pmu_kobj, "pmd%u", i);
+		if (ret)
+			goto undo_pmds;
+
+		ret = sysfs_create_group(&reg->kobj, &pfm_reg_attr_group);
+		if (ret) {
+			kobject_del(&reg->kobj);
+			goto undo_pmds;
+		}
+	}
+	return 0;
+undo_pmds:
+	reg = pmu->pmd_desc;
+	for (k = 0; k < i; k++, reg++) {
+		if (!(reg->type & PFM_REG_I))
+			continue;
+		sysfs_remove_group(&reg->kobj, &pfm_reg_attr_group);
+		kobject_del(&reg->kobj);
+	}
+	i = pmu->num_pmc_entries;
+	/* fall through */
+undo_pmcs:
+	reg = pmu->pmc_desc;
+	for (k = 0; k < i; k++, reg++) {
+		if (!(reg->type & PFM_REG_I))
+			continue;
+		sysfs_remove_group(&reg->kobj, &pfm_reg_attr_group);
+		kobject_del(&reg->kobj);
+	}
+	return ret;
+}
+
+static int pfm_sysfs_del_pmu_regs(struct pfm_pmu_config *pmu)
+{
+	struct pfm_regmap_desc *reg;
+	unsigned int i;
+
+	reg = pmu->pmc_desc;
+	for (i = 0; i < pmu->num_pmc_entries; i++, reg++) {
+
+		if (!(reg->type & PFM_REG_I))
+			continue;
+
+		sysfs_remove_group(&reg->kobj, &pfm_reg_attr_group);
+		kobject_del(&reg->kobj);
+	}
+
+	reg = pmu->pmd_desc;
+	for (i = 0; i < pmu->num_pmd_entries; i++, reg++) {
+
+		if (!(reg->type & PFM_REG_I))
+			continue;
+
+		sysfs_remove_group(&reg->kobj, &pfm_reg_attr_group);
+		kobject_del(&reg->kobj);
+	}
+	return 0;
+}
+
+/*
+ * when a PMU description module is inserted, we create
+ * a pmu_desc subdir in sysfs and we populate it with
+ * PMU specific information, such as register mappings
+ */
+int pfm_sysfs_add_pmu(struct pfm_pmu_config *pmu)
+{
+	int ret;
+
+	pfm_pmu_kobj = kobject_create_and_add("pmu_desc", pfm_kernel_kobj);
+	if (!pfm_pmu_kobj)
+		return -ENOMEM;
+
+	ret = sysfs_create_group(pfm_pmu_kobj, &pfm_pmu_desc_attr_group);
+	if (ret) {
+		/* will release pfm_pmu_kobj */
+		kobject_put(pfm_pmu_kobj);
+		return ret;
+	}
+
+	ret = pfm_sysfs_add_pmu_regs(pmu);
+	if (ret) {
+		sysfs_remove_group(pfm_pmu_kobj, &pfm_pmu_desc_attr_group);
+		/* will release pfm_pmu_kobj */
+		kobject_put(pfm_pmu_kobj);
+	} else
+		kobject_uevent(pfm_pmu_kobj, KOBJ_ADD);
+
+	return ret;
+}
+
+/*
+ * when a PMU description module is removed, we also remove
+ * all its information from sysfs, i.e., the pmu_desc subdir
+ * disappears
+ */
+int pfm_sysfs_remove_pmu(struct pfm_pmu_config *pmu)
+{
+	pfm_sysfs_del_pmu_regs(pmu);
+	sysfs_remove_group(pfm_pmu_kobj, &pfm_pmu_desc_attr_group);
+	kobject_uevent(pfm_pmu_kobj, KOBJ_REMOVE);
+	kobject_put(pfm_pmu_kobj);
+	pfm_pmu_kobj = NULL;
+	return 0;
+}
+
+static ssize_t pfm_fmt_show(void *data, struct pfm_attribute *attr, char *buf)
+{
+	struct pfm_smpl_fmt *fmt = data;
+
+	if (is_attr_name(attr, "version"))
+		return snprintf(buf, PAGE_SIZE, "%u.%u\n",
+			fmt->fmt_version >> 16 & 0xffff,
+			fmt->fmt_version & 0xffff);
+	return 0;
+}
+
+/*
+ * do not use predefined macros because of name conflict
+ * with /sys/kernel/perfmon/version
+ */
+struct pfm_attribute attr_fmt_version = {
+	.attr	= { .name = "version", .mode = 0444 },
+	.show	= pfm_fmt_show,
+};
+
+static struct attribute *pfm_fmt_attrs[] = {
+	&attr_fmt_version.attr,
+	NULL
+};
+
+static struct attribute_group pfm_fmt_attr_group = {
+	.attrs = pfm_fmt_attrs,
+};
+
+/*
+ * when a sampling format module is inserted, we populate
+ * sysfs with some information
+ */
+int pfm_sysfs_add_fmt(struct pfm_smpl_fmt *fmt)
+{
+	int ret;
+
+	ret = kobject_init_and_add(&fmt->kobj, &pfm_fmt_ktype,
+				   pfm_fmt_kobj, fmt->fmt_name);
+	if (ret)
+		return ret;
+
+	ret = sysfs_create_group(&fmt->kobj, &pfm_fmt_attr_group);
+	if (ret)
+		kobject_del(&fmt->kobj);
+	else
+		kobject_uevent(&fmt->kobj, KOBJ_ADD);
+
+	return ret;
+}
+
+/*
+ * when a sampling format module is removed, its information
+ * must also be removed from sysfs
+ */
+void pfm_sysfs_remove_fmt(struct pfm_smpl_fmt *fmt)
+{
+	sysfs_remove_group(&fmt->kobj, &pfm_fmt_attr_group);
+	kobject_uevent(&fmt->kobj, KOBJ_REMOVE);
+	kobject_del(&fmt->kobj);
+}
+
+int __init pfm_init_sysfs(void)
+{
+	int ret;
+
+	pfm_kernel_kobj = kobject_create_and_add("perfmon", kernel_kobj);
+	if (!pfm_kernel_kobj) {
+		PFM_ERR("cannot add kernel object: /sys/kernel/perfmon");
+		return -ENOMEM;
+	}
+
+	ret = sysfs_create_group(pfm_kernel_kobj, &pfm_kernel_attr_group);
+	if (ret) {
+		kobject_put(pfm_kernel_kobj);
+		return ret;
+	}
+
+	pfm_fmt_kobj = kobject_create_and_add("formats", pfm_kernel_kobj);
+	if (ret) {
+		PFM_ERR("cannot add fmt object: %d", ret);
+		goto error_fmt;
+	}
+	if (pfm_pmu_conf)
+		pfm_sysfs_add_pmu(pfm_pmu_conf);
+
+	pfm_sysfs_builtin_fmt_add();
+
+	return 0;
+
+error_fmt:
+	kobject_del(pfm_kernel_kobj);
+	return ret;
+}
