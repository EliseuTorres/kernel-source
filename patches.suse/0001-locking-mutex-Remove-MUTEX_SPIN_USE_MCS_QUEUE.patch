From c3161da01ca632e3cca7c992ab775d7247142dbb Mon Sep 17 00:00:00 2001
From: Davidlohr Bueso <dbueso@suse.de>
Date: Wed, 10 Dec 2014 16:43:52 -0800
Subject: [PATCH] locking/mutex: Remove MUTEX_SPIN_USE_MCS_QUEUE
Git-commit: No, bringing another patch in line with upstream
Patch-mainline: No, bringing another patch in line with upstream
References: fate#317232

The use of mutex optimistic spinning will now only depend
on MUTEX_SPIN_ON_OWNER, which in turn depends on (SMP &&
!DEBUG_MUTEXES).

SUSE note: Unlike upstream, it does not account for ARCH_SUPPORTS_ATOMIC_RMW,
as we do not support archs that do not play nice with regular
stores and cmpxchg(), such as parisc, sparc32, tile32, metag and
hexagon.

Signed-off-by: Davidlohr Bueso <dbueso@suse.de>

---
 include/linux/mutex.h |  4 ++--
 kernel/Kconfig.locks  | 12 ------------
 kernel/mutex-debug.c  |  2 +-
 kernel/mutex.c        |  8 ++++----
 kernel/mutex.h        |  6 +++---
 5 files changed, 10 insertions(+), 22 deletions(-)

diff --git a/include/linux/mutex.h b/include/linux/mutex.h
index 4404d9e..a3f1474 100644
--- a/include/linux/mutex.h
+++ b/include/linux/mutex.h
@@ -49,7 +49,7 @@ struct mutex {
 	/* 1: unlocked, 0: locked, negative: locked, possible waiters */
 	atomic_t		count;
 	spinlock_t		wait_lock;
-#if defined(CONFIG_MUTEX_SPIN_USE_MCS_QUEUE)
+#if defined(CONFIG_MUTEX_SPIN_ON_OWNER)
 	struct list_head	*wait_list;
 	void *			*spin_mlock;
 #else
@@ -107,7 +107,7 @@ do {							\
 # define __DEP_MAP_MUTEX_INITIALIZER(lockname)
 #endif
 
-#if defined(CONFIG_MUTEX_SPIN_USE_MCS_QUEUE)
+#if defined(CONFIG_MUTEX_SPIN_ON_OWNER)
 # define MUTEX_WAIT_LIST_INIT(x)	(struct list_head *)&(x)
 # define MUTEX_INIT_WAIT_LIST(x)	*(x) = (struct list_head *)(x)
 # define MUTEX_LIST_EMPTY(x)		(*(x) == (struct list_head *)(x))
diff --git a/kernel/Kconfig.locks b/kernel/Kconfig.locks
index e52106f..5068e2a 100644
--- a/kernel/Kconfig.locks
+++ b/kernel/Kconfig.locks
@@ -200,15 +200,3 @@ config INLINE_WRITE_UNLOCK_IRQRESTORE
 
 config MUTEX_SPIN_ON_OWNER
 	def_bool SMP && !DEBUG_MUTEXES
-
-#
-# The following option has kABI impact and should only be enabled on special
-# kernel flavor with limited distribution.
-#
-config MUTEX_SPIN_USE_MCS_QUEUE
-	bool "mutex: spin using mcs queue"
-	depends on MUTEX_SPIN_ON_OWNER
-	help
-	  This option enables MCS locking but is kABI-incompatible with
-	  the non-MCS locking variant.
-	bool
diff --git a/kernel/mutex-debug.c b/kernel/mutex-debug.c
index c56ab87..f2e1148 100644
--- a/kernel/mutex-debug.c
+++ b/kernel/mutex-debug.c
@@ -78,7 +78,7 @@ void debug_mutex_unlock(struct mutex *lock)
 			DEBUG_LOCKS_WARN_ON(!lock->owner);
 		else
 			DEBUG_LOCKS_WARN_ON(lock->owner != current);
-#if defined(CONFIG_MUTEX_SPIN_USE_MCS_QUEUE)
+#if defined(CONFIG_MUTEX_SPIN_ON_OWNER)
 		DEBUG_LOCKS_WARN_ON(!lock->wait_list)
 #else
 		DEBUG_LOCKS_WARN_ON(!lock->wait_list.prev && !lock->wait_list.next);
diff --git a/kernel/mutex.c b/kernel/mutex.c
index dd019be..7faba12 100644
--- a/kernel/mutex.c
+++ b/kernel/mutex.c
@@ -57,7 +57,7 @@ __mutex_init(struct mutex *lock, const char *name, struct lock_class_key *key)
 	spin_lock_init(&lock->wait_lock);
 	MUTEX_INIT_WAIT_LIST(&lock->wait_list);
 	mutex_clear_owner(lock);
-#ifdef CONFIG_MUTEX_SPIN_USE_MCS_QUEUE
+#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
 	lock->spin_mlock = NULL;
 #endif
 
@@ -111,7 +111,7 @@ void __sched mutex_lock(struct mutex *lock)
 EXPORT_SYMBOL(mutex_lock);
 #endif
 
-#ifdef CONFIG_MUTEX_SPIN_USE_MCS_QUEUE
+#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
 /*
  * In order to avoid a stampede of mutex spinners from acquiring the mutex
  * more or less simultaneously, the spinners need to acquire a MCS lock
@@ -330,13 +330,13 @@ static inline int mutex_can_spin_on_owner(struct mutex *lock)
 	 */
 	return retval;
 }
-#else /* CONFIG_MUTEX_SPIN_USE_MCS_QUEUE */
+#else /* CONFIG_MUTEX_SPIN_ON_OWNER */
 
 #define osq_lock(mutex, node) (1)
 #define osq_unlock(mutex) do { } while(0)
 #define mutex_can_spin_on_owner(a) (1)
 
-#endif /* CONFIG_MUTEX_SPIN_USE_MCS_QUEUE */
+#endif /* CONFIG_MUTEX_SPIN_ON_OWNER */
 
 static __used noinline void __sched __mutex_unlock_slowpath(atomic_t *lock_count);
 
diff --git a/kernel/mutex.h b/kernel/mutex.h
index 52a401a..dd4ca76 100644
--- a/kernel/mutex.h
+++ b/kernel/mutex.h
@@ -14,7 +14,7 @@
 #define spin_unlock_mutex(lock, flags) \
 		do { spin_unlock(lock); (void)(flags); } while (0)
 
-#ifdef CONFIG_MUTEX_SPIN_USE_MCS_QUEUE
+#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
 /*
  * If CONFIG_MUTEX_SPIN_ON_OWNER and CONFIG_USE_MCS_SPIN_WAIT_QUEUE are
  * enabled, the content of the double pointers in wait_list will be
@@ -73,13 +73,13 @@ __mutex_remove_waiter(struct mutex *lock, struct list_head *waiter)
 	}
 }
 
-#else /* MUTEX_SPIN_USE_MCS_QUEUE */
+#else /* MUTEX_SPIN_ON_OWNER */
 
 #define mutex_add_waiter(waiter, list)	list_add_tail(waiter, list)
 #define mutex_waitlist_head(lock)	(lock)->wait_list.next
 #define mutex_remove_waiter(lock, waiter, ti) \
 		__list_del((waiter)->list.prev, (waiter)->list.next)
-#endif /* MUTEX_SPIN_USE_MCS_QUEUE */
+#endif /* MUTEX_SPIN_ON_OWNER */
 
 #ifdef CONFIG_SMP
 static inline void mutex_set_owner(struct mutex *lock)
-- 
2.1.2

