From a1228e1e07f39ae3527f2402767e677ba8d488ab Mon Sep 17 00:00:00 2001
From: David Sterba <dsterba@suse.cz>
Date: Thu, 8 Dec 2011 01:15:35 +0100
Patch-mainline: no
Subject: [PATCH] setpagedirty tracing v4

Signed-off-by: David Sterba <dsterba@suse.cz>
---
 fs/btrfs/extent_io.c       |   14 ++++++++++++++
 fs/btrfs/inode.c           |   11 +++++++++++
 fs/btrfs/ordered-data.c    |    3 +++
 include/linux/mm_types.h   |   10 ++++++++++
 include/linux/page-flags.h |   34 ++++++++++++++++++++++++++++++++++
 mm/filemap.c               |   27 +++++++++++++++++++++++++--
 mm/page_alloc.c            |    3 +++
 7 files changed, 100 insertions(+), 2 deletions(-)

--- a/fs/btrfs/extent_io.c
+++ b/fs/btrfs/extent_io.c
@@ -2863,6 +2863,8 @@ static int __extent_writepage(struct pag
 	int write_flags;
 	unsigned long nr_written = 0;
 	bool fill_delalloc = true;
+	int fda_trace = 0;
+	int fda_0_counter = 0;
 
 	if (wbc->sync_mode == WB_SYNC_ALL)
 		write_flags = WRITE_SYNC;
@@ -2909,6 +2911,7 @@ static int __extent_writepage(struct pag
 		 * to this page.
 		 */
 		update_nr_written(page, wbc, 0);
+		fda_trace|=(1<<1);;
 
 		while (delalloc_end < page_end) {
 			nr_delalloc = find_lock_delalloc_range(inode, tree,
@@ -2917,7 +2920,9 @@ static int __extent_writepage(struct pag
 						       &delalloc_end,
 						       128 * 1024 * 1024);
 			if (nr_delalloc == 0) {
+				fda_0_counter++;
 				delalloc_start = delalloc_end + 1;
+				fda_trace|=(1<<2);;
 				continue;
 			}
 			tree->ops->fill_delalloc(inode, page, delalloc_start,
@@ -2932,10 +2937,12 @@ static int __extent_writepage(struct pag
 					      PAGE_CACHE_SIZE) >>
 					      PAGE_CACHE_SHIFT;
 			delalloc_start = delalloc_end + 1;
+			fda_trace|=(1<<3);;
 		}
 		if (wbc->nr_to_write < delalloc_to_write) {
 			int thresh = 8192;
 
+			fda_trace|=(1<<4);;
 			if (delalloc_to_write < thresh * 2)
 				thresh = delalloc_to_write;
 			wbc->nr_to_write = min_t(u64, delalloc_to_write,
@@ -2955,11 +2962,18 @@ static int __extent_writepage(struct pag
 			wbc->nr_to_write -= nr_written;
 			goto done_unlocked;
 		}
+		fda_trace|=(1<<5);;
 	}
 	if (tree->ops && tree->ops->writepage_start_hook) {
 		ret = tree->ops->writepage_start_hook(page, start,
 						      page_end);
 		if (ret == -EAGAIN) {
+			if (BTRFS_I(inode)->flags & BTRFS_INODE_PREALLOC)
+				printk(KERN_ERR "fixup: inode %lld is PREALLOC\n",
+						(unsigned long long)btrfs_ino(inode));
+			printk(KERN_ERR "fixup: start_hook for %p 0x%lx, fill_da %d fda_trace %x 0_counter %d\n",
+					page, page->flags, fill_delalloc, fda_trace, fda_0_counter);
+			WARN_ON(1);
 			redirty_page_for_writepage(wbc, page);
 			update_nr_written(page, wbc, nr_written);
 			unlock_page(page);
--- a/fs/btrfs/inode.c
+++ b/fs/btrfs/inode.c
@@ -1591,6 +1591,8 @@ static void btrfs_writepage_fixup_worker
 	fixup = container_of(work, struct btrfs_writepage_fixup, work);
 	page = fixup->page;
 again:
+	printk(KERN_ERR "btrfs: fixup work: page %p flags=0x%lx\n",
+			page, page->flags);
 	lock_page(page);
 	if (!page->mapping || !PageDirty(page) || !PageChecked(page)) {
 		ClearPageChecked(page);
@@ -1614,8 +1616,12 @@ again:
 				     page_end, &cached_state);
 		unlock_page(page);
 		btrfs_start_ordered_extent(inode, ordered, 1);
+		printk(KERN_ERR "btrfs: fixup had ordered, again\n");
 		goto again;
 	}
+	printk(KERN_ERR "btrfs: fixup failure: page %p flags=0x%lx\n",
+			page, page->flags);
+	print_stack_trace(&page->dirty_trace, 5);
 
 	BUG();
 	btrfs_set_extent_delalloc(inode, page_start, page_end, &cached_state);
@@ -1662,6 +1668,11 @@ static int btrfs_writepage_start_hook(st
 	fixup->work.func = btrfs_writepage_fixup_worker;
 	fixup->page = page;
 	btrfs_queue_worker(&root->fs_info->fixup_workers, &fixup->work);
+	printk(KERN_ERR "btrfs: schedule fixup for %p 0x%lx [%llx-%llx] ino %lld\n",
+			page, page->flags,
+			(unsigned long long)start,
+			(unsigned long long)end,
+			(unsigned long long)btrfs_ino(inode));
 	return -EAGAIN;
 }
 
--- a/fs/btrfs/ordered-data.c
+++ b/fs/btrfs/ordered-data.c
@@ -238,6 +238,9 @@ int btrfs_add_ordered_extent(struct inod
 int btrfs_add_ordered_extent_dio(struct inode *inode, u64 file_offset,
 				 u64 start, u64 len, u64 disk_len, int type)
 {
+	if (BTRFS_I(inode)->force_compress) {
+		printk(KERN_ERR "btrfs: submit DIO with inode compression\n");
+	}
 	return __btrfs_add_ordered_extent(inode, file_offset, start, len,
 					  disk_len, type, 1,
 					  BTRFS_COMPRESS_NONE);
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -12,6 +12,8 @@
 #include <linux/completion.h>
 #include <linux/cpumask.h>
 #include <linux/page-debug-flags.h>
+#include <linux/hrtimer.h>
+#include <linux/stacktrace.h>
 #include <asm/page.h>
 #include <asm/mmu.h>
 
@@ -24,6 +26,8 @@ struct address_space;
 
 #define USE_SPLIT_PTLOCKS	(NR_CPUS >= CONFIG_SPLIT_PTLOCK_CPUS)
 
+#define PAGE_STACKTRACE_SIZE   (12UL)
+
 /*
  * Each physical page in the system has a struct page associated with
  * it to keep track of whatever it is we are using the page for at the
@@ -114,6 +118,12 @@ struct page {
 	 */
 	void *shadow;
 #endif
+	/*
+	 * Debugging
+	 */
+	struct list_head bt_list;
+	struct stack_trace dirty_trace;
+	unsigned long dirty_entries[PAGE_STACKTRACE_SIZE];
 };
 
 typedef unsigned long __nocast vm_flags_t;
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -209,7 +209,41 @@ TESTPAGEFLAG(Locked, locked)
 PAGEFLAG(Waiters, waiters)
 PAGEFLAG(Error, error) TESTCLEARFLAG(Error, error)
 PAGEFLAG(Referenced, referenced) TESTCLEARFLAG(Referenced, referenced)
+#if 0
 PAGEFLAG(Dirty, dirty) TESTSCFLAG(Dirty, dirty) __CLEARPAGEFLAG(Dirty, dirty)
+#else
+extern void save_page_dirty_trace(struct page *pg);
+static inline int PageDirty(const struct page *page)
+{
+        return test_bit((PG_dirty), (&page-> flags));
+}
+static inline void SetPageDirty(struct page *page)
+{
+        set_bit(PG_dirty, &page->flags);
+	save_page_dirty_trace(page);
+}
+static inline void ClearPageDirty(struct page *page)
+{
+        clear_bit(PG_dirty, &page->flags);
+}
+static inline int TestSetPageDirty(struct page *page)
+{
+        int ret = test_and_set_bit(PG_dirty, &page->flags);
+	if (ret == 0)
+		save_page_dirty_trace(page);
+	return ret;
+}
+static inline int TestClearPageDirty(struct page *page)
+{
+        return test_and_clear_bit(PG_dirty, &page->flags);
+}
+static inline void __ClearPageDirty(struct page *page)
+{
+        __clear_bit(PG_dirty, &page->flags);
+}
+
+#endif
+
 PAGEFLAG(LRU, lru) __CLEARPAGEFLAG(LRU, lru)
 PAGEFLAG(Active, active) __CLEARPAGEFLAG(Active, active)
 	TESTCLEARFLAG(Active, active)
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -1198,8 +1198,9 @@ static void shrink_readahead_size_eio(st
  * This is really ugly. But the goto's actually try to clarify some
  * of the logic when it comes to error handling etc.
  */
-static void do_generic_file_read(struct file *filp, loff_t *ppos,
-		read_descriptor_t *desc, read_actor_t actor)
+static noinline void do_generic_file_read(struct file *filp, loff_t *ppos,
+					  read_descriptor_t *desc,
+					  read_actor_t actor)
 {
 	struct address_space *mapping = filp->f_mapping;
 	struct inode *inode = mapping->host;
@@ -2769,3 +2770,25 @@ int try_to_release_page(struct page *pag
 }
 
 EXPORT_SYMBOL(try_to_release_page);
+
+/* Debugging */
+
+void init_page_dirty_trace(struct page *page)
+{
+       INIT_LIST_HEAD(&page->bt_list);
+       page->dirty_trace.nr_entries = 0;
+}
+EXPORT_SYMBOL(init_page_dirty_trace);
+
+void save_page_dirty_trace(struct page *page)
+{
+       page->dirty_trace.nr_entries = 0;
+       page->dirty_trace.max_entries = PAGE_STACKTRACE_SIZE;
+       page->dirty_trace.entries = page->dirty_entries;
+       page->dirty_trace.skip = 1;
+
+       save_stack_trace(&page->dirty_trace);
+       page->dirty_trace.nr_entries--;
+}
+EXPORT_SYMBOL(save_page_dirty_trace);
+
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -767,6 +767,7 @@ static inline int check_new_page(struct
 	return 0;
 }
 
+extern void init_page_dirty_trace(struct page *page);
 static int prep_new_page(struct page *page, int order, gfp_t gfp_flags)
 {
 	int i;
@@ -789,6 +790,8 @@ static int prep_new_page(struct page *pa
 	if (order && (gfp_flags & __GFP_COMP))
 		prep_compound_page(page, order);
 
+	init_page_dirty_trace(page);
+
 	return 0;
 }
 
