From: Jeff Mahoney <jeffm@suse.com>
Subject: btrfs: Use mempools for extent_state structures
Patch-mainline: Submitted 23 Nov 2011 to linux-btrfs

The extent_state structure is used at the core of the extent i/o code
for managing flags, locking, etc. It requires allocations deep in the
write code and if failures occur they are difficult to recover from.

We avoid most of the failures by using a mempool, which can sleep when
required, to honor the allocations. This allows future patches to convert
most of the {set,clear,convert}_extent_bit and derivatives to return
void.

Signed-off-by: Jeff Mahoney <jeffm@suse.com>
---
 fs/btrfs/extent_io.c |   87 +++++++++++++++++++++++++++++++++++++--------------
 1 file changed, 64 insertions(+), 23 deletions(-)

--- a/fs/btrfs/extent_io.c
+++ b/fs/btrfs/extent_io.c
@@ -12,6 +12,7 @@
 #include <linux/pagevec.h>
 #include <linux/prefetch.h>
 #include <linux/cleancache.h>
+#include <linux/mempool.h>
 #include "extent_io.h"
 #include "extent_map.h"
 #include "compat.h"
@@ -21,6 +22,8 @@
 
 static struct kmem_cache *extent_state_cache;
 static struct kmem_cache *extent_buffer_cache;
+static mempool_t *extent_state_pool;
+#define EXTENT_STATE_POOL_SIZE (64*1024)
 
 static LIST_HEAD(buffers);
 static LIST_HEAD(states);
@@ -66,13 +69,22 @@ int __init extent_io_init(void)
 	if (!extent_state_cache)
 		return -ENOMEM;
 
+	extent_state_pool = mempool_create_slab_pool(
+						EXTENT_STATE_POOL_SIZE /
+						sizeof(struct extent_state),
+						extent_state_cache);
+	if (!extent_state_pool)
+		goto free_state_cache;
+
 	extent_buffer_cache = kmem_cache_create("extent_buffers",
 			sizeof(struct extent_buffer), 0,
 			SLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);
 	if (!extent_buffer_cache)
-		goto free_state_cache;
+		goto free_state_mempool;
 	return 0;
 
+free_state_mempool:
+	mempool_destroy(extent_state_pool);
 free_state_cache:
 	kmem_cache_destroy(extent_state_cache);
 	return -ENOMEM;
@@ -103,6 +115,8 @@ void extent_io_exit(void)
 		list_del(&eb->leak_list);
 		kmem_cache_free(extent_buffer_cache, eb);
 	}
+	if (extent_state_pool)
+		mempool_destroy(extent_state_pool);
 	if (extent_state_cache)
 		kmem_cache_destroy(extent_state_cache);
 	if (extent_buffer_cache)
@@ -121,16 +135,11 @@ void extent_io_tree_init(struct extent_i
 	tree->mapping = mapping;
 }
 
-static struct extent_state *alloc_extent_state(gfp_t mask)
+static void init_extent_state(struct extent_state *state)
 {
-	struct extent_state *state;
 #if LEAK_DEBUG
 	unsigned long flags;
 #endif
-
-	state = kmem_cache_alloc(extent_state_cache, mask);
-	if (!state)
-		return state;
 	state->state = 0;
 	state->private = 0;
 	state->tree = NULL;
@@ -141,9 +150,25 @@ static struct extent_state *alloc_extent
 #endif
 	atomic_set(&state->refs, 1);
 	init_waitqueue_head(&state->wq);
+}
+
+static struct extent_state *alloc_extent_state(gfp_t mask)
+{
+	struct extent_state *state;
+
+	state = kmem_cache_alloc(extent_state_cache, mask);
+	if (!state)
+		return state;
+	init_extent_state(state);
 	return state;
 }
 
+static struct extent_state *alloc_extent_state_nofail(gfp_t mask)
+{
+	BUG_ON(!(mask & __GFP_WAIT));
+	return alloc_extent_state(mask);
+}
+
 void free_extent_state(struct extent_state *state)
 {
 	if (!state)
@@ -158,7 +183,11 @@ void free_extent_state(struct extent_sta
 		list_del(&state->leak_list);
 		spin_unlock_irqrestore(&leak_lock, flags);
 #endif
-		kmem_cache_free(extent_state_cache, state);
+		/*
+		 * Going through the mempool lets us replenish the pool
+		 * with objects that weren't necessarily drawn from it.
+		 */
+		mempool_free(state, extent_state_pool);
 	}
 }
 
@@ -435,6 +464,12 @@ static int clear_state_bit(struct extent
 	return ret;
 }
 
+static void
+assert_atomic_alloc(struct extent_state *prealloc, gfp_t mask)
+{
+	WARN_ON(!prealloc && (mask & __GFP_WAIT));
+}
+
 static struct extent_state *
 alloc_extent_state_atomic(struct extent_state *prealloc)
 {
@@ -462,6 +497,7 @@ NORET_TYPE void extent_io_tree_panic(str
  * the range [start, end] is inclusive.
  *
  * This takes the tree lock, and returns 0 on success and < 0 on error.
+ * If (mask & __GFP_WAIT) == 0, there are no error conditions.
  */
 int clear_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,
 		     int bits, int wake, int delete,
@@ -476,6 +512,7 @@ int clear_extent_bit(struct extent_io_tr
 	u64 last_end;
 	int err;
 	int clear = 0;
+	int wait = (mask & __GFP_WAIT);
 
 	if (delete)
 		bits |= ~EXTENT_CTLBITS;
@@ -484,11 +521,8 @@ int clear_extent_bit(struct extent_io_tr
 	if (bits & (EXTENT_IOBITS | EXTENT_BOUNDARY))
 		clear = 1;
 again:
-	if (!prealloc && (mask & __GFP_WAIT)) {
-		prealloc = alloc_extent_state(mask);
-		if (!prealloc)
-			return -ENOMEM;
-	}
+	if (!prealloc && wait)
+		prealloc = alloc_extent_state_nofail(mask);
 
 	spin_lock(&tree->lock);
 	if (cached_state) {
@@ -540,6 +574,7 @@ hit_next:
 	 */
 
 	if (state->start < start) {
+		assert_atomic_alloc(prealloc, mask);
 		prealloc = alloc_extent_state_atomic(prealloc);
 		BUG_ON(!prealloc);
 		err = split_state(tree, state, prealloc, start);
@@ -564,6 +599,7 @@ hit_next:
 	 * on the first half
 	 */
 	if (state->start <= end && state->end > end) {
+		assert_atomic_alloc(prealloc, mask);
 		prealloc = alloc_extent_state_atomic(prealloc);
 		BUG_ON(!prealloc);
 		err = split_state(tree, state, prealloc, end + 1);
@@ -724,15 +760,14 @@ int set_extent_bit(struct extent_io_tree
 	struct extent_state *prealloc = NULL;
 	struct rb_node *node;
 	int err = 0;
+	int wait = mask & __GFP_WAIT;
 	u64 last_start;
 	u64 last_end;
 
 	bits |= EXTENT_FIRST_DELALLOC;
 again:
-	if (!prealloc && (mask & __GFP_WAIT)) {
-		prealloc = alloc_extent_state(mask);
-		BUG_ON(!prealloc);
-	}
+	if (!prealloc && wait)
+		prealloc = alloc_extent_state_nofail(mask);
 
 	spin_lock(&tree->lock);
 	if (cached_state && *cached_state) {
@@ -749,6 +784,7 @@ again:
 	 */
 	node = tree_search(tree, start);
 	if (!node) {
+		assert_atomic_alloc(prealloc, mask);
 		prealloc = alloc_extent_state_atomic(prealloc);
 		BUG_ON(!prealloc);
 		err = insert_state(tree, prealloc, start, end, &bits);
@@ -818,6 +854,7 @@ hit_next:
 			goto out;
 		}
 
+		assert_atomic_alloc(prealloc, mask);
 		prealloc = alloc_extent_state_atomic(prealloc);
 		BUG_ON(!prealloc);
 		err = split_state(tree, state, prealloc, start);
@@ -851,6 +888,7 @@ hit_next:
 		else
 			this_end = last_start - 1;
 
+		assert_atomic_alloc(prealloc, mask);
 		prealloc = alloc_extent_state_atomic(prealloc);
 		BUG_ON(!prealloc);
 
@@ -881,6 +919,7 @@ hit_next:
 			goto out;
 		}
 
+		assert_atomic_alloc(prealloc, mask);
 		prealloc = alloc_extent_state_atomic(prealloc);
 		BUG_ON(!prealloc);
 		err = split_state(tree, state, prealloc, end + 1);
@@ -907,7 +946,7 @@ search_again:
 	if (start > end)
 		goto out;
 	spin_unlock(&tree->lock);
-	if (mask & __GFP_WAIT)
+	if (wait)
 		cond_resched();
 	goto again;
 }
@@ -936,13 +975,11 @@ int convert_extent_bit(struct extent_io_
 	int err = 0;
 	u64 last_start;
 	u64 last_end;
+	int wait = (mask & __GFP_WAIT);
 
 again:
-	if (!prealloc && (mask & __GFP_WAIT)) {
-		prealloc = alloc_extent_state(mask);
-		if (!prealloc)
-			return -ENOMEM;
-	}
+	if (!prealloc && wait)
+		prealloc = alloc_extent_state_nofail(mask);
 
 	spin_lock(&tree->lock);
 	/*
@@ -951,6 +988,7 @@ again:
 	 */
 	node = tree_search(tree, start);
 	if (!node) {
+		assert_atomic_alloc(prealloc, mask);
 		prealloc = alloc_extent_state_atomic(prealloc);
 		if (!prealloc)
 			return -ENOMEM;
@@ -1009,6 +1047,7 @@ hit_next:
 	 * desired bit on it.
 	 */
 	if (state->start < start) {
+		assert_atomic_alloc(prealloc, mask);
 		prealloc = alloc_extent_state_atomic(prealloc);
 		if (!prealloc)
 			return -ENOMEM;
@@ -1042,6 +1081,7 @@ hit_next:
 		else
 			this_end = last_start - 1;
 
+		assert_atomic_alloc(prealloc, mask);
 		prealloc = alloc_extent_state_atomic(prealloc);
 		if (!prealloc)
 			return -ENOMEM;
@@ -1065,6 +1105,7 @@ hit_next:
 	 * on the first half
 	 */
 	if (state->start <= end && state->end > end) {
+		assert_atomic_alloc(prealloc, mask);
 		prealloc = alloc_extent_state_atomic(prealloc);
 		if (!prealloc)
 			return -ENOMEM;
