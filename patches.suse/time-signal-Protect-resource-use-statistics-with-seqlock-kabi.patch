From: Mel Gorman <mgorman@suse.de>
Date: Wed, 18 Feb 2015 15:55:46 +0000
Subject: [PATCH] time, signal: Protect resource use statistics with seqlock
 -kabi

References: Time scalability
Patch-mainline: No, never
Patch.name: patches.suse/time-signal-Protect-resource-use-statistics-with-seqlock-kabi.patch

Workaround KABI problem using KABI padding and kmalloc.

Signed-off-by: Mel Gorman <mgorman@suse.de>
---
 include/linux/sched.h  |  5 ++++-
 kernel/exit.c          |  8 ++++----
 kernel/fork.c          | 10 +++++++++-
 kernel/sched/cputime.c |  6 +++---
 4 files changed, 20 insertions(+), 9 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5c998b90314d..9a81386a0071 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -557,7 +557,6 @@ struct signal_struct {
 	 * Live threads maintain their own counters and add to these
 	 * in __exit_signal, except for the group leader.
 	 */
-	seqlock_t stats_lock;
 	cputime_t utime, stime, cutime, cstime;
 	cputime_t gtime;
 	cputime_t cgtime;
@@ -621,7 +620,11 @@ struct signal_struct {
 	struct mutex cred_guard_mutex;	/* guard against foreign influences on
 					 * credential calculations
 					 * (notably. ptrace) */
+#ifdef __GENKSYMS__
 	void *suse_kabi_padding;
+#else
+	seqlock_t *stats_lock;
+#endif
 };
 
 /*
diff --git a/kernel/exit.c b/kernel/exit.c
index ac23046327d2..db785bd427dc 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -127,7 +127,7 @@ static void __exit_signal(struct task_struct *tsk)
 	 * the signal_struct.
 	 */
 	task_cputime(tsk, &utime, &stime);
-	write_seqlock(&sig->stats_lock);
+	write_seqlock(sig->stats_lock);
 	sig->utime += utime;
 	sig->stime += stime;
 	sig->gtime += task_gtime(tsk);
@@ -141,7 +141,7 @@ static void __exit_signal(struct task_struct *tsk)
 	sig->sum_sched_runtime += tsk->se.sum_exec_runtime;
 	sig->nr_threads--;
 	__unhash_process(tsk, group_dead);
-	write_sequnlock(&sig->stats_lock);
+	write_sequnlock(sig->stats_lock);
 
 	/*
 	 * Do this under ->siglock, we can race with another thread
@@ -1088,7 +1088,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		spin_lock_irq(&p->real_parent->sighand->siglock);
 		psig = p->real_parent->signal;
 		sig = p->signal;
-		write_seqlock(&psig->stats_lock);
+		write_seqlock(psig->stats_lock);
 		psig->cutime += tgutime + sig->cutime;
 		psig->cstime += tgstime + sig->cstime;
 		psig->cgtime += task_gtime(p) + sig->gtime + sig->cgtime;
@@ -1111,7 +1111,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 			psig->cmaxrss = maxrss;
 		task_io_accounting_add(&psig->ioac, &p->ioac);
 		task_io_accounting_add(&psig->ioac, &sig->ioac);
-		write_sequnlock(&psig->stats_lock);
+		write_sequnlock(psig->stats_lock);
 		spin_unlock_irq(&p->real_parent->sighand->siglock);
 	}
 
diff --git a/kernel/fork.c b/kernel/fork.c
index 6c380a9f6e68..759d75be506e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -224,6 +224,7 @@ static inline void free_signal_struct(struct signal_struct *sig)
 {
 	taskstats_tgid_free(sig);
 	sched_autogroup_exit(sig);
+	kfree(sig->stats_lock);
 	kmem_cache_free(signal_cachep, sig);
 }
 
@@ -1043,6 +1044,13 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	if (!sig)
 		return -ENOMEM;
 
+	tsk->signal->stats_lock = kmalloc(sizeof(seqlock_t), GFP_KERNEL);
+	if (!tsk->signal->stats_lock) {
+		tsk->signal = NULL;
+		kmem_cache_free(signal_cachep, sig);
+		return -ENOMEM;
+	}
+
 	sig->nr_threads = 1;
 	atomic_set(&sig->live, 1);
 	atomic_set(&sig->sigcnt, 1);
@@ -1055,7 +1063,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->curr_target = tsk;
 	init_sigpending(&sig->shared_pending);
 	INIT_LIST_HEAD(&sig->posix_timers);
-	seqlock_init(&sig->stats_lock);
+	seqlock_init(sig->stats_lock);
 
 	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	sig->real_timer.function = it_real_fn;
diff --git a/kernel/sched/cputime.c b/kernel/sched/cputime.c
index e3202268b86a..edc46072340c 100644
--- a/kernel/sched/cputime.c
+++ b/kernel/sched/cputime.c
@@ -290,7 +290,7 @@ void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times)
 	nextseq = 0;
 	do {
 		seq = nextseq;
-		flags = read_seqbegin_or_lock_irqsave(&sig->stats_lock, &seq);
+		flags = read_seqbegin_or_lock_irqsave(sig->stats_lock, &seq);
 		times->utime = sig->utime;
 		times->stime = sig->stime;
 		times->sum_exec_runtime = sig->sum_sched_runtime;
@@ -303,8 +303,8 @@ void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times)
 		}
 		/* If lockless access failed, take the lock. */
 		nextseq = 1;
-	} while (need_seqretry(&sig->stats_lock, seq));
-	done_seqretry_irqrestore(&sig->stats_lock, seq, flags);
+	} while (need_seqretry(sig->stats_lock, seq));
+	done_seqretry_irqrestore(sig->stats_lock, seq, flags);
 	rcu_read_unlock();
 }
 
