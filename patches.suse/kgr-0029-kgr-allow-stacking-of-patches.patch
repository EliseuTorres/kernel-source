From: Jiri Slaby <jslaby@suse.cz>
Date: Tue, 8 Jul 2014 11:09:24 +0200
Subject: kgr: allow stacking of patches
Patch-mainline: submitted for review
References: fate#313296

We may want to patch some function twice. kGraft did not support this
yet. So take care of this path correctly by remembering all patches in
a list and walking the list when adding another patch. If it patches
the same function, set the 'loc_old' to already patched function, not
the original one.

On the top of that ensure that patches are reverted in the correct
order by incrementing the reference count of previous patches.

js: do not chain ftrace locations, always jump from 'name'

Signed-off-by: Jiri Slaby <jslaby@suse.cz>
---
 include/linux/kgraft.h |  9 ++++++-
 kernel/kgraft.c        | 68 +++++++++++++++++++++++++++++++++++++++++++++++---
 kernel/kgraft_files.c  | 10 ++++++++
 3 files changed, 83 insertions(+), 4 deletions(-)

diff --git a/include/linux/kgraft.h b/include/linux/kgraft.h
index f7cc1f769e53..de21eec63745 100644
--- a/include/linux/kgraft.h
+++ b/include/linux/kgraft.h
@@ -20,6 +20,7 @@
 #include <linux/bitops.h>
 #include <linux/compiler.h>
 #include <linux/kobject.h>
+#include <linux/list.h>
 #include <linux/ftrace.h>
 #include <linux/sched.h>
 
@@ -36,7 +37,8 @@ struct kgr_patch;
  *
  * @name: function to patch
  * @new_fun: function with the new body
- * @loc_old: cache of @name's fentry
+ * @loc_name: cache of @name's fentry
+ * @loc_old: cache of the last entry for @name in the patches list
  * @loc_new: cache of @new_name's fentry
  * @ftrace_ops_slow: ftrace ops for slow (temporary) stub
  * @ftrace_ops_fast: ftrace ops for fast () stub
@@ -59,6 +61,7 @@ struct kgr_patch_fun {
 		KGR_PATCH_SKIPPED,
 	} state;
 
+	unsigned long loc_name;
 	unsigned long loc_old;
 	unsigned long loc_new;
 
@@ -70,8 +73,10 @@ struct kgr_patch_fun {
  * struct kgr_patch -- a kGraft patch
  *
  * @kobj: object representing the sysfs entry
+ * @list: member in patches list
  * @finish: waiting till it is safe to remove the module with the patch
  * @irq_use_new: per-cpu array to remember kGraft state for interrupts
+ * @refs: how many patches need to be reverted before this one
  * @name: name of the patch (to appear in sysfs)
  * @owner: module to refcount on patching
  * @patches: array of @kgr_patch_fun structures
@@ -79,8 +84,10 @@ struct kgr_patch_fun {
 struct kgr_patch {
 	/* internal state information */
 	struct kobject kobj;
+	struct list_head list;
 	struct completion finish;
 	bool __percpu *irq_use_new;
+	unsigned int refs;
 
 	/* a patch shall set these */
 	const char *name;
diff --git a/kernel/kgraft.c b/kernel/kgraft.c
index af9b5329b803..8396179dcf1a 100644
--- a/kernel/kgraft.c
+++ b/kernel/kgraft.c
@@ -18,6 +18,7 @@
 #include <linux/hardirq.h> /* for in_interrupt() */
 #include <linux/kallsyms.h>
 #include <linux/kgraft.h>
+#include <linux/list.h>
 #include <linux/module.h>
 #include <linux/percpu.h>
 #include <linux/sched.h>
@@ -34,6 +35,7 @@ static void kgr_work_fn(struct work_struct *work);
 static struct workqueue_struct *kgr_wq;
 static DECLARE_DELAYED_WORK(kgr_work, kgr_work_fn);
 static DEFINE_MUTEX(kgr_in_progress_lock);
+static LIST_HEAD(patches);
 bool kgr_in_progress;
 static bool kgr_initialized;
 static struct kgr_patch *kgr_patch;
@@ -77,6 +79,22 @@ static void kgr_stub_slow(unsigned long ip, unsigned long parent_ip,
 	}
 }
 
+static void kgr_refs_inc(void)
+{
+	struct kgr_patch *p;
+
+	list_for_each_entry(p, &patches, list)
+		p->refs++;
+}
+
+static void kgr_refs_dec(void)
+{
+	struct kgr_patch *p;
+
+	list_for_each_entry(p, &patches, list)
+		p->refs--;
+}
+
 static bool kgr_still_patching(void)
 {
 	struct task_struct *p;
@@ -112,6 +130,11 @@ static void kgr_finalize(void)
 
 	mutex_lock(&kgr_in_progress_lock);
 	kgr_in_progress = false;
+	if (kgr_revert) {
+		list_del(&kgr_patch->list);
+		kgr_refs_dec();
+	} else
+		list_add_tail(&kgr_patch->list, &patches);
 	mutex_unlock(&kgr_in_progress_lock);
 }
 
@@ -206,6 +229,29 @@ static void kgr_handle_irqs(void)
 	schedule_on_each_cpu(kgr_handle_irq_cpu);
 }
 
+static unsigned long kgr_get_old_fun(const struct kgr_patch_fun *patch_fun)
+{
+	const char *name = patch_fun->name;
+	unsigned long last_new_fun = 0;
+	struct kgr_patch_fun *pf;
+	struct kgr_patch *p;
+
+	list_for_each_entry(p, &patches, list) {
+		kgr_for_each_patch_fun(p, pf) {
+			if (pf->state != KGR_PATCH_APPLIED)
+				continue;
+
+			if (!strcmp(pf->name, name))
+				last_new_fun = (unsigned long)pf->new_fun;
+		}
+	}
+
+	if (last_new_fun)
+		return ftrace_function_to_fentry(last_new_fun);
+
+	return patch_fun->loc_name;
+}
+
 static int kgr_init_ftrace_ops(struct kgr_patch_fun *patch_fun)
 {
 	struct ftrace_ops *fops;
@@ -232,6 +278,14 @@ static int kgr_init_ftrace_ops(struct kgr_patch_fun *patch_fun)
 	if (IS_ERR_VALUE(fentry_loc))
 		return fentry_loc;
 
+	pr_debug("kgr: storing %lx to loc_name for %s\n",
+			fentry_loc, patch_fun->name);
+	patch_fun->loc_name = fentry_loc;
+
+	fentry_loc = kgr_get_old_fun(patch_fun);
+	if (IS_ERR_VALUE(fentry_loc))
+		return fentry_loc;
+
 	pr_debug("kgr: storing %lx to loc_old for %s\n",
 			fentry_loc, patch_fun->name);
 	patch_fun->loc_old = fentry_loc;
@@ -253,13 +307,13 @@ static int kgr_ftrace_enable(struct kgr_patch_fun *pf, struct ftrace_ops *fops)
 {
 	int ret;
 
-	ret = ftrace_set_filter_ip(fops, pf->loc_old, 0, 0);
+	ret = ftrace_set_filter_ip(fops, pf->loc_name, 0, 0);
 	if (ret)
 		return ret;
 
 	ret = register_ftrace_function(fops);
 	if (ret)
-		ftrace_set_filter_ip(fops, pf->loc_old, 1, 0);
+		ftrace_set_filter_ip(fops, pf->loc_name, 1, 0);
 
 	return ret;
 }
@@ -272,7 +326,7 @@ static int kgr_ftrace_disable(struct kgr_patch_fun *pf, struct ftrace_ops *fops)
 	if (ret)
 		return ret;
 
-	ret = ftrace_set_filter_ip(fops, pf->loc_old, 1, 0);
+	ret = ftrace_set_filter_ip(fops, pf->loc_name, 1, 0);
 	if (ret)
 		register_ftrace_function(fops);
 
@@ -370,6 +424,12 @@ int kgr_modify_kernel(struct kgr_patch *patch, bool revert)
 	}
 
 	mutex_lock(&kgr_in_progress_lock);
+	if (patch->refs) {
+		pr_err("kgr: can't patch, this patch is still referenced\n");
+		ret = -EBUSY;
+		goto err_unlock;
+	}
+
 	if (kgr_in_progress) {
 		pr_err("kgr: can't patch, another patching not yet finalized\n");
 		ret = -EAGAIN;
@@ -406,6 +466,8 @@ int kgr_modify_kernel(struct kgr_patch *patch, bool revert)
 	kgr_in_progress = true;
 	kgr_patch = patch;
 	kgr_revert = revert;
+	if (!revert)
+		kgr_refs_inc();
 	mutex_unlock(&kgr_in_progress_lock);
 
 	kgr_handle_irqs();
diff --git a/kernel/kgraft_files.c b/kernel/kgraft_files.c
index 4adbff8b7d42..d2d8b5dfe1bd 100644
--- a/kernel/kgraft_files.c
+++ b/kernel/kgraft_files.c
@@ -56,6 +56,14 @@ static ssize_t state_show(struct kobject *kobj, struct kobj_attribute *attr,
 	return size;
 }
 
+static ssize_t refs_show(struct kobject *kobj, struct kobj_attribute *attr,
+		char *buf)
+{
+	struct kgr_patch *p = kobj_to_patch(kobj);
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", p->refs);
+}
+
 static ssize_t revert_store(struct kobject *kobj,
 		struct kobj_attribute *attr, const char *buf, size_t count)
 {
@@ -68,10 +76,12 @@ static ssize_t revert_store(struct kobject *kobj,
 }
 
 static struct kobj_attribute kgr_attr_state = __ATTR_RO(state);
+static struct kobj_attribute kgr_attr_refs = __ATTR_RO(refs);
 static struct kobj_attribute kgr_attr_revert = __ATTR_WO(revert);
 
 static struct attribute *kgr_patch_sysfs_entries[] = {
 	&kgr_attr_state.attr,
+	&kgr_attr_refs.attr,
 	&kgr_attr_revert.attr,
 	NULL
 };
-- 
2.0.1

