From: Jiri Slaby <jslaby@suse.cz>
Date: Tue, 8 Jul 2014 11:09:24 +0200
Subject: kgr: allow stacking of patches
Patch-mainline: submitted for review
References: fate#313296

We may want to patch some function twice. kGraft did not support this
yet. So take care of this path correctly by remembering all patches in
a list and walking the list when adding another patch. If it patches
the same function, set the 'loc_old' to already patched function, not
the original one.

On the top of that ensure that patches are reverted in the correct
order by incrementing the reference count of previous patches.

Signed-off-by: Jiri Slaby <jslaby@suse.cz>
---
 include/linux/kgraft.h |  5 +++++
 kernel/kgraft.c        | 56 +++++++++++++++++++++++++++++++++++++++++++++++++-
 kernel/kgraft_files.c  | 10 +++++++++
 3 files changed, 70 insertions(+), 1 deletion(-)

diff --git a/include/linux/kgraft.h b/include/linux/kgraft.h
index d9747463c733..7caadbba710e 100644
--- a/include/linux/kgraft.h
+++ b/include/linux/kgraft.h
@@ -20,6 +20,7 @@
 #include <linux/bitops.h>
 #include <linux/compiler.h>
 #include <linux/kobject.h>
+#include <linux/list.h>
 #include <linux/ftrace.h>
 #include <linux/sched.h>
 
@@ -70,7 +71,9 @@ struct kgr_patch_fun {
  * struct kgr_patch -- a kGraft patch
  *
  * @kobj: object representing the sysfs entry
+ * @list: member in patches list
  * @finish: waiting till it is safe to remove the module with the patch
+ * @refs: how many patches need to be reverted before this one
  * @name: name of the patch (to appear in sysfs)
  * @owner: module to refcount on patching
  * @irq_use_new: per-cpu array to remember kGraft state for interrupts
@@ -78,7 +81,9 @@ struct kgr_patch_fun {
  */
 struct kgr_patch {
 	struct kobject kobj;
+	struct list_head list;
 	struct completion finish;
+	unsigned int refs;
 	const char *name;
 	struct module *owner;
 	bool __percpu *irq_use_new;
diff --git a/kernel/kgraft.c b/kernel/kgraft.c
index f7791202d67b..05a7a7c91d12 100644
--- a/kernel/kgraft.c
+++ b/kernel/kgraft.c
@@ -18,6 +18,7 @@
 #include <linux/hardirq.h> /* for in_interrupt() */
 #include <linux/kallsyms.h>
 #include <linux/kgraft.h>
+#include <linux/list.h>
 #include <linux/module.h>
 #include <linux/percpu.h>
 #include <linux/sched.h>
@@ -34,6 +35,7 @@ static void kgr_work_fn(struct work_struct *work);
 static struct workqueue_struct *kgr_wq;
 static DECLARE_DELAYED_WORK(kgr_work, kgr_work_fn);
 static DEFINE_MUTEX(kgr_in_progress_lock);
+static LIST_HEAD(patches);
 bool kgr_in_progress;
 static bool kgr_initialized;
 static struct kgr_patch *kgr_patch;
@@ -80,6 +82,22 @@ static void kgr_stub_slow(unsigned long ip, unsigned long parent_ip,
 	}
 }
 
+static void kgr_refs_inc(void)
+{
+	struct kgr_patch *p;
+
+	list_for_each_entry(p, &patches, list)
+		p->refs++;
+}
+
+static void kgr_refs_dec(void)
+{
+	struct kgr_patch *p;
+
+	list_for_each_entry(p, &patches, list)
+		p->refs--;
+}
+
 static bool kgr_still_patching(void)
 {
 	struct task_struct *p;
@@ -115,6 +133,11 @@ static void kgr_finalize(void)
 
 	mutex_lock(&kgr_in_progress_lock);
 	kgr_in_progress = false;
+	if (kgr_revert) {
+		list_del(&kgr_patch->list);
+		kgr_refs_dec();
+	} else
+		list_add_tail(&kgr_patch->list, &patches);
 	mutex_unlock(&kgr_in_progress_lock);
 }
 
@@ -209,6 +232,29 @@ static void kgr_handle_irqs(void)
 	schedule_on_each_cpu(kgr_handle_irq_cpu);
 }
 
+static unsigned long kgr_get_old_fun(const struct kgr_patch_fun *patch_fun)
+{
+	const char *name = patch_fun->name;
+	unsigned long last_new_fun = 0;
+	struct kgr_patch_fun *pf;
+	struct kgr_patch *p;
+
+	list_for_each_entry(p, &patches, list) {
+		kgr_for_each_patch(p, pf) {
+			if (pf->state != KGR_PATCH_APPLIED)
+				continue;
+
+			if (!strcmp(pf->name, name))
+				last_new_fun = (unsigned long)pf->new_fun;
+		}
+	}
+
+	if (last_new_fun)
+		return ftrace_function_to_fentry(last_new_fun);
+
+	return kgr_get_fentry_loc(name);
+}
+
 static int kgr_init_ftrace_ops(struct kgr_patch_fun *patch_fun)
 {
 	struct ftrace_ops *fops;
@@ -231,7 +277,7 @@ static int kgr_init_ftrace_ops(struct kgr_patch_fun *patch_fun)
 			fentry_loc, patch_fun->new_fun);
 	patch_fun->loc_new = fentry_loc;
 
-	fentry_loc = kgr_get_fentry_loc(patch_fun->name);
+	fentry_loc = kgr_get_old_fun(patch_fun);
 	if (IS_ERR_VALUE(fentry_loc))
 		return fentry_loc;
 
@@ -356,6 +402,12 @@ int kgr_modify_kernel(struct kgr_patch *patch, bool revert)
 	}
 
 	mutex_lock(&kgr_in_progress_lock);
+	if (patch->refs) {
+		pr_err("kgr: can't patch, this patch is still referenced\n");
+		ret = -EBUSY;
+		goto err_unlock;
+	}
+
 	if (kgr_in_progress) {
 		pr_err("kgr: can't patch, another patching not yet finalized\n");
 		ret = -EAGAIN;
@@ -385,6 +437,8 @@ int kgr_modify_kernel(struct kgr_patch *patch, bool revert)
 	kgr_in_progress = true;
 	kgr_patch = patch;
 	kgr_revert = revert;
+	if (!revert)
+		kgr_refs_inc();
 	mutex_unlock(&kgr_in_progress_lock);
 
 	kgr_handle_irqs();
diff --git a/kernel/kgraft_files.c b/kernel/kgraft_files.c
index e155a9c487e1..a6f0c726beb7 100644
--- a/kernel/kgraft_files.c
+++ b/kernel/kgraft_files.c
@@ -56,6 +56,14 @@ static ssize_t state_show(struct kobject *kobj, struct kobj_attribute *attr,
 	return size;
 }
 
+static ssize_t refs_show(struct kobject *kobj, struct kobj_attribute *attr,
+		char *buf)
+{
+	struct kgr_patch *p = kobj_to_patch(kobj);
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", p->refs);
+}
+
 static ssize_t revert_store(struct kobject *kobj,
 		struct kobj_attribute *attr, const char *buf, size_t count)
 {
@@ -68,10 +76,12 @@ static ssize_t revert_store(struct kobject *kobj,
 }
 
 static struct kobj_attribute kgr_attr_state = __ATTR_RO(state);
+static struct kobj_attribute kgr_attr_refs = __ATTR_RO(refs);
 static struct kobj_attribute kgr_attr_revert = __ATTR_WO(revert);
 
 static struct attribute *kgr_patch_sysfs_entries[] = {
 	&kgr_attr_state.attr,
+	&kgr_attr_refs.attr,
 	&kgr_attr_revert.attr,
 	NULL
 };
-- 
2.0.0

