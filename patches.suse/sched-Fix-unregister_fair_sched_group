Subject: sched: Fix unregister_fair_sched_group()
From: Paul Turner <pjt@google.com>
Date: Mon Nov 29 16:55:40 2010 -0800
Git-commit: 822bc180a7f7a7bc5fcaaea195f41b487cc8cae8
References:

In the flipping and flopping between calling
unregister_fair_sched_group() on a per-cpu versus per-group basis
we ended up in a bad state.

Remove from the list for the passed cpu as opposed to some
arbitrary index.

( This fixes explosions w/ autogroup as well as a group
  creation/destruction stress test. )

Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
Signed-off-by: Paul Turner <pjt@google.com>
Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
Cc: Mike Galbraith <efault@gmx.de>
LKML-Reference: <20101130005740.080828123@google.com>
Signed-off-by: Ingo Molnar <mingo@elte.hu>
Acked-by: Mike Galbraith <mgalbraith@suse.de>

---
 kernel/sched.c |    3 +--
 1 file changed, 1 insertion(+), 2 deletions(-)

Index: linux-2.6.32/kernel/sched.c
===================================================================
--- linux-2.6.32.orig/kernel/sched.c
+++ linux-2.6.32/kernel/sched.c
@@ -8268,7 +8268,6 @@ static inline void unregister_fair_sched
 {
 	struct rq *rq = cpu_rq(cpu);
 	unsigned long flags;
-	int i;
 
 	/*
 	* Only empty task groups can be destroyed; so we can speculatively
@@ -8278,7 +8277,7 @@ static inline void unregister_fair_sched
 		return;
 
 	spin_lock_irqsave(&rq->lock, flags);
-	list_del_leaf_cfs_rq(tg->cfs_rq[i]);
+	list_del_leaf_cfs_rq(tg->cfs_rq[cpu]);
 	spin_unlock_irqrestore(&rq->lock, flags);
 }
 #else /* !CONFG_FAIR_GROUP_SCHED */
