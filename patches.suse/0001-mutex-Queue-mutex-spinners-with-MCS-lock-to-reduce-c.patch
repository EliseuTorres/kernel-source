From a6e3b4fcb50d0c4d4d5109d52abdfa71b1a73008 Mon Sep 17 00:00:00 2001
From: Waiman Long <Waiman.Long@hp.com>
Date: Mon, 12 May 2014 16:58:16 -0400
Subject: [PATCH 1/5] mutex: Queue mutex spinners with MCS lock to reduce cacheline contention
Git-commit: 2bd2c92cf07cc4a373bf316c75b78ac465fefd35
Patch-mainline: v3.10-rc1
References: FATE#317271

The current mutex spinning code (with MUTEX_SPIN_ON_OWNER option turned
on) allows multiple tasks to spin on a single mutex concurrently. A
potential problem with the current approach is that when the mutex
becomes available, all the spinning tasks will try to acquire the
mutex more or less simultaneously. As a result, there will be a lot of
cacheline bouncing especially on systems with a large number of CPUs.

This patch tries to reduce this kind of contention by putting the
mutex spinners into a queue so that only the first one in the queue
will try to acquire the mutex. This will reduce contention and allow
all the tasks to move forward faster.

The queuing of mutex spinners is done using an MCS lock based
implementation which will further reduce contention on the mutex
cacheline than a similar ticket spinlock based implementation.

This patch is based on the following 3.10 kernel submit:

  2bd2c92cf07cc4a373bf316c75b78ac465fefd35
  mutex: Queue mutex spinners with MCS lock to reduce cacheline
  contention

Instead of adding a new member to the mutex structure, which will break
binary compatibility, the 2-pointer wait_list structure are replaced
by a single list_head pointer and a pointer to the internal MCS lock
structure with some additional changes because of that. As a result,
the size of the mutex structure won't be changed. So as long as no
one is trying to access the wait_list structure directly outside of
the mutex code, this change should not affect binary compatibility.

Signed-off-by: Waiman Long <Waiman.Long@hp.com>
Acked-by: Jeff Mahoney <jeffm@suse.com>
---

 include/linux/mutex.h |   17 ++++++
 kernel/mutex-debug.c  |    6 ++
 kernel/mutex-debug.h  |    3 +
 kernel/mutex.c        |  122 +++++++++++++++++++++++++++++++++++++++++++++++---
 kernel/mutex.h        |   65 ++++++++++++++++++++++++++
 5 files changed, 204 insertions(+), 9 deletions(-)

--- a/include/linux/mutex.h
+++ b/include/linux/mutex.h
@@ -49,7 +49,12 @@ struct mutex {
 	/* 1: unlocked, 0: locked, negative: locked, possible waiters */
 	atomic_t		count;
 	spinlock_t		wait_lock;
+#if defined(CONFIG_MUTEX_SPIN_USE_MCS_QUEUE)
+	struct list_head	*wait_list;
+	void *			*spin_mlock;
+#else
 	struct list_head	wait_list;
+#endif
 #if defined(CONFIG_DEBUG_MUTEXES) || defined(CONFIG_SMP)
 	struct task_struct	*owner;
 #endif
@@ -102,10 +107,20 @@ do {							\
 # define __DEP_MAP_MUTEX_INITIALIZER(lockname)
 #endif
 
+#if defined(CONFIG_MUTEX_SPIN_USE_MCS_QUEUE)
+# define MUTEX_WAIT_LIST_INIT(x)	(struct list_head *)&(x)
+# define MUTEX_INIT_WAIT_LIST(x)	*(x) = (struct list_head *)(x)
+# define MUTEX_LIST_EMPTY(x)		(*(x) == (struct list_head *)(x))
+#else
+# define MUTEX_WAIT_LIST_INIT(x)	LIST_HEAD_INIT(x)
+# define MUTEX_INIT_WAIT_LIST(x)	INIT_LIST_HEAD(x)
+# define MUTEX_LIST_EMPTY(x)		list_empty(x)
+#endif
+
 #define __MUTEX_INITIALIZER(lockname) \
 		{ .count = ATOMIC_INIT(1) \
 		, .wait_lock = __SPIN_LOCK_UNLOCKED(lockname.wait_lock) \
-		, .wait_list = LIST_HEAD_INIT(lockname.wait_list) \
+		, .wait_list = MUTEX_WAIT_LIST_INIT(lockname.wait_list) \
 		__DEBUG_MUTEX_INITIALIZER(lockname) \
 		__DEP_MAP_MUTEX_INITIALIZER(lockname) }
 
--- a/kernel/mutex-debug.c
+++ b/kernel/mutex-debug.c
@@ -37,7 +37,7 @@ void debug_mutex_lock_common(struct mute
 void debug_mutex_wake_waiter(struct mutex *lock, struct mutex_waiter *waiter)
 {
 	SMP_DEBUG_LOCKS_WARN_ON(!spin_is_locked(&lock->wait_lock));
-	DEBUG_LOCKS_WARN_ON(list_empty(&lock->wait_list));
+	DEBUG_LOCKS_WARN_ON(MUTEX_LIST_EMPTY(&lock->wait_list));
 	DEBUG_LOCKS_WARN_ON(waiter->magic != waiter);
 	DEBUG_LOCKS_WARN_ON(list_empty(&waiter->list));
 }
@@ -76,7 +76,11 @@ void debug_mutex_unlock(struct mutex *lo
 
 	DEBUG_LOCKS_WARN_ON(lock->magic != lock);
 	DEBUG_LOCKS_WARN_ON(lock->owner != current);
+#if defined(CONFIG_MUTEX_SPIN_USE_MCS_QUEUE)
+	DEBUG_LOCKS_WARN_ON(!lock->wait_list)
+#else
 	DEBUG_LOCKS_WARN_ON(!lock->wait_list.prev && !lock->wait_list.next);
+#endif
 	mutex_clear_owner(lock);
 }
 
--- a/kernel/mutex.c
+++ b/kernel/mutex.c
@@ -47,8 +47,11 @@ __mutex_init(struct mutex *lock, const c
 {
 	atomic_set(&lock->count, 1);
 	spin_lock_init(&lock->wait_lock);
-	INIT_LIST_HEAD(&lock->wait_list);
+	MUTEX_INIT_WAIT_LIST(&lock->wait_list);
 	mutex_clear_owner(lock);
+#ifdef CONFIG_MUTEX_SPIN_USE_MCS_QUEUE
+	lock->spin_mlock = NULL;
+#endif
 
 	debug_mutex_init(lock, name, key);
 }
@@ -100,6 +103,94 @@ void __sched mutex_lock(struct mutex *lo
 EXPORT_SYMBOL(mutex_lock);
 #endif
 
+#ifdef CONFIG_MUTEX_SPIN_USE_MCS_QUEUE
+/*
+ * In order to avoid a stampede of mutex spinners from acquiring the mutex
+ * more or less simultaneously, the spinners need to acquire a MCS lock
+ * first before spinning on the owner field.
+ *
+ * We don't inline mspin_lock() so that perf can correctly account for the
+ * time spent in this lock function.
+ */
+struct mspin_node {
+	struct mspin_node *next ;
+	int		  locked;	/* 1 if lock acquired */
+};
+#define	MLOCK(mutex)	((struct mspin_node **)&((mutex)->spin_mlock))
+
+static noinline
+void mspin_lock(struct mutex *mutex, struct mspin_node *node)
+{
+	struct mspin_node *prev;
+
+	/* Init node */
+	node->locked = 0;
+	node->next   = NULL;
+
+	prev = xchg(MLOCK(mutex), node);
+	/*
+	 * An external kernel module may have a statically allocated mutex
+	 * initialized with the old initializer. As a result, a pointer to
+	 * wait_list should be treated as NULL.
+	 */
+	if (unlikely(prev == (struct mspin_node *)&mutex->wait_list))
+		prev = NULL;
+	if (likely(prev == NULL)) {
+		/* Lock acquired */
+		node->locked = 1;
+		return;
+	}
+	ACCESS_ONCE(prev->next) = node;
+	smp_wmb();
+	/* Wait until the lock holder passes the lock down */
+	while (!ACCESS_ONCE(node->locked))
+		arch_mutex_cpu_relax();
+}
+
+static void mspin_unlock(struct mutex *mutex, struct mspin_node *node)
+{
+	struct mspin_node *next = ACCESS_ONCE(node->next);
+
+	if (likely(!next)) {
+		/*
+		 * Release the lock by setting it to NULL
+		 */
+		if (cmpxchg(MLOCK(mutex), node, NULL) == node)
+			return;
+		/* Wait until the next pointer is set */
+		while (!(next = ACCESS_ONCE(node->next)))
+			arch_mutex_cpu_relax();
+	}
+	ACCESS_ONCE(next->locked) = 1;
+	smp_wmb();
+}
+
+/**
+ * mutex_can_spin_on_owner - initial check for entering the mutex spinning loop
+ * @lock: the mutex to be checked
+ */
+static inline int mutex_can_spin_on_owner(struct mutex *lock)
+{
+	int retval = 1;
+
+	rcu_read_lock();
+	if (lock->owner)
+		retval = lock->owner->on_cpu;
+	rcu_read_unlock();
+	/*
+	 * if lock->owner is not set, the mutex owner may have just acquired
+	 * it and not set the owner yet or the mutex has been released.
+	 */
+	return retval;
+}
+#else /* CONFIG_MUTEX_SPIN_USE_MCS_QUEUE */
+
+#define mspin_lock(mutex, node) do { } while(0)
+#define mspin_unlock(mutex, node) do { } while(0)
+#define mutex_can_spin_on_owner(a) (1)
+
+#endif /* CONFIG_MUTEX_SPIN_USE_MCS_QUEUE */
+
 static __used noinline void __sched __mutex_unlock_slowpath(atomic_t *lock_count);
 
 /**
@@ -163,26 +254,42 @@ __mutex_lock_common(struct mutex *lock,
 	 *
 	 * We can't do this for DEBUG_MUTEXES because that relies on wait_lock
 	 * to serialize everything.
+	 *
+	 * The mutex spinners are queued up using MCS lock so that only one
+	 * spinner can compete for the mutex. However, if mutex spinning isn't
+	 * going to happen, there is no point in going through the lock/unlock
+	 * overhead.
 	 */
 
+	if (!mutex_can_spin_on_owner(lock))
+		goto slowpath;
+
 	for (;;) {
 		struct task_struct *owner;
+#ifdef CONFIG_MUTEX_SPIN_USE_MCS_QUEUE
+		struct mspin_node  node;
+#endif
 
 		/*
 		 * If there's an owner, wait for it to either
 		 * release the lock or go to sleep.
 		 */
+		mspin_lock(lock, &node);
 		owner = ACCESS_ONCE(lock->owner);
-		if (owner && !mutex_spin_on_owner(lock, owner))
+		if (owner && !mutex_spin_on_owner(lock, owner)) {
+			mspin_unlock(lock, &node);
 			break;
+		}
 
 		if ((atomic_read(&lock->count) == 1) &&
 		    (atomic_cmpxchg(&lock->count, 1, 0) == 1)) {
 			lock_acquired(&lock->dep_map, ip);
 			mutex_set_owner(lock);
+			mspin_unlock(lock, &node);
 			preempt_enable();
 			return 0;
 		}
+		mspin_unlock(lock, &node);
 
 		/*
 		 * When there's no owner, we might have preempted between the
@@ -201,6 +308,7 @@ __mutex_lock_common(struct mutex *lock,
 		 */
 		arch_mutex_cpu_relax();
 	}
+slowpath:
 #endif
 	spin_lock_mutex(&lock->wait_lock, flags);
 
@@ -208,7 +316,7 @@ __mutex_lock_common(struct mutex *lock,
 	debug_mutex_add_waiter(lock, &waiter, task_thread_info(task));
 
 	/* add waiting tasks to the end of the waitqueue (FIFO): */
-	list_add_tail(&waiter.list, &lock->wait_list);
+	mutex_add_waiter(&waiter.list, &lock->wait_list);
 	waiter.task = task;
 
 	if (MUTEX_SHOW_NO_WAITER(lock) && (atomic_xchg(&lock->count, -1) == 1))
@@ -261,7 +369,7 @@ done:
 	mutex_set_owner(lock);
 
 	/* set it to 0 if there are no waiters left: */
-	if (likely(list_empty(&lock->wait_list)))
+	if (likely(MUTEX_LIST_EMPTY(&lock->wait_list)))
 		atomic_set(&lock->count, 0);
 
 	spin_unlock_mutex(&lock->wait_lock, flags);
@@ -331,10 +439,10 @@ __mutex_unlock_common_slowpath(atomic_t
 	if (__mutex_slowpath_needs_to_unlock())
 		atomic_set(&lock->count, 1);
 
-	if (!list_empty(&lock->wait_list)) {
+	if (!MUTEX_LIST_EMPTY(&lock->wait_list)) {
 		/* get the first entry from the wait-list: */
 		struct mutex_waiter *waiter =
-				list_entry(lock->wait_list.next,
+				list_entry(mutex_waitlist_head(lock),
 					   struct mutex_waiter, list);
 
 		debug_mutex_wake_waiter(lock, waiter);
@@ -449,7 +557,7 @@ static inline int __mutex_trylock_slowpa
 	}
 
 	/* Set it back to 0 if there are no waiters: */
-	if (likely(list_empty(&lock->wait_list)))
+	if (likely(MUTEX_LIST_EMPTY(&lock->wait_list)))
 		atomic_set(&lock->count, 0);
 
 	spin_unlock_mutex(&lock->wait_lock, flags);
--- a/kernel/mutex.h
+++ b/kernel/mutex.h
@@ -13,8 +13,73 @@
 		do { spin_lock(lock); (void)(flags); } while (0)
 #define spin_unlock_mutex(lock, flags) \
 		do { spin_unlock(lock); (void)(flags); } while (0)
+
+#ifdef CONFIG_MUTEX_SPIN_USE_MCS_QUEUE
+/*
+ * If CONFIG_MUTEX_SPIN_ON_OWNER and CONFIG_USE_MCS_SPIN_WAIT_QUEUE are
+ * enabled, the content of the double pointers in wait_list will be
+ * recasted as:
+ * wait_list.next - A pointer to the list of waiting tasks
+ * wait_list.prev - A pointer to the tail of the MCS node linked list
+ *		    (mspin_lock)
+ *
+ * In other words, the dual-pointer list_head structure in mutex is reduced
+ * to a single pointer to make room for a pointer to the mspin_node structure.
+ * The single pointer (.next) now points to a circular doubly-linked list.
+ * The list_empty() macro will still works with the modified interpretation.
+ */
+
+/**
+ * mutex_add_waiter - add a waiter to the end of the list
+ * @waiter: 	the waiter to be added
+ * @list:	the list head pointer
+ */
+static inline void
+mutex_add_waiter(struct list_head *waiter, struct list_head **list)
+{
+	if (MUTEX_LIST_EMPTY(list)) {
+		*list        = waiter;
+		waiter->next = waiter;
+		waiter->prev = waiter;
+	} else {
+		list_add_tail(waiter, *list);
+	}
+}
+#define mutex_waitlist_head(lock)	(lock)->wait_list
+#define mutex_remove_waiter(lock, waiter, ti) \
+		__mutex_remove_waiter(lock, (struct list_head *)waiter)
+
+/**
+ * __mutex_remove_waiter - remove a waiter from the list
+ * @mutex: 	the mutex data structure pointer
+ * @waiter:	the waiter list_head structure pointer
+ */
+static inline void
+__mutex_remove_waiter(struct mutex *lock, struct list_head *waiter)
+{
+	if (waiter->next == waiter)
+		/*
+		 * Last element in the list, mark the wait list empty
+		 */
+		lock->wait_list = (struct list_head *)&lock->wait_list;
+	else {
+		/*
+		 * If the element pointed to by wait_list.next is to be removed,
+		 * wait_list will have to point to the next one in queue.
+		 */
+		if (lock->wait_list == waiter)
+			lock->wait_list = waiter->next;
+		__list_del(waiter->prev, waiter->next);
+	}
+}
+
+#else /* MUTEX_SPIN_USE_MCS_QUEUE */
+
+#define mutex_add_waiter(waiter, list)	list_add_tail(waiter, list)
+#define mutex_waitlist_head(lock)	(lock)->wait_list.next
 #define mutex_remove_waiter(lock, waiter, ti) \
 		__list_del((waiter)->list.prev, (waiter)->list.next)
+#endif /* MUTEX_SPIN_USE_MCS_QUEUE */
 
 #ifdef CONFIG_SMP
 static inline void mutex_set_owner(struct mutex *lock)
--- a/kernel/mutex-debug.h
+++ b/kernel/mutex-debug.h
@@ -53,3 +53,6 @@ static inline void mutex_clear_owner(str
 		local_irq_restore(flags);			\
 		preempt_check_resched();			\
 	} while (0)
+
+#define mutex_add_waiter(waiter, list)	list_add_tail(waiter, list)
+#define mutex_waitlist_head(lock)	(lock)->wait_list.next
