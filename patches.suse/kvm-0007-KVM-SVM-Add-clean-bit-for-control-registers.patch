From: Andre Przywara <andre.przywara@amd.com>
Subject: [PATCH 07/12] KVM: SVM: Add clean-bit for control registers
References: FATE#309760
Git-commit: dcca1a6506123cd47af334b7ee2a4b0288196389
Patch-mainline: v2.6.38

This patch implements the CRx clean-bit for the vmcb. This
bit covers cr0, cr3, cr4, and efer.

Backport of dcca1a6506123cd47af334b7ee2a4b0288196389
(done by: Joerg Roedel <joerg.roedel@amd.com>)

Signed-off-by: Andre Przywara <andre.przywara@amd.com>
Acked-by: Bruce Rogers <brogers@suse.com>
---
 arch/x86/kvm/svm.c |   12 +++++++++++-
 1 file changed, 11 insertions(+), 1 deletion(-)

Index: b/arch/x86/kvm/svm.c
===================================================================
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -148,6 +148,7 @@ enum {
 	VMCB_ASID,       /* ASID */
 	VMCB_INTR,       /* int_ctl, int_vector */
 	VMCB_NPT,        /* npt_en, nCR3, gPAT */
+	VMCB_CR,         /* CR0, CR3, CR4, EFER */
 	VMCB_DIRTY_MAX,
 };
 
@@ -269,6 +270,7 @@ static void svm_set_efer(struct kvm_vcpu
 		efer &= ~EFER_LME;
 
 	to_svm(vcpu)->vmcb->save.efer = efer | EFER_SVME;
+	mark_dirty(to_svm(vcpu)->vmcb, VMCB_CR);
 	vcpu->arch.shadow_efer = efer;
 }
 
@@ -1054,6 +1056,7 @@ static void update_cr0_intercept(struct
 		*hcr0 = (*hcr0 & ~SVM_CR0_SELECTIVE_MASK)
 			| (gcr0 & SVM_CR0_SELECTIVE_MASK);
 
+	mark_dirty(svm->vmcb, VMCB_CR);
 
 	if (gcr0 == *hcr0 && svm->vcpu.fpu_active) {
 		svm->vmcb->control.intercept_cr_read &= ~INTERCEPT_CR0_MASK;
@@ -1112,6 +1115,7 @@ static void svm_set_cr4(struct kvm_vcpu
 		cr4 |= X86_CR4_PAE;
 	cr4 |= host_cr4_mce;
 	to_svm(vcpu)->vmcb->save.cr4 = cr4;
+	mark_dirty(to_svm(vcpu)->vmcb, VMCB_CR);
 }
 
 static void svm_set_segment(struct kvm_vcpu *vcpu,
@@ -1164,6 +1168,7 @@ static void update_db_intercept(struct k
 		vcpu->guest_debug = 0;
 
 	mark_dirty(svm->vmcb, VMCB_INTERCEPTS);
+	mark_dirty(svm->vmcb, VMCB_CR);
 }
 
 static int svm_guest_debug(struct kvm_vcpu *vcpu, struct kvm_guest_debug *dbg)
@@ -1821,6 +1826,7 @@ static int nested_svm_vmexit(struct vcpu
 	} else {
 		kvm_set_cr3(&svm->vcpu, hsave->save.cr3);
 	}
+	mark_dirty(svm->vmcb, VMCB_CR);
 	kvm_register_write(&svm->vcpu, VCPU_REGS_RAX, hsave->save.rax);
 	kvm_register_write(&svm->vcpu, VCPU_REGS_RSP, hsave->save.rsp);
 	kvm_register_write(&svm->vcpu, VCPU_REGS_RIP, hsave->save.rip);
@@ -1922,6 +1928,7 @@ static bool nested_svm_vmrun(struct vcpu
 		kvm_set_cr3(&svm->vcpu, nested_vmcb->save.cr3);
 		kvm_mmu_reset_context(&svm->vcpu);
 	}
+	mark_dirty(svm->vmcb, VMCB_CR);
 	svm->vmcb->save.cr2 = svm->vcpu.arch.cr2 = nested_vmcb->save.cr2;
 	kvm_register_write(&svm->vcpu, VCPU_REGS_RAX, nested_vmcb->save.rax);
 	kvm_register_write(&svm->vcpu, VCPU_REGS_RSP, nested_vmcb->save.rsp);
@@ -2934,8 +2941,10 @@ static void svm_vcpu_run(struct kvm_vcpu
 	ldt_selector = kvm_read_ldt();
 	svm->vmcb->save.cr2 = vcpu->arch.cr2;
 	/* required for live migration with NPT */
-	if (npt_enabled)
+	if (npt_enabled) {
 		svm->vmcb->save.cr3 = vcpu->arch.cr3;
+		mark_dirty(to_svm(vcpu)->vmcb, VMCB_CR);
+	}
 
 	clgi();
 
@@ -3067,6 +3076,7 @@ static void svm_set_cr3(struct kvm_vcpu
 	}
 
 	svm->vmcb->save.cr3 = root;
+	mark_dirty(svm->vmcb, VMCB_CR);
 	force_new_asid(vcpu);
 }
 
