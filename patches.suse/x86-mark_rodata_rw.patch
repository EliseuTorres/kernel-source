From: Nick Piggin <npiggin@novell.com>
Subject: Add mark_rodata_rw() to un-protect read-only kernel code pages
Patch-mainline: never
References: bnc#439348

CONFIG_RODATA presents a problem for antivirus vendors who do not have a
clean user-space interface for getting virus scanning triggered, and
currently resort to patching the kernel code instead (presumably the
ystem call table). With CONFIG_RODATA enabled, the kernel rejects such
write accesses.

Add a new mark_rodata_rw() function to un-protect the read-only kernel code
pages for now, and export mark_rodata_ro() and mark_rodata_rw() to modules.

This is not meant as a permanent workaround, and will be removed again in the
next release!

Acked-by: Andres Gruenbacher <agruen@suse.de>

---
 arch/x86/include/asm/cacheflush.h |    3 +++
 arch/x86/mm/init_32.c             |   14 ++++++++++++++
 arch/x86/mm/init_64.c             |   31 +++++++++++++++++++++++++------
 arch/x86/mm/pageattr.c            |   30 ++++++++++++++++++++++++++++--
 4 files changed, 70 insertions(+), 8 deletions(-)

Index: linux-2.6.27/arch/x86/mm/init_32.c
===================================================================
--- linux-2.6.27.orig/arch/x86/mm/init_32.c
+++ linux-2.6.27/arch/x86/mm/init_32.c
@@ -1062,6 +1062,20 @@ void mark_rodata_ro(void)
 	set_pages_ro(virt_to_page(start), size >> PAGE_SHIFT);
 #endif
 }
+EXPORT_SYMBOL_GPL(mark_rodata_ro);
+
+void mark_rodata_rw(void)
+{
+	unsigned long start = PFN_ALIGN(_text);
+	unsigned long size = PFN_ALIGN(_etext) - start;
+
+	start += size;
+	size = (unsigned long)__end_rodata - start;
+	set_pages_rw_force(virt_to_page(start), size >> PAGE_SHIFT);
+	printk(KERN_INFO "Write enabling the kernel read-only data: %luk\n",
+		size >> 10);
+}
+EXPORT_SYMBOL_GPL(mark_rodata_rw);
 #endif
 
 int __init reserve_bootmem_generic(unsigned long phys, unsigned long len,
Index: linux-2.6.27/arch/x86/mm/init_64.c
===================================================================
--- linux-2.6.27.orig/arch/x86/mm/init_64.c
+++ linux-2.6.27/arch/x86/mm/init_64.c
@@ -734,37 +734,58 @@ void mark_rodata_ro(void)
 	unsigned long rodata_end = PAGE_ALIGN((unsigned long) &__end_rodata);
 	unsigned long data_start = (unsigned long) &_sdata;
 
-	printk(KERN_INFO "Write protecting the kernel read-only data: %luk\n",
-	       (end - start) >> 10);
-	set_memory_ro(start, (end - start) >> PAGE_SHIFT);
-
-	kernel_set_to_readonly = 1;
-
-	/*
-	 * The rodata section (but not the kernel text!) should also be
-	 * not-executable.
-	 */
-	set_memory_nx(rodata_start, (end - rodata_start) >> PAGE_SHIFT);
+	if (!kernel_set_to_readonly) {
+		printk(KERN_INFO "Write protecting the kernel read-only data: %luk\n",
+		       (end - start) >> 10);
+		set_memory_ro(start, (end - start) >> PAGE_SHIFT);
+
+		kernel_set_to_readonly = 1;
+
+		/*
+		 * The rodata section (but not the kernel text!) should also be
+		 * not-executable.
+		 */
+		set_memory_nx(rodata_start, (end - rodata_start) >> PAGE_SHIFT);
 
-	rodata_test();
+		rodata_test();
 
 #ifdef CONFIG_CPA_DEBUG
-	printk(KERN_INFO "Testing CPA: undo %lx-%lx\n", start, end);
-	set_memory_rw(start, (end-start) >> PAGE_SHIFT);
+		printk(KERN_INFO "Testing CPA: undo %lx-%lx\n", start, end);
+		set_memory_rw(start, (end-start) >> PAGE_SHIFT);
 
-	printk(KERN_INFO "Testing CPA: again\n");
-	set_memory_ro(start, (end-start) >> PAGE_SHIFT);
+		printk(KERN_INFO "Testing CPA: again\n");
+		set_memory_ro(start, (end-start) >> PAGE_SHIFT);
 #endif
 
-	free_init_pages("unused kernel memory",
-			(unsigned long) page_address(virt_to_page(text_end)),
-			(unsigned long)
+		free_init_pages("unused kernel memory",
+				(unsigned long)
+				 page_address(virt_to_page(text_end)),
+				(unsigned long)
 				 page_address(virt_to_page(rodata_start)));
-	free_init_pages("unused kernel memory",
-			(unsigned long) page_address(virt_to_page(rodata_end)),
-			(unsigned long) page_address(virt_to_page(data_start)));
+		free_init_pages("unused kernel memory",
+				(unsigned long)
+				 page_address(virt_to_page(rodata_end)),
+				(unsigned long)
+				 page_address(virt_to_page(data_start)));
+	} else {
+		printk(KERN_INFO "Write protecting the kernel read-only data: %luk\n",
+		       (rodata_end - rodata_start) >> 10);
+		set_memory_ro(rodata_start, (rodata_end - rodata_start) >> PAGE_SHIFT);
+	}
 }
+EXPORT_SYMBOL_GPL(mark_rodata_ro);
 
+void mark_rodata_rw(void)
+{
+	unsigned long rodata_start =
+		((unsigned long)__start_rodata + PAGE_SIZE - 1) & PAGE_MASK;
+	unsigned long rodata_end = PAGE_ALIGN((unsigned long) &__end_rodata);
+
+	printk(KERN_INFO "Write un-protecting the kernel read-only data: %luk\n",
+	       (rodata_end - rodata_start) >> 10);
+	set_memory_rw_force(rodata_start, (rodata_end - rodata_start) >> PAGE_SHIFT);
+}
+EXPORT_SYMBOL_GPL(mark_rodata_rw);
 #endif
 
 int __init reserve_bootmem_generic(unsigned long phys, unsigned long len,
Index: linux-2.6.27/arch/x86/mm/pageattr.c
===================================================================
--- linux-2.6.27.orig/arch/x86/mm/pageattr.c
+++ linux-2.6.27/arch/x86/mm/pageattr.c
@@ -245,6 +245,8 @@ static void cpa_flush_array(unsigned lon
 	}
 }
 
+static int static_protections_allow_rodata __read_mostly;
+
 /*
  * Certain areas of memory on x86 require very specific protection flags,
  * for example the BIOS area or kernel text. Callers don't always get this
@@ -276,8 +278,10 @@ static inline pgprot_t static_protection
 	 * catches all aliases.
 	 */
 	if (within(pfn, __pa((unsigned long)__start_rodata) >> PAGE_SHIFT,
-		   __pa((unsigned long)__end_rodata) >> PAGE_SHIFT))
-		pgprot_val(forbidden) |= _PAGE_RW;
+		   __pa((unsigned long)__end_rodata) >> PAGE_SHIFT)) {
+		if (!static_protections_allow_rodata)
+			pgprot_val(forbidden) |= _PAGE_RW;
+	}
 
 #if defined(CONFIG_X86_64) && defined(CONFIG_DEBUG_RODATA)
 	/*
@@ -1105,6 +1109,21 @@ int set_memory_rw(unsigned long addr, in
 }
 EXPORT_SYMBOL_GPL(set_memory_rw);
 
+/* hack: bypass kernel rodata section static_protections check. */
+int set_memory_rw_force(unsigned long addr, int numpages)
+{
+	static DEFINE_MUTEX(lock);
+	int ret;
+
+	mutex_lock(&lock);
+	static_protections_allow_rodata = 1;
+	ret = change_page_attr_set(&addr, numpages, __pgprot(_PAGE_RW), 0);
+	static_protections_allow_rodata = 0;
+	mutex_unlock(&lock);
+
+	return ret;
+}
+
 int set_memory_np(unsigned long addr, int numpages)
 {
 	return change_page_attr_clear(&addr, numpages, __pgprot(_PAGE_PRESENT), 0);
@@ -1219,6 +1238,13 @@ int set_pages_rw(struct page *page, int
 	return set_memory_rw(addr, numpages);
 }
 
+int set_pages_rw_force(struct page *page, int numpages)
+{
+	unsigned long addr = (unsigned long)page_address(page);
+
+	return set_memory_rw_force(addr, numpages);
+}
+
 #ifdef CONFIG_DEBUG_PAGEALLOC
 
 static int __set_pages_p(struct page *page, int numpages)
Index: linux-2.6.27/arch/x86/include/asm/cacheflush.h
===================================================================
--- linux-2.6.27.orig/arch/x86/include/asm/cacheflush.h
+++ linux-2.6.27/arch/x86/include/asm/cacheflush.h
@@ -134,6 +134,7 @@ int set_memory_x(unsigned long addr, int
 int set_memory_nx(unsigned long addr, int numpages);
 int set_memory_ro(unsigned long addr, int numpages);
 int set_memory_rw(unsigned long addr, int numpages);
+int set_memory_rw_force(unsigned long addr, int numpages);
 int set_memory_np(unsigned long addr, int numpages);
 int set_memory_4k(unsigned long addr, int numpages);
 
@@ -169,16 +170,20 @@ int set_pages_x(struct page *page, int n
 int set_pages_nx(struct page *page, int numpages);
 int set_pages_ro(struct page *page, int numpages);
 int set_pages_rw(struct page *page, int numpages);
+int set_pages_rw_force(struct page *page, int numpages);
 
 
 void clflush_cache_range(void *addr, unsigned int size);
 
 #ifdef CONFIG_DEBUG_RODATA
 void mark_rodata_ro(void);
+void mark_rodata_rw(void);
 extern const int rodata_test_data;
 void set_kernel_text_rw(void);
 void set_kernel_text_ro(void);
 #else
+static inline void mark_rodata_ro(void) { }
+static inline void mark_rodata_rw(void) { }
 static inline void set_kernel_text_rw(void) { }
 static inline void set_kernel_text_ro(void) { }
 #endif
