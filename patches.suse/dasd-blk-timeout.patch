From: Hannes Reinecke <hare@suse.de>
Date: Wed, 7 Mar 2012 10:42:48 +0100
Subject: [PATCH] dasd: Implement block timeout handling
Patch-Mainline: Not yet
References: bnc#746717

This patch implements generic block layer timeout handling
callbacks for DASDs. We now can set a fixed upper limit
to DASD requests, after which the I/O is either terminated
or completed.

The block timeout handling is invoked for blocks with
'failfast' flag set. This allows us to guarantee an
I/O response time, regardless of the state of the
underlying device.

It also ensures that handling of requests without the
'failfast' flag will be unmodified, retaining the
original behaviour.

Signed-off-by: Hannes Reinecke <hare@suse.de>
Acked-by: Stefan Weinhuber <wein@de.ibm.com>

diff --git a/drivers/s390/block/dasd.c b/drivers/s390/block/dasd.c
index a024bda..79ca363 100644
--- a/drivers/s390/block/dasd.c
+++ b/drivers/s390/block/dasd.c
@@ -1981,8 +1981,10 @@ static void __dasd_process_request_queue(struct dasd_block *block)
 		 */
 		cqr->callback_data = (void *) req;
 		cqr->status = DASD_CQR_FILLED;
+		req->completion_data = cqr;
 		blk_start_request(req);
 		list_add_tail(&cqr->blocklist, &block->ccw_queue);
+		INIT_LIST_HEAD(&cqr->devlist);
 		dasd_profile_start(block, cqr, req);
 	}
 }
@@ -2248,6 +2250,79 @@ static void do_dasd_request(struct request_queue *queue)
 }
 
 /*
+ * Block timeout callback, called from the block layer
+ *
+ * request_queue lock is held on entry.
+ *
+ * Return values:
+ * BLK_EH_RESET_TIMER if the request should be left running
+ * BLK_EH_NOT_HANDLED if the request is handled or terminated
+ *                    by the driver.
+ */
+enum blk_eh_timer_return dasd_times_out(struct request *req)
+{
+	struct dasd_ccw_req *cqr = req->completion_data;
+	struct dasd_block *block = req->q->queuedata;
+	struct dasd_device *device;
+	int rc = 0;
+
+	if (!cqr)
+		return BLK_EH_NOT_HANDLED;
+
+	if (!test_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags))
+		return BLK_EH_RESET_TIMER;
+
+	device = cqr->startdev ? cqr->startdev : block->base;
+	DBF_DEV_EVENT(DBF_WARNING, device,
+		      " dasd_times_out cqr %p status %x",
+		      cqr, cqr->status);
+
+	spin_lock(&block->queue_lock);
+	spin_lock(get_ccwdev_lock(device->cdev));
+	cqr->retries = -1;
+	spin_unlock(get_ccwdev_lock(device->cdev));
+	if (cqr->status >= DASD_CQR_QUEUED) {
+		rc = dasd_cancel_req(cqr);
+	} else if (cqr->status == DASD_CQR_FILLED ||
+		   cqr->status == DASD_CQR_NEED_ERP) {
+		cqr->status = DASD_CQR_TERMINATED;
+	} else if (cqr->status == DASD_CQR_IN_ERP) {
+		struct ccw_cqr_req *searchcqr, nextcqr, tmpcqr;
+
+		list_for_each_entry_safe(searchcqr, nextcqr,
+					 &block->ccw_queue) {
+			tmpcqr = searchcqr;
+			while (tmpcqr->refers)
+				tmpcqr = tmpcqr->refers;
+			if (tmpcqr != cqr)
+				continue;
+			/* searchcqr is an ERP request for cqr */
+			spin_lock(&get_ccwdev_lock(device->cdev));
+			searchcqr->retries = -1;
+			spin_unlock(&get_ccwdev_lock(device->cdev));
+			if (searchcqr->status >= DASD_CQR_QUEUED) {
+				rc = dasd_cancel_req(searchcqr);
+			} else if ((searchcqr->status == DASD_CQR_FILLED) ||
+				   (searchcqr->status == DASD_CQR_NEED_ERP)) {
+				searchcqr->status = DASD_CQR_TERMINATED;
+				rc = 0;
+			} else if (searchcqr->status == DASD_CQR_IN_ERP) {
+				/*
+				 * Shouldn't happen; most recent ERP
+				 * request is at the front of queue
+				 */
+				continue;
+			}
+			break;
+		}
+	}
+	dasd_schedule_block_bh(block);
+	spin_unlock(&block->queue_lock);
+
+	return rc ? BLK_EH_RESET_TIMER : BLK_EH_NOT_HANDLED;
+}
+
+/*
  * Allocate and initialize request queue and default I/O scheduler.
  */
 static int dasd_alloc_queue(struct dasd_block *block)
@@ -2277,6 +2352,7 @@ static int dasd_alloc_queue(struct dasd_block *block)
 static void dasd_setup_queue(struct dasd_block *block)
 {
 	int max;
+	unsigned int timeout;
 
 	if (block->base->features & DASD_FEATURE_USERAW) {
 		/*
@@ -2299,6 +2375,14 @@ static void dasd_setup_queue(struct dasd_block *block)
 	 */
 	blk_queue_max_segment_size(block->request_queue, PAGE_SIZE);
 	blk_queue_segment_boundary(block->request_queue, PAGE_SIZE - 1);
+	blk_queue_rq_timed_out(block->request_queue, dasd_times_out);
+	/*
+	 * We always set the failfast timeout;
+	 * dasd_times_out() will ignore requests when failfast
+	 * is not enabled.
+	 */
+	timeout = block->base->failfast_expires * block->base->failfast_retries;
+	blk_queue_rq_timeout(block->request_queue, timeout * HZ);
 }
 
 /*
diff --git a/drivers/s390/block/dasd_devmap.c b/drivers/s390/block/dasd_devmap.c
index 9bb58d9..a90e2de 100644
--- a/drivers/s390/block/dasd_devmap.c
+++ b/drivers/s390/block/dasd_devmap.c
@@ -1232,7 +1232,7 @@ dasd_failfast_expires_store(struct device *dev, struct device_attribute *attr,
 	       const char *buf, size_t count)
 {
 	struct dasd_device *device;
-	unsigned long val;
+	unsigned long val, timeout;
 
 	device = dasd_device_from_cdev(to_ccwdev(dev));
 	if (IS_ERR(device))
@@ -1244,8 +1244,11 @@ dasd_failfast_expires_store(struct device *dev, struct device_attribute *attr,
 		return -EINVAL;
 	}
 
-	if (val)
+	if (val) {
 		device->failfast_expires = val;
+		timeout = val * device->failfast_retries;
+		blk_queue_rq_timeout(device->block->request_queue, timeout * HZ);
+	}
 
 	dasd_put_device(device);
 	return count;
@@ -1314,7 +1317,7 @@ dasd_failfast_retries_store(struct device *dev, struct device_attribute *attr,
 	       const char *buf, size_t count)
 {
 	struct dasd_device *device;
-	unsigned long val;
+	unsigned long val, timeout;
 
 	device = dasd_device_from_cdev(to_ccwdev(dev));
 	if (IS_ERR(device))
@@ -1326,8 +1329,11 @@ dasd_failfast_retries_store(struct device *dev, struct device_attribute *attr,
 		return -EINVAL;
 	}
 
-	if (val)
+	if (val) {
 		device->failfast_retries = val;
+		timeout = val * device->failfast_expires;
+		blk_queue_rq_timeout(device->block->request_queue, timeout * HZ);
+	}
 
 	dasd_put_device(device);
 	return count;
diff --git a/drivers/s390/block/dasd_diag.c b/drivers/s390/block/dasd_diag.c
index 287c645..c604361 100644
--- a/drivers/s390/block/dasd_diag.c
+++ b/drivers/s390/block/dasd_diag.c
@@ -591,7 +591,10 @@ dasd_diag_free_cp(struct dasd_ccw_req *cqr, struct request *req)
 
 static void dasd_diag_handle_terminated_request(struct dasd_ccw_req *cqr)
 {
-	cqr->status = DASD_CQR_FILLED;
+	if (cqr->retries < 0)
+		cqr->status = DASD_CQR_FAILED;
+	else
+		cqr->status = DASD_CQR_FILLED;
 };
 
 /* Fill in IOCTL data for device. */
diff --git a/drivers/s390/block/dasd_eckd.c b/drivers/s390/block/dasd_eckd.c
index 1fab77a..6dd044a 100644
--- a/drivers/s390/block/dasd_eckd.c
+++ b/drivers/s390/block/dasd_eckd.c
@@ -2017,6 +2017,10 @@ dasd_eckd_format_device(struct dasd_device * device,
 
 static void dasd_eckd_handle_terminated_request(struct dasd_ccw_req *cqr)
 {
+	if (cqr->retries < 0) {
+		cqr->status = DASD_CQR_FAILED;
+		return;
+	}
 	cqr->status = DASD_CQR_FILLED;
 	if (cqr->block && (cqr->startdev != cqr->block->base)) {
 		dasd_eckd_reset_ccw_to_base_io(cqr);
diff --git a/drivers/s390/block/dasd_fba.c b/drivers/s390/block/dasd_fba.c
index 839df09..7257a52 100644
--- a/drivers/s390/block/dasd_fba.c
+++ b/drivers/s390/block/dasd_fba.c
@@ -434,7 +434,10 @@ out:
 
 static void dasd_fba_handle_terminated_request(struct dasd_ccw_req *cqr)
 {
-	cqr->status = DASD_CQR_FILLED;
+	if (cqr->retries < 0)
+		cqr->status = DASD_CQR_FAILED;
+	else
+		cqr->status = DASD_CQR_FILLED;
 };
 
 static int
