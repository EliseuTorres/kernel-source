From a5c8fd616f803efe4f02ba868763aa2724cfab20 Mon Sep 17 00:00:00 2001
From: Mel Gorman <mgorman@suse.de>
Date: Sat, 8 Mar 2014 22:14:16 +0000
Subject: [PATCH] mm: vmscan: fix endless loop in kswapd balancing

References: High memory utilisation performance (bnc#859225)
Patch-mainline: Yes (v3.7)
Git-commit: 60cefed485a02bd99b6299dad70666fe49245da7

Kswapd does not in all places have the same criteria for a balanced
zone.  Zones are only being reclaimed when their high watermark is
breached, but compaction checks loop over the zonelist again when the
zone does not meet the low watermark plus two times the size of the
allocation.  This gets kswapd stuck in an endless loop over a small
zone, like the DMA zone, where the high watermark is smaller than the
compaction requirement.

Add a function, zone_balanced(), that checks the watermark, and, for
higher order allocations, if compaction has enough free memory.  Then
use it uniformly to check for balanced zones.

This makes sure that when the compaction watermark is not met, at least
reclaim happens and progress is made - or the zone is declared
unreclaimable at some point and skipped entirely.

Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
Reported-by: George Spelvin <linux@horizon.com>
Reported-by: Johannes Hirte <johannes.hirte@fem.tu-ilmenau.de>
Reported-by: Tomas Racek <tracek@redhat.com>
Tested-by: Johannes Hirte <johannes.hirte@fem.tu-ilmenau.de>
Reviewed-by: Rik van Riel <riel@redhat.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Mel Gorman <mgorman@suse.de>
---
 mm/vmscan.c | 29 +++++++++++++++++++++--------
 1 file changed, 21 insertions(+), 8 deletions(-)

diff --git a/mm/vmscan.c b/mm/vmscan.c
index 6e22fae..d1de3ac1 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2654,6 +2654,19 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *mem_cont,
 }
 #endif
 
+static bool zone_balanced(struct zone *zone, int order,
+			  unsigned long balance_gap, int classzone_idx)
+{
+	if (!zone_watermark_ok_safe(zone, order, high_wmark_pages(zone) +
+				   balance_gap, classzone_idx, 0))
+		return false;
+
+	if (COMPACTION_BUILD && order && !compaction_suitable(zone, order))
+		return false;
+
+	return true;
+}
+
 /*
  * pgdat_balanced is used when checking if a node is balanced for high-order
  * allocations. Only zones that meet watermarks and are in a zone allowed
@@ -2732,8 +2745,7 @@ static bool prepare_kswapd_sleep(pg_data_t *pgdat, int order, long remaining,
 			continue;
 		}
 
-		if (!zone_watermark_ok_safe(zone, order, high_wmark_pages(zone),
-							i, 0))
+		if (!zone_balanced(zone, order, 0, 0))
 			all_zones_ok = false;
 		else
 			balanced += zone->present_pages;
@@ -2766,6 +2778,7 @@ static bool kswapd_shrink_zone(struct zone *zone,
 			       unsigned long lru_pages,
 			       unsigned long *nr_attempted)
 {
+	bool lowmem_pressure;
 	unsigned long nr_slab;
 	int testorder = sc->order;
 	unsigned long balance_gap;
@@ -2800,8 +2813,9 @@ static bool kswapd_shrink_zone(struct zone *zone,
 		KSWAPD_ZONE_BALANCE_GAP_RATIO);
 
 	/* If the zone is balanced then no reclaim is necessary */
-	if (zone_watermark_ok_safe(zone, testorder,
-			high_wmark_pages(zone) + balance_gap, classzone_idx, 0))
+	lowmem_pressure = (buffer_heads_over_limit && is_highmem(zone));
+	if (!lowmem_pressure && zone_balanced(zone, testorder,
+						balance_gap, classzone_idx))
 		return true;
 
 	shrink_zone(zone, sc);
@@ -2824,8 +2838,8 @@ static bool kswapd_shrink_zone(struct zone *zone,
 	 * BDIs but as pressure is relieved, speculatively avoid congestion
 	 * waits.
 	 */
-	if (!zone->all_unreclaimable &&
-	    zone_watermark_ok_safe(zone, testorder, 0, classzone_idx, 0)) {
+	if (zone_reclaimable(zone) &&
+	    zone_balanced(zone, testorder, 0, classzone_idx)) {
 		zone_clear_flag(zone, ZONE_CONGESTED);
 		zone_clear_flag(zone, ZONE_TAIL_LRU_DIRTY);
 	}
@@ -2910,8 +2924,7 @@ static unsigned long balance_pgdat(pg_data_t *pgdat, int order,
 				shrink_active_list(SWAP_CLUSTER_MAX, zone,
 							&sc, 0);
 
-			if (!zone_watermark_ok_safe(zone, order,
-					high_wmark_pages(zone), 0, 0)) {
+			if (!zone_balanced(zone, order, 0, 0)) {
 				end_zone = i;
 				break;
 			} else {
